{
  "questions": [
    {
      "id": "816",
      "question": "A company is creating a solution that must offer disaster recovery across multiple AWS Regions. The solution requires relational a database that can support a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of 1 minute. Which AWS solution can achieve this?",
      "options": {
        "A": "Amazon RDS for with Multi-AZ enabled.",
        "B": "Amazon RDS for with a cross-Region replica.",
        "C": "Amazon DynamoDB global tables.",
        "D": "Amazon Aurora Global Database."
      },
      "correct_answer": "D",
      "explanation": "Aurora Global Database lets you easily scale database reads across the world and place your applications close to your users. Your applications enjoy quick data access regardless of the number and location of secondary regions, with typical cross-region replication latencies below 1 second. If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan. CORRECT: \"Amazon Aurora Global Database\" is the correct answer. INCORRECT: \"Amazon RDS for with Multi-AZ enabled\" is incorrect. RDS Multi-AZ is across availability zones, not across Regions. INCORRECT: \"Amazon RDS for with a cross-Region replica\" is incorrect. A cross-Region replica for RDS cannot provide an RPO of 1 second as there is typically more latency. You also cannot achieve a minute RPO as it takes much longer to promote a replica to a master. INCORRECT: \"Amazon DynamoDB global tables\" is incorrect. This is not a relational database; it is a non-relational database (NoSQL). INCORRECT: \"Amazon RDS for with Multi-AZ enabled\" is incorrect. INCORRECT: \"Amazon RDS for with a cross-Region replica\" is incorrect. INCORRECT: \"Amazon DynamoDB global tables\" is incorrect."
    },
    {
      "id": "817",
      "question": "A company is planning to migrate a large quantity of important data to Amazon S3. The data will be uploaded to a versioning enabled bucket in the us-west-1 Region. The solution needs to include replication of the data to another Region for disaster recovery purposes. How should a solutions architect configure the replication?",
      "options": {
        "A": "Create an additional S3 bucket with versioning in another Region and configure cross-origin resource sharing (CORS)",
        "B": "Create an additional S3 bucket in another Region and configure cross-origin resource sharing (CORS)",
        "C": "Create an additional S3 bucket with versioning in another Region and configure cross-Region replication",
        "D": "Create an additional S3 bucket in another Region and configure cross-Region replication"
      },
      "correct_answer": "C",
      "explanation": "Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can copy objects between different AWS Regions or within the same Region. Both source and destination buckets must have versioning enabled. CORRECT: \"Create an additional S3 bucket with versioning in another Region and configure cross-Region replication\" is the correct answer. INCORRECT: \"Create an additional S3 bucket in another Region and configure cross-Region replication\" is incorrect as the destination bucket must also have versioning enabled. INCORRECT: \"Create an additional S3 bucket in another Region and configure cross-origin resource sharing (CORS)\" is incorrect as CORS is not related to replication. INCORRECT: \"Create an additional S3 bucket with versioning in another Region and configure cross-origin resource sharing (CORS)\" is incorrect as CORS is not related to replication. INCORRECT: \"Create an additional S3 bucket in another Region and configure cross-Region replication\" is incorrect as the destination bucket must also have versioning enabled. INCORRECT: \"Create an additional S3 bucket in another Region and configure cross-origin resource sharing (CORS)\" is incorrect as CORS is not related to replication. INCORRECT: \"Create an additional S3 bucket with versioning in another Region and configure cross-origin resource sharing (CORS)\" is incorrect as CORS is not related to replication."
    },
    {
      "id": "818",
      "question": "An application requires a MySQL database which will only be used several times a week for short periods. The database needs to provide automatic instantiation and scaling. Which database service is most suitable?",
      "options": {
        "A": "Amazon RDS MySQL",
        "B": "Amazon EC2 instance with MySQL database installed",
        "C": "Amazon Aurora",
        "D": "Amazon Aurora Serverless"
      },
      "correct_answer": "C",
      "explanation": "Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. The database automatically starts up, shuts down, and scales capacity up or down based on application needs. This is an ideal database solution for infrequently-used applications. CORRECT: \"Amazon Aurora Serverless\" is the correct answer. INCORRECT: \"Amazon RDS MySQL\" is incorrect as this service requires an instance to be running all the time which is more costly. INCORRECT: \"Amazon EC2 instance with MySQL database installed\" is incorrect as this service requires an instance to be running all the time which is more costly. INCORRECT: \"Amazon Aurora\" is incorrect as this service requires an instance to be running all the time which is more costly. INCORRECT: \"Amazon RDS MySQL\" is incorrect as this service requires an instance to be running all the time which is more costly. INCORRECT: \"Amazon EC2 instance with MySQL database installed\" is incorrect as this service requires an instance to be running all the time which is more costly. INCORRECT: \"Amazon Aurora\" is incorrect as this service requires an instance to be running all the time which is more costly."
    },
    {
      "id": "819",
      "question": "A company operates a critical Python-based application that analyzes incoming real-time data. The application runs every 15 minutes and takes approximately 2 minutes to complete a run. It requires 1.5 GB of memory and uses the CPU intensively during its operation. The company wants to minimize the costs associated with running this application. Which solution will meet these requirements?",
      "options": {
        "A": "Implement the application as an AWS Lambda function configured with 1.5 GB of memory. Use Amazon EventBridge to schedule the function to run every 15 minutes.",
        "B": "Deploy the application on an Amazon EC2 instance and manually start and stop the instance in alignment with the schedule of the application run.",
        "C": "Use AWS App2Container (A2C) to containerize the application. Deploy the container on an Amazon EC2 instance, configure an Amazon CloudWatch alarm to stop the instance when the application is not running.",
        "D": "Use AWS App2Container (A2C) to containerize the application. Run the application as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 1 virtual CPU (vCPU) and 1.5 GB of memory."
      },
      "correct_answer": "A",
      "explanation": "This is the most cost-effective solution. AWS Lambda is designed for running code in response to events or on a schedule, and you only pay for the compute time that you consume. Configuring the function with 1.5GB memory would ensure the function has enough resources, and using Amazon EventBridge for scheduling would enable running the function every 15 minutes. CORRECT: \"Implement the application as an AWS Lambda function configured with 1.5 GB of memory. Use Amazon EventBridge to schedule the function to run every 15 minutes\" is the correct answer (as explained above.) INCORRECT: \"Use AWS App2Container (A2C) to containerize the application. Run the application as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 1 virtual CPU (vCPU) and 1.5 GB of memory\" is incorrect. This is not the most cost-effective solution. Even though AWS App2Container (A2C) would help in containerizing the application and AWS Fargate would abstract the need to manage underlying EC2 instances, it is still an overkill for an application that runs for short durations intermittently. It would still result in paying for unused compute resources. INCORRECT: \"Use AWS App2Container (A2C) to containerize the application. Deploy the container on an Amazon EC2 instance, configure an Amazon CloudWatch alarm to stop the instance when the application is not running\" is incorrect. AWS App2Container (A2C) is used to help containerize applications, but this does not optimize for cost because it requires running an EC2 instance continuously and stopping the instance when not in use can be complex and might not be timely, resulting in potential unnecessary costs. INCORRECT: \"Deploy the application on an Amazon EC2 instance and manually start and stop the instance in alignment with the schedule of the application run\" is incorrect. This solution involves significant manual intervention and managing EC2 instances. While it can work, it's not an optimized way, especially in terms of cost and operation overhead. It does not take advantage of the pay-per-use model and automatic scaling provided by AWS Lambda. INCORRECT: \"Use AWS App2Container (A2C) to containerize the application. Run the application as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 1 virtual CPU (vCPU) and 1.5 GB of memory\" is incorrect. INCORRECT: \"Use AWS App2Container (A2C) to containerize the application. Deploy the container on an Amazon EC2 instance, configure an Amazon CloudWatch alarm to stop the instance when the application is not running\" is incorrect. INCORRECT: \"Deploy the application on an Amazon EC2 instance and manually start and stop the instance in alignment with the schedule of the application run\" is incorrect."
    },
    {
      "id": "820",
      "question": "The database layer of an on-premises web application is being migrated to AWS. The database currently uses an in-memory cache. A Solutions Architect must deliver a solution that supports high availability and replication for the caching layer. Which service should the Solutions Architect recommend?",
      "options": {
        "A": "Amazon RDS Multi-AZ",
        "B": "Amazon DynamoDB",
        "C": "Amazon ElastiCache Redis",
        "D": "Amazon ElastiCache Memcached"
      },
      "correct_answer": "C",
      "explanation": "Amazon ElastiCache Redis is an in-memory database cache and supports high availability through replicas and multi-AZ. The table below compares ElastiCache Redis with Memcached: CORRECT: \"Amazon ElastiCache Redis\" is the correct answer. INCORRECT: \"Amazon ElastiCache Memcached\" is incorrect as it does not support high availability or multi-AZ. INCORRECT: \"Amazon RDS Multi-AZ\" is incorrect. This is not an in-memory database and it not suitable for use as a caching layer. INCORRECT: \"Amazon DynamoDB\" is incorrect. DynamoDB is a non-relational database. You would not use it for a caching layer. Also, the in-memory, low-latency caching for DynamoDB is implemented using DynamoDB Accelerator (DAX). INCORRECT: \"Amazon ElastiCache Memcached\" is incorrect as it does not support high availability or multi-AZ. INCORRECT: \"Amazon RDS Multi-AZ\" is incorrect. INCORRECT: \"Amazon DynamoDB\" is incorrect."
    },
    {
      "id": "821",
      "question": "A multinational organization has a distributed application that runs on Amazon EC2 instances, which are behind an Application Load Balancer in an Auto Scaling group. The application utilizes a MySQL database hosted on Amazon Aurora. The database cluster spans across multiple Availability Zones in a single region. The organization plans to launch its services in a new geographical area and wants to ensure maximum availability with minimal service interruption. Which strategy should the organization adopt?",
      "options": {
        "A": "Create a similar application layer in the new region. Establish a new Aurora MySQL database in this region. Use AWS Database Migration Service (AWS DMS) for ongoing replication from the primary database to the new region. Implement Amazon Route 53 health checks with a failover routing policy to the new region.",
        "B": "Replicate the application layer in the new region. Implement an Aurora MySQL Read Replica in the new region using Route 53 health checks and a failover routing policy. In case of primary failure, promote the Read Replica to primary.",
        "C": "Expand the existing Auto Scaling group into the new Region. Utilize Amazon Aurora Global Database to extend the database across the primary and new regions. Implement Amazon Route 53 health checks with a failover routing policy directed towards the new region.",
        "D": "Establish the application layer in the new region. Use Amazon Aurora Global Database for deploying the database in the primary and new regions. Apply Amazon Route 53 health checks with a failover routing policy to the new region. Promote the secondary to primary as needed."
      },
      "correct_answer": "A",
      "explanation": "This solution involves creating an application layer in the new region and using Amazon Aurora Global Database, which supports replicating your databases across multiple regions with minimal impact on performance. This configuration can enhance disaster recovery capabilities and can reduce the impact of planned maintenance. Amazon Route 53 health checks with a failover routing policy can automatically route traffic to the new region in the event of a failure in the primary region, thereby ensuring high availability. With an Aurora global database, there are two different approaches to failover depending on the scenario. You can use manual unplanned failover (detach and promote) or managed planned failover. CORRECT: \"Establish the application layer in the new region. Use Amazon Aurora Global Database for deploying the database in the primary and new regions. Apply Amazon Route 53 health checks with a failover routing policy to the new region. Perform a manual failover as required\" is the correct answer (as explained above.) INCORRECT: \"Replicate the application layer in the new region. Implement an Aurora MySQL Read Replica in the new region using Route 53 health checks and a failover routing policy. In case of primary failure, promote the Read Replica to primary\" is incorrect. This solution involves creating a Read Replica in the new region, which would indeed allow for the promotion of the Read Replica to a primary instance if necessary. However, this process isn't instantaneous and could lead to service interruption, which is not what the question asked for. Aurora Global Database provides a lower RTO/RPO. INCORRECT: \"Create a similar application layer in the new region. Establish a new Aurora MySQL database in this region. Use AWS Database Migration Service (AWS DMS) for ongoing replication from the primary database to the new region. Implement Amazon Route 53 health checks with a failover routing policy to the new region\" is incorrect. AWS Database Migration Service (AWS DMS) is primarily used for migrating databases to AWS from on-premises environments or for replicating databases for data warehousing and other use cases. It isn't as suitable for ongoing high-availability or failover scenarios as Amazon Aurora Global Database, which is specifically designed for these situations. INCORRECT: \"Expand the existing Auto Scaling group into the new Region. Utilize Amazon Aurora Global Database to extend the database across the primary and new regions. Implement Amazon Route 53 health checks with a failover routing policy directed towards the new region\" is incorrect. It is not possible to expand an Auto Scaling group across multiple Regions. ASGs operate within a Region only. INCORRECT: \"Replicate the application layer in the new region. Implement an Aurora MySQL Read Replica in the new region using Route 53 health checks and a failover routing policy. In case of primary failure, promote the Read Replica to primary\" is incorrect. INCORRECT: \"Create a similar application layer in the new region. Establish a new Aurora MySQL database in this region. Use AWS Database Migration Service (AWS DMS) for ongoing replication from the primary database to the new region. Implement Amazon Route 53 health checks with a failover routing policy to the new region\" is incorrect. INCORRECT: \"Expand the existing Auto Scaling group into the new Region. Utilize Amazon Aurora Global Database to extend the database across the primary and new regions. Implement Amazon Route 53 health checks with a failover routing policy directed towards the new region\" is incorrect."
    },
    {
      "id": "822",
      "question": "An on-premises server runs a MySQL database and will be migrated to the AWS Cloud. The company require a managed solution that supports high availability and automatic failover in the event of the outage of an Availability Zone (AZ). Which solution is the BEST fit for these requirements?",
      "options": {
        "A": "Use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon RDS MySQL. Use the Schema Conversion Tool (SCT) to enable conversion from MySQL to Amazon RDS",
        "B": "Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon EC2 MySQL Multi-AZ deployment",
        "C": "Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon RDS MySQL Multi-AZ deployment",
        "D": "Create a snapshot of the MySQL database server and use AWS DataSync to migrate the data Amazon S3. Launch a new Amazon RDS MySQL Multi-AZ deployment from the snapshot"
      },
      "correct_answer": "C",
      "explanation": "The AWS DMS service can be used to directly migrate the MySQL database to an Amazon RDS Multi-AZ deployment. The entire process can be online and is managed for you. There is no need to perform schema translation between MySQL and RDS (assuming you choose the MySQL RDS engine). CORRECT: \"Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon RDS MySQL Multi-AZ deployment\" is the correct answer. INCORRECT: \"Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon EC2 MySQL Multi-AZ deployment\" is incorrect as there is no such thing as “multi-AZ” on Amazon EC2 with MySQL, you must use RDS. INCORRECT: \"Create a snapshot of the MySQL database server and use AWS DataSync to migrate the data Amazon S3. Launch a new Amazon RDS MySQL Multi-AZ deployment from the snapshot\" is incorrect. You cannot create a snapshot of a MySQL database server running on-premises. INCORRECT: \"Use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon RDS MySQL. Use the Schema Conversion Tool (SCT) to enable conversion from MySQL to Amazon RDS\" is incorrect. There is no need to convert the schema when migrating from MySQL to Amazon RDS (MySQL engine). INCORRECT: \"Use the AWS Database Migration Service (DMS) to directly migrate the database to an Amazon EC2 MySQL Multi-AZ deployment\" is incorrect as there is no such thing as “multi-AZ” on Amazon EC2 with MySQL, you must use RDS. INCORRECT: \"Create a snapshot of the MySQL database server and use AWS DataSync to migrate the data Amazon S3. Launch a new Amazon RDS MySQL Multi-AZ deployment from the snapshot\" is incorrect. INCORRECT: \"Use the AWS Database Migration Service (DMS) to directly migrate the database to Amazon RDS MySQL. Use the Schema Conversion Tool (SCT) to enable conversion from MySQL to Amazon RDS\" is incorrect."
    },
    {
      "id": "823",
      "question": "A company operates a production web application that uses an Amazon RDS MySQL database. The database has automated, non-encrypted daily backups. To increase the security of the data, it has been recommended that encryption should be enabled for backups. Unencrypted backups will be destroyed after the first encrypted backup has been completed. What should be done to enable encryption for future backups?",
      "options": {
        "A": "Modify the backup section of the database configuration to toggle the Enable encryption check box",
        "B": "Enable an encrypted read replica on RDS for MySQL. Promote the encrypted read replica to primary. Remove the original database instance",
        "C": "Create a snapshot of the database. Copy it to an encrypted snapshot. Restore the database from the encrypted snapshot",
        "D": "Enable default encryption for the Amazon S3 bucket where backups are stored"
      },
      "correct_answer": "C",
      "explanation": "Amazon RDS uses snapshots for backup. Snapshots are encrypted when created only if the database is encrypted and you can only select encryption for the database when you first create it. In this case the database, and hence the snapshots, ad unencrypted. However, you can create an encrypted copy of a snapshot. You can restore using that snapshot which creates a new DB instance that has encryption enabled. From that point on encryption will be enabled for all snapshots. CORRECT: \"Create a snapshot of the database. Copy it to an encrypted snapshot. Restore the database from the encrypted snapshot\" is the correct answer. INCORRECT: \"Enable an encrypted read replica on RDS for MySQL. Promote the encrypted read replica to primary. Remove the original database instance\" is incorrect as you cannot create an encrypted read replica from an unencrypted master. INCORRECT: \"Modify the backup section of the database configuration to toggle the Enable encryption check box\" is incorrect as you cannot add encryption for an existing database. INCORRECT: \"Enable default encryption for the Amazon S3 bucket where backups are stored\" is incorrect because you do not have access to the S3 bucket in which snapshots are stored. INCORRECT: \"Enable an encrypted read replica on RDS for MySQL. Promote the encrypted read replica to primary. Remove the original database instance\" is incorrect as you cannot create an encrypted read replica from an unencrypted master. INCORRECT: \"Modify the backup section of the database configuration to toggle the Enable encryption check box\" is incorrect as you cannot add encryption for an existing database. INCORRECT: \"Enable default encryption for the Amazon S3 bucket where backups are stored\" is incorrect because you do not have access to the S3 bucket in which snapshots are stored."
    },
    {
      "id": "824",
      "question": "A Solutions Architect needs a solution for hosting a website that will be used by a development team. The website contents will consist of HTML, CSS, client-side JavaScript, and images. Which solution is MOST cost-effective?",
      "options": {
        "A": "Create an Amazon S3 bucket and host the website there.",
        "B": "Use a Docker container to host the website on AWS Fargate.",
        "C": "Create an Application Load Balancer with an AWS Lambda target.",
        "D": "Launch an Amazon EC2 instance and host the website there."
      },
      "correct_answer": "A",
      "explanation": "Amazon S3 can be used for hosting static websites and cannot be used for dynamic content. In this case the content is purely static with client-side code running. Therefore, an S3 static website will be the most cost-effective solution for hosting this website. CORRECT: \"Create an Amazon S3 bucket and host the website there\" is the correct answer. INCORRECT: \"Launch an Amazon EC2 instance and host the website there\" is incorrect. This will be more expensive as it uses an EC2 instances. INCORRECT: \"Use a Docker container to host the website on AWS Fargate\" is incorrect. A static website on S3 is sufficient for this use case and will be more cost-effective than Fargate. INCORRECT: \"Create an Application Load Balancer with an AWS Lambda target\" is incorrect. This is also a more expensive solution and unnecessary for this use case. INCORRECT: \"Launch an Amazon EC2 instance and host the website there\" is incorrect. INCORRECT: \"Use a Docker container to host the website on AWS Fargate\" is incorrect. INCORRECT: \"Create an Application Load Balancer with an AWS Lambda target\" is incorrect."
    },
    {
      "id": "825",
      "question": "A solutions architect has created a new AWS account and must secure AWS account root user access. Which combination of actions will accomplish this? (Select TWO.)",
      "options": {
        "A": "Delete the root user account",
        "B": "Add the root user to a group containing administrative permissions",
        "C": "Ensure the root user uses a strong password",
        "D": "Enable multi-factor authentication to the root user"
      },
      "correct_answer": "C,D",
      "explanation": "There are several security best practices for securing the root user account: · Lock away root user access keys OR delete them if possible · Use a strong password · Enable multi-factor authentication (MFA) The root user automatically has full privileges to the account and these privileges cannot be restricted so it is extremely important to follow best practice advice about securing the root user account. CORRECT: \"Ensure the root user uses a strong password\" is the correct answer. CORRECT: \"Enable multi-factor authentication to the root user\" is the correct answer. INCORRECT: \"Store root user access keys in an encrypted Amazon S3 bucket\" is incorrect as the best practice is to lock away or delete the root user access keys. An S3 bucket is not a suitable location for storing them, even if encrypted. INCORRECT: \"Add the root user to a group containing administrative permissions\" is incorrect as this does not restrict access and is unnecessary. INCORRECT: \"Delete the root user account\" is incorrect as you cannot delete the root user account. INCORRECT: \"Store root user access keys in an encrypted Amazon S3 bucket\" is incorrect as the best practice is to lock away or delete the root user access keys. INCORRECT: \"Add the root user to a group containing administrative permissions\" is incorrect as this does not restrict access and is unnecessary. INCORRECT: \"Delete the root user account\" is incorrect as you cannot delete the root user account."
    },
    {
      "id": "826",
      "question": "An international software firm provides its clients with custom solutions and tools designed for efficient data collection and analysis on AWS. The firm intends to centrally manage and distribute a standard set of solutions and tools for its clients' self-service needs. Which solution would best satisfy these requirements?",
      "options": {
        "A": "Create AWS CloudFormation stacks for the clients.",
        "B": "Create AWS Config rules for the clients.",
        "C": "Create AWS Systems Manager documents for the clients.",
        "D": "Create AWS Service Catalog portfolios for the clients."
      },
      "correct_answer": "D",
      "explanation": "AWS Service Catalog enables organizations to create and manage catalogs of IT services that are approved for use on AWS. It allows centrally managed service portfolios, which clients can use on a self-service basis. AWS Service Catalog provides a single location where organizations can centrally manage catalogs of IT services, which simplifies the organizational process and helps ensure compliance. CORRECT: \"Create AWS Service Catalog portfolios for the clients\" is the correct answer (as explained above.) INCORRECT: \"Create AWS CloudFormation stacks for the clients\" is incorrect. While AWS CloudFormation is a powerful service for infrastructure as code (IaC), it doesn't provide a straightforward way for clients to discover and use shared tools or solutions for self-service needs. It lacks the management features and access control mechanisms necessary for this scenario. INCORRECT: \"Create AWS Systems Manager documents for the clients\" is incorrect. AWS Systems Manager documents define the actions that Systems Manager performs on your managed instances. Although Systems Manager allows the central management of resources and applications, it doesn't provide an effective means for clients to self-discover and use shared tools or solutions. INCORRECT: \"Create AWS Config rules for the clients\" is incorrect. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It isn't designed to centrally manage and distribute software tools or solutions. INCORRECT: \"Create AWS CloudFormation stacks for the clients\" is incorrect. INCORRECT: \"Create AWS Systems Manager documents for the clients\" is incorrect. INCORRECT: \"Create AWS Config rules for the clients\" is incorrect."
    },
    {
      "id": "827",
      "question": "A company is looking for ways to incorporate its current AWS usage expenditure into its operational expense tracking dashboard. A solutions architect has been tasked with proposing a method that enables the company to fetch its current year's cost data and project the costs for the forthcoming 12 months programmatically. Which approach would fulfill these needs with the MINIMUM operational burden?",
      "options": {
        "A": "Leverage the AWS Cost Explorer API to retrieve usage cost-related data, using pagination for larger data sets.",
        "B": "Make use of downloadable AWS Cost Explorer report files in the .csv format to access usage cost-related data.",
        "C": "Generate AWS Budgets reports on usage cost data and dispatch the data to the corporation through SMTP.",
        "D": "Set up AWS Budgets actions to transmit usage cost data to the corporation via FTP."
      },
      "correct_answer": "A",
      "explanation": "AWS Cost Explorer API provides programmatic access to AWS cost and usage information. The user can query for aggregated data such as total monthly costs or total daily usage with this API. Also, the Cost Explorer API supports pagination for managing larger data sets, making it efficient for larger queries. CORRECT: \"Leverage the AWS Cost Explorer API to retrieve usage cost-related data, using pagination for larger data sets\" is the correct answer (as explained above.) INCORRECT: \"Make use of downloadable AWS Cost Explorer report files in the .csv format to access usage cost-related data\" is incorrect. While AWS Cost Explorer does allow you to download .csv reports of your cost data, this method would not be programmatically accessible and would involve manual steps to download and process the data. INCORRECT: \"Set up AWS Budgets actions to transmit usage cost data to the corporation via FTP\" is incorrect. AWS Budgets actions allow you to set custom cost and usage budgets that trigger actions (such as turning off EC2 instances) when the budget thresholds you set are breached. However, AWS Budgets does not support transmitting data via FTP. INCORRECT: \"Generate AWS Budgets reports on usage cost data and dispatch the data to the corporation through SMTP\" is incorrect. AWS Budgets does not support the dispatching of data through SMTP. AWS Budgets is primarily a tool for setting up alerts on your AWS costs or usage to control your costs, rather than a tool for exporting or transmitting cost data. INCORRECT: \"Make use of downloadable AWS Cost Explorer report files in the .csv format to access usage cost-related data\" is incorrect. INCORRECT: \"Set up AWS Budgets actions to transmit usage cost data to the corporation via FTP\" is incorrect. INCORRECT: \"Generate AWS Budgets reports on usage cost data and dispatch the data to the corporation through SMTP\" is incorrect."
    },
    {
      "id": "828",
      "question": "A Solutions Architect has created an AWS Organization with several AWS accounts. Security policy requires that use of specific API actions are limited across all accounts. The Solutions Architect requires a method of centrally controlling these actions. What is the SIMPLEST method of achieving the requirements?",
      "options": {
        "A": "Create a service control policy in the root organizational unit to deny access to the services or actions",
        "B": "Create cross-account roles in each account to limit access to the services and actions that are allowed",
        "C": "Create a Network ACL that limits access to the services or actions and attach it to all relevant subnets",
        "D": "Create an IAM policy in the root account and attach it to users and groups in each account"
      },
      "correct_answer": "A",
      "explanation": "Service control policies (SCPs) offer central control over the maximum available permissions for all accounts in your organization allowing you to ensure your accounts stay within your organization’s access control guidelines. In the example below, a policy in OU1 restricts all users from launching EC2 instance types other than a t2.micro: CORRECT: \"Create a service control policy in the root organizational unit to deny access to the services or actions\" is the correct answer. INCORRECT: \"Create a Network ACL that limits access to the services or actions and attach it to all relevant subnets\" is incorrect. Network ACLs control network traffic - not API actions. INCORRECT: \"Create an IAM policy in the root account and attach it to users and groups in each account\" is incorrect. This is not an efficient or centrally managed method of applying the security restrictions. INCORRECT: \"Create cross-account roles in each account to limit access to the services and actions that are allowed\" is incorrect. This is another example of a complex and inefficient method of providing access across accounts and does not restrict API actions within the account. INCORRECT: \"Create a Network ACL that limits access to the services or actions and attach it to all relevant subnets\" is incorrect. INCORRECT: \"Create an IAM policy in the root account and attach it to users and groups in each account\" is incorrect. INCORRECT: \"Create cross-account roles in each account to limit access to the services and actions that are allowed\" is incorrect."
    },
    {
      "id": "829",
      "question": "The Solutions Architect in charge of a critical application must ensure the Amazon EC2 instances are able to be launched in another AWS Region in the event of a disaster. What steps should the Solutions Architect take? (Select TWO.)",
      "options": {
        "A": "Create AMIs of the instances and copy them to another Region",
        "B": "Enable cross-region snapshots for the Amazon EC2 instances",
        "C": "Copy the snapshots using Amazon S3 cross-region replication",
        "D": "Launch instances in the second Region from the AMIs"
      },
      "correct_answer": "A,D",
      "explanation": "You can create AMIs of the EC2 instances and then copy them across Regions. This provides a point-in-time copy of the state of the EC2 instance in the remote Region. Once you’ve created AMIs of EC2 instances and copied them to the second Region, you can then launch the EC2 instances from the AMIs in that Region. This is a good DR strategy as you have moved stateful EC2 instances to another Region. CORRECT: \"Create AMIs of the instances and copy them to another Region\" is the correct answer. CORRECT: \"Launch instances in the second Region from the AMIs\" is also a correct answer. INCORRECT: \"Launch instances in the second Region using the S3 API\" is incorrect. Though snapshots (and EBS-backed AMIs) are stored on Amazon S3, you cannot actually access them using the S3 API. You must use the EC2 API. INCORRECT: \"Enable cross-region snapshots for the Amazon EC2 instances\" is incorrect. You cannot enable “cross-region snapshots” as this is not a feature that currently exists. INCORRECT: \"Copy the snapshots using Amazon S3 cross-region replication\" is incorrect. You cannot work with snapshots using Amazon S3 at all including leveraging the cross-region replication feature. INCORRECT: \"Launch instances in the second Region using the S3 API\" is incorrect. INCORRECT: \"Enable cross-region snapshots for the Amazon EC2 instances\" is incorrect. INCORRECT: \"Copy the snapshots using Amazon S3 cross-region replication\" is incorrect."
    },
    {
      "id": "830",
      "question": "A company runs an application on premises that stores a large quantity of semi-structured data using key-value pairs. The application code will be migrated to AWS Lambda and a highly scalable solution is required for storing the data. Which datastore will be the best fit for these requirements?",
      "options": {
        "A": "Amazon EFS",
        "B": "Amazon DynamoDB",
        "C": "Amazon EBS",
        "D": "Amazon RDS MySQL"
      },
      "correct_answer": "B",
      "explanation": "Amazon DynamoDB is a no-SQL database that stores data using key-value pairs. It is ideal for storing large amounts of semi-structured data and is also highly scalable. This is the best solution for storing this data based on the requirements in the scenario. CORRECT: \"Amazon DynamoDB\" is the correct answer. INCORRECT: \"Amazon EFS\" is incorrect. The Amazon Elastic File System (EFS) is not suitable for storing key-value pairs. INCORRECT: \"Amazon RDS MySQL\" is incorrect. Amazon Relational Database Service (RDS) is used for structured data as it is an SQL type of database. INCORRECT: \"Amazon EBS\" is incorrect. Amazon Elastic Block Store (EBS) is a block-based storage system. You attach volumes to EC2 instances. It is not used for key-value pairs or to be used by Lambda functions. INCORRECT: \"Amazon EFS\" is incorrect. INCORRECT: \"Amazon RDS MySQL\" is incorrect. INCORRECT: \"Amazon EBS\" is incorrect."
    },
    {
      "id": "831",
      "question": "A solutions architect is designing a high performance computing (HPC) application using Amazon EC2 Linux instances. All EC2 instances need to communicate to each other with low latency and high throughput network performance. Which EC2 solution BEST meets these requirements?",
      "options": {
        "A": "Launch the EC2 instances in an Auto Scaling group spanning multiple Availability Zones",
        "B": "Launch the EC2 instances in a cluster placement group in one Availability Zone",
        "C": "Launch the EC2 instances in an Auto Scaling group in two Regions. Place a Network Load Balancer in front of the instances",
        "D": "Launch the EC2 instances in a spread placement group in one Availability Zone"
      },
      "correct_answer": "B",
      "explanation": "When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies: Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures. For this scenario, a cluster placement group should be used as this is the best option for providing low-latency network performance for a HPC application. CORRECT: \"Launch the EC2 instances in a cluster placement group in one Availability Zone\" is the correct answer. INCORRECT: \"Launch the EC2 instances in a spread placement group in one Availability Zone\" is incorrect as the spread placement group is used to spread instances across distinct underlying hardware. INCORRECT: \"Launch the EC2 instances in an Auto Scaling group in two Regions. Place a Network Load Balancer in front of the instances\" is incorrect as this does not achieve the stated requirement to provide low-latency, high throughput network performance between instances. Also, you cannot use an ELB across Regions. INCORRECT: \"Launch the EC2 instances in an Auto Scaling group spanning multiple Availability Zones\" is incorrect as this does not reduce network latency or improve performance. INCORRECT: \"Launch the EC2 instances in a spread placement group in one Availability Zone\" is incorrect as the spread placement group is used to spread instances across distinct underlying hardware. INCORRECT: \"Launch the EC2 instances in an Auto Scaling group in two Regions. Place a Network Load Balancer in front of the instances\" is incorrect as this does not achieve the stated requirement to provide low-latency, high throughput network performance between instances. INCORRECT: \"Launch the EC2 instances in an Auto Scaling group spanning multiple Availability Zones\" is incorrect as this does not reduce network latency or improve performance."
    },
    {
      "id": "832",
      "question": "There are badge readers located at every entrance of an organization’s warehouses. A message is sent over HTTPS when badges are scanned to indicate who tried to access the entrance. A solutions architect must design a system to process these messages. A highly available solution is required. The solution must store results in a durable data store for later analysis. Which system architecture should the solutions architect recommend?",
      "options": {
        "A": "Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB.",
        "B": "Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results.",
        "C": "Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function.",
        "D": "Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket."
      },
      "correct_answer": "C",
      "explanation": "Amazon API Gateway would be ideal for providing a secure entry point for your application, and for traffic to be sent via HTTPS. AWS Lambda would integrate seamlessly with API Gateway to process the data, as an event-driven solution like this would be perfect when designing a scalable system based on sporadic use. Finally, DynamoDB is highly scalable and is a perfect repository for data to be stored for future analysis. CORRECT: \"Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function\" is the correct answer (as explained above.) INCORRECT: \"Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results\" is incorrect. As the action of a badge being read to initiate access to a warehouse should only take a few seconds, spinning up an EC2 instance to serve as a HTTPS endpoint would take minutes, and is not suitable for this use case. INCORRECT: \"Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB” is incorrect. Amazon Route 53 is a managed DNS service, and DNS is not required in this instance as the badge reader does not have a DNS name. INCORRECT: \"Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket\" is incorrect. VPC endpoints are designed to facilitate traffic across the AWS backbone network between AWS services and are not used to create connections between external endpoints outside of the AWS network and an Amazon S3 bucket. INCORRECT: \"Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results\" is incorrect. INCORRECT: \"Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket\" is incorrect."
    },
    {
      "id": "833",
      "question": "An e-commerce web application needs a highly scalable key-value database. Which AWS database service should be used?",
      "options": {
        "A": "Amazon RedShift",
        "B": "Amazon ElastiCache",
        "C": "Amazon DynamoDB",
        "D": "Amazon RDS"
      },
      "correct_answer": "C",
      "explanation": "A key-value database is a type of nonrelational (NoSQL) database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability – this is the best database for these requirements. CORRECT: \"Amazon DynamoDB\" is the correct answer. INCORRECT: \"Amazon RDS\" is incorrect. Amazon RDS is a relational (SQL) type of database, not a key-value / nonrelational database. INCORRECT: \"Amazon RedShift\" is incorrect. Amazon RedShift is a data warehouse service used for online analytics processing (OLAP) workloads. INCORRECT: \"Amazon ElastiCache\" is incorrect. Amazon ElastiCache is an in-memory caching database. This is not a nonrelational key-value database. INCORRECT: \"Amazon RDS\" is incorrect. INCORRECT: \"Amazon RedShift\" is incorrect. INCORRECT: \"Amazon ElastiCache\" is incorrect."
    },
    {
      "id": "834",
      "question": "A data analytics company is building a high-performance application that requires concurrent writes to a shared block storage volume from multiple Amazon EC2 instances. The EC2 instances are Nitro-based and reside within the same Availability Zone. The company needs a storage solution that supports simultaneous connections to facilitate data resilience and high availability. Which solution will meet these requirements?",
      "options": {
        "A": "Use Amazon EFS with NFSv4.1 protocol across multiple EC2 instances.",
        "B": "Use General Purpose SSD (gp2) EBS volumes with Amazon EBS Multi-Attach.",
        "C": "Use Provisioned IOPS SSD (io2) EBS volumes with Amazon EBS Multi-Attach.",
        "D": "Use Amazon S3 with S3 Transfer Acceleration to enhance speed."
      },
      "correct_answer": "C",
      "explanation": "io2 volumes are designed for I/O-intensive workloads, particularly database workloads, that require high performance and low latency. io1 and io2 volumes support Multi-Attach, which enables you to attach a single volume to multiple EC2 instances in the same Availability Zone. CORRECT: \"Use Provisioned IOPS SSD (io2) EBS volumes with Amazon EBS Multi-Attach\" is the correct answer (as explained above.) INCORRECT: \"Use Amazon EFS with NFSv4.1 protocol across multiple EC2 instances\" is incorrect. Amazon Elastic File System (EFS) is a scalable file storage for use with Amazon EC2. You can use an Amazon EFS file system as a common data source for workloads and applications running on multiple instances, but it does not provide the block-level storage required for high IOPS operations. INCORRECT: \"Use Amazon S3 with S3 Transfer Acceleration to enhance speed\" is incorrect. Amazon S3 is an object storage service. While S3 Transfer Acceleration does enhance the speed of in-transit file transfers, it is not a block storage solution, it is an object storage solution and is not suitable for this use case. INCORRECT: \"Use General Purpose SSD (gp2) EBS volumes with Amazon EBS Multi-Attach\" is incorrect. Amazon EBS Multi-Attach only supports io1 and io2 volumes, and it is not supported on gp2 volumes. INCORRECT: \"Use Amazon EFS with NFSv4.1 protocol across multiple EC2 instances\" is incorrect. INCORRECT: \"Use Amazon S3 with S3 Transfer Acceleration to enhance speed\" is incorrect. INCORRECT: \"Use General Purpose SSD (gp2) EBS volumes with Amazon EBS Multi-Attach\" is incorrect."
    },
    {
      "id": "835",
      "question": "An application uses an Amazon RDS database and Amazon EC2 instances in a web tier. The web tier instances must not be directly accessible from the internet to improve security. How can a Solutions Architect meet these requirements?",
      "options": {
        "A": "Launch the EC2 instances in a private subnet and create an Application Load Balancer in a public subnet",
        "B": "Launch the EC2 instances in a public subnet and use AWS WAF to protect the instances from internet-based attacks",
        "C": "Launch the EC2 instances in a public subnet and create an Application Load Balancer in a public subnet",
        "D": "Launch the EC2 instances in a private subnet with a NAT gateway and update the route table"
      },
      "correct_answer": "A",
      "explanation": "To prevent direct connectivity to the EC2 instances from the internet you can deploy your EC2 instances in a private subnet and have the ELB in a public subnet. To configure this you must enable a public subnet in the ELB that is in the same AZ as the private subnet. CORRECT: \"Launch the EC2 instances in a private subnet and create an Application Load Balancer in a public subnet\" is the correct answer. INCORRECT: \"Launch the EC2 instances in a private subnet with a NAT gateway and update the route table\" is incorrect. This configuration will not allow the application to be accessible from the internet, the aim is to only prevent direct access to the EC2 instances. INCORRECT: \"Launch the EC2 instances in a public subnet and use AWS WAF to protect the instances from internet-based attacks\" is incorrect. With the EC2 instances in a public subnet, direct access from the internet is possible. It only takes a security group misconfiguration or software exploit and the instance becomes vulnerable to attack. INCORRECT: \"Launch the EC2 instances in a public subnet and create an Application Load Balancer in a public subnet\" is incorrect. The EC2 instances should be launched in a private subnet. INCORRECT: \"Launch the EC2 instances in a private subnet with a NAT gateway and update the route table\" is incorrect. INCORRECT: \"Launch the EC2 instances in a public subnet and use AWS WAF to protect the instances from internet-based attacks\" is incorrect. INCORRECT: \"Launch the EC2 instances in a public subnet and create an Application Load Balancer in a public subnet\" is incorrect."
    },
    {
      "id": "836",
      "question": "An application runs on EC2 instances in a private subnet behind an Application Load Balancer in a public subnet. The application is highly available and distributed across multiple AZs. The EC2 instances must make API calls to an internet-based service. How can the Solutions Architect enable highly available internet connectivity?",
      "options": {
        "A": "Create a NAT gateway and attach it to the VPC. Add a route to the gateway to each private subnet route table",
        "B": "Configure an internet gateway. Add a route to the gateway to each private subnet route table",
        "C": "Create a NAT gateway in the public subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT gateway",
        "D": "Create a NAT instance in the private subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT instance"
      },
      "correct_answer": "C",
      "explanation": "The only solution presented that actually works is to create a NAT gateway in the public subnet of each AZ. They must be created in the public subnet as they gain public IP addresses and use an internet gateway for internet access. The route tables in the private subnets must then be configured with a route to the NAT gateway and then the EC2 instances will be able to access the internet (subject to security group configuration). CORRECT: \"Create a NAT gateway in the public subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT gateway\" is the correct answer. INCORRECT: \"Create a NAT gateway and attach it to the VPC. Add a route to the gateway to each private subnet route table\" is incorrect. You do not attach NAT gateways to VPCs, you add them to public subnets. INCORRECT: \"Configure an internet gateway. Add a route to the gateway to each private subnet route table\" is incorrect. You cannot add a route to an internet gateway to a private subnet route table (private EC2 instances don’t even have public IP addresses). INCORRECT: \"Create a NAT instance in the private subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT instance\" is incorrect. You do not create NAT instances in private subnets, they must be created in public subnets. INCORRECT: \"Create a NAT gateway and attach it to the VPC. Add a route to the gateway to each private subnet route table\" is incorrect. INCORRECT: \"Configure an internet gateway. Add a route to the gateway to each private subnet route table\" is incorrect. INCORRECT: \"Create a NAT instance in the private subnet of each AZ. Update the route tables for each private subnet to direct internet-bound traffic to the NAT instance\" is incorrect."
    },
    {
      "id": "837",
      "question": "An application runs on Amazon EC2 instances in a private subnet. The EC2 instances process data that is stored in an Amazon S3 bucket. The data is highly confidential and a private and secure connection is required between the EC2 instances and the S3 bucket. Which solution meets these requirements?",
      "options": {
        "A": "Set up an IAM policy to grant read-write access to the S3 bucket.",
        "B": "Set up S3 bucket policies to allow access from a VPC endpoint.",
        "C": "Configure a custom SSL/TLS certificate on the S3 bucket.",
        "D": "Configure encryption for the S3 bucket using an AWS KMS key."
      },
      "correct_answer": "B",
      "explanation": "A gateway VPC endpoint can be used to access an Amazon S3 bucket using private IP addresses. To further secure the solution an S3 bucket policy can be created that restricts access to the VPC endpoint so connections cannot be made to the bucket from other sources. CORRECT: \"Set up S3 bucket policies to allow access from a VPC endpoint\" is the correct answer. INCORRECT: \"Set up an IAM policy to grant read-write access to the S3 bucket\" is incorrect. This does not enable private access from EC2. A gateway VPC endpoint is required. INCORRECT: \"Configure encryption for the S3 bucket using an AWS KMS key\" is incorrect. This will encrypt data at rest but does not secure the connection to the bucket or ensure private connections must be made. INCORRECT: \"Configure a custom SSL/TLS certificate on the S3 bucket\" is incorrect. You cannot add a custom SSL/TLS certificate to Amazon S3. INCORRECT: \"Set up an IAM policy to grant read-write access to the S3 bucket\" is incorrect. INCORRECT: \"Configure encryption for the S3 bucket using an AWS KMS key\" is incorrect. INCORRECT: \"Configure a custom SSL/TLS certificate on the S3 bucket\" is incorrect."
    },
    {
      "id": "838",
      "question": "Health related data in Amazon S3 needs to be frequently accessed for up to 90 days. After that time the data must be retained for compliance reasons for seven years and is rarely accessed. Which storage classes should be used?",
      "options": {
        "A": "Store data in INTELLIGENT_TIERING for 90 days then transition to STANDARD_IA",
        "B": "Store data in STANDARD for 90 days then transition the data to DEEP_ARCHIVE",
        "C": "Store data in STANDARD for 90 days then expire the data",
        "D": "Store data in STANDARD for 90 days then transition to REDUCED_REDUNDANCY"
      },
      "correct_answer": "B",
      "explanation": "In this case the data is frequently accessed so must be stored in standard for the first 90 days. After that the data is still to be kept for compliance reasons but is rarely accessed so is a good use case for DEEP_ARCHIVE. CORRECT: \"Store data in STANDARD for 90 days then transition the data to DEEP_ARCHIVE\" is the correct answer. INCORRECT: \"Store data in INTELLIGENT_TIERING for 90 days then transition to STANDARD_IA\" is incorrect. You cannot transition from INTELLIGENT_TIERING to STANDARD_IA. INCORRECT: \"Store data in STANDARD for 90 days then expire the data\" is incorrect. Expiring the data is not possible as it must be retained for compliance. INCORRECT: \"Store data in STANDARD for 90 days then transition to REDUCED_REDUNDANCY\" is incorrect. You cannot transition from any storage class to REDUCED_REDUNDANCY. INCORRECT: \"Store data in INTELLIGENT_TIERING for 90 days then transition to STANDARD_IA\" is incorrect. INCORRECT: \"Store data in STANDARD for 90 days then expire the data\" is incorrect. INCORRECT: \"Store data in STANDARD for 90 days then transition to REDUCED_REDUNDANCY\" is incorrect."
    },
    {
      "id": "839",
      "question": "A company has several AWS accounts that are used by developers for development, testing and pre-production environments. The company has received large bills for Amazon EC2 instances that are underutilized. A Solutions Architect has been tasked with restricting the ability to launch large EC2 instances in all accounts. How can the Solutions Architect meet this requirement with the LEAST operational overhead?",
      "options": {
        "A": "Create a resource-based policy that denies the launch of large EC2 instances and attach it to Amazon EC2 in each account.",
        "B": "Create a service-linked role for Amazon EC2 and attach a policy the denies the launch of large EC2 instances.",
        "C": "Create an organization in AWS Organizations that includes all accounts and create a service control policy (SCP) that denies the launch of large EC2 instances.",
        "D": "Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role."
      },
      "correct_answer": "C",
      "explanation": "Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. In this case the Solutions Architect can use an SCP to define a restriction that denies the launch of large EC2 instances. The SCP can be applied to all accounts, and this will ensure that even those users with permissions to launch EC2 instances will be restricted to smaller EC2 instance types. CORRECT: \"Create an organization in AWS Organizations that includes all accounts and create a service control policy (SCP) that denies the launch of large EC2 instances\" is the correct answer. INCORRECT: \"Create a service-linked role for Amazon EC2 and attach a policy the denies the launch of large EC2 instances\" is incorrect. You cannot create service-linked roles yourself; they are created by AWS with predefined policies. INCORRECT: \"Create a resource-based policy that denies the launch of large EC2 instances and attach it to Amazon EC2 in each account\" is incorrect. You cannot attach a resource-based policy to Amazon EC2. INCORRECT: \"Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role\" is incorrect. This is much less operationally efficient compared to using SCPs with AWS Organizations. INCORRECT: \"Create a service-linked role for Amazon EC2 and attach a policy the denies the launch of large EC2 instances\" is incorrect. INCORRECT: \"Create a resource-based policy that denies the launch of large EC2 instances and attach it to Amazon EC2 in each account\" is incorrect. INCORRECT: \"Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role\" is incorrect."
    },
    {
      "id": "840",
      "question": "A Solutions Architect is designing an application that will run on Amazon EC2 instances. The application will use Amazon S3 for storing image files and an Amazon DynamoDB table for storing customer information. The security team require that traffic between the EC2 instances and AWS services must not traverse the public internet. How can the Solutions Architect meet the security team’s requirements?",
      "options": {
        "A": "Create a virtual private gateway and configure VPC route tables.",
        "B": "Create interface VPC endpoints for Amazon S3 and DynamoDB.",
        "C": "Create gateway VPC endpoints for Amazon S3 and DynamoDB.",
        "D": "Create a NAT gateway in a public subnet and configure route tables."
      },
      "correct_answer": "C",
      "explanation": "A VPC endpoint enables private connections between your VPC and supported AWS services and VPC endpoint services powered by AWS PrivateLink. A gateway endpoint is used for Amazon S3 and Amazon DynamoDB. You specify a gateway endpoint as a route table target for traffic that is destined for the supported AWS services. CORRECT: \"Create gateway VPC endpoints for Amazon S3 and DynamoDB\" is the correct answer. INCORRECT: \"Create a NAT gateway in a public subnet and configure route tables\" is incorrect. A NAT gateway is used for enabling internet connectivity for instances in private subnets. Connections will traverse the internet. INCORRECT: \"Create interface VPC endpoints for Amazon S3 and DynamoDB\" is incorrect. You should use a gateway VPC endpoint for S3 and DynamoDB. INCORRECT: \"Create a virtual private gateway and configure VPC route tables\" is incorrect VGWs are used for VPN connections, they do not allow access to AWS services from a VPC. INCORRECT: \"Create a NAT gateway in a public subnet and configure route tables\" is incorrect. INCORRECT: \"Create interface VPC endpoints for Amazon S3 and DynamoDB\" is incorrect. INCORRECT: \"Create a virtual private gateway and configure VPC route tables\" is incorrect VGWs are used for VPN connections, they do not allow access to AWS services from a VPC."
    },
    {
      "id": "841",
      "question": "A digital media company uses an Amazon RDS MySQL instance for its content management system. Recently, the company has observed that their RDS instance is nearing its storage capacity due to the constant influx of new data. The company wants to ensure there's always sufficient storage without any operational interruption or manual intervention. Which solution should the company use to address this situation with the LEAST operational overhead?",
      "options": {
        "A": "Enable automatic storage scaling for the MySQL instance.",
        "B": "Migrate the database to a larger Amazon RDS MySQL instance.",
        "C": "Implement a lifecycle policy to delete older data from the MySQL instance.",
        "D": "Utilize Amazon ElastiCache to offload some read traffic and reduce database load."
      },
      "correct_answer": "A",
      "explanation": "Amazon RDS's automatic storage scaling allows the database to automatically increase its storage capacity when the available storage is low. This feature helps to prevent out-of-storage situations and requires no operational overhead. CORRECT: \"Enable automatic storage scaling for the MySQL instance\" is the correct answer (as explained above.) INCORRECT: \"Migrate the database to a larger Amazon RDS MySQL instance\" is incorrect. While this would provide more storage, it does not address the issue of potential future storage shortages and requires significant operational effort for the migration. INCORRECT: \"Implement a lifecycle policy to delete older data from the MySQL instance\" is incorrect. While this might help free up some storage, it might not be suitable if all data is essential for business operations. Also, this does not provide a long-term solution if data growth continues. INCORRECT: \"Utilize Amazon ElastiCache to offload some read traffic and reduce database load\" is incorrect. While ElastiCache can help to improve the database's read efficiency, it doesn't directly address the disk space concern for the RDS instance. INCORRECT: \"Migrate the database to a larger Amazon RDS MySQL instance\" is incorrect. INCORRECT: \"Implement a lifecycle policy to delete older data from the MySQL instance\" is incorrect. INCORRECT: \"Utilize Amazon ElastiCache to offload some read traffic and reduce database load\" is incorrect."
    },
    {
      "id": "842",
      "question": "An application allows users to upload and download files. Files older than 2 years will be accessed less frequently. A solutions architect needs to ensure that the application can scale to any number of files while maintaining high availability and durability. Which scalable solutions should the solutions architect recommend?",
      "options": {
        "A": "Store the files on Amazon Elastic File System (EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA)",
        "B": "Store the files in Amazon Elastic Block Store (EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2 years",
        "C": "Store the files in Amazon Elastic Block Store (EBS) volumes. Create a lifecycle policy to move files older than 2 years to Amazon S3 Glacier",
        "D": "Store the files on Amazon S3 with a lifecycle policy that moves objects older than 2 years to S3 Standard Infrequent Access (S3 Standard-IA)"
      },
      "correct_answer": "D",
      "explanation": "S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. CORRECT: \"Store the files on Amazon S3 with a lifecycle policy that moves objects older than 2 years to S3 Standard Infrequent Access (S3 Standard-IA)\" is the correct answer. INCORRECT: \"Store the files on Amazon Elastic File System (EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA)\" is incorrect. With EFS you can transition files to EFS IA after a file has not been accessed for a specified period of time with options up to 90 days. You cannot transition based on an age of 2 years. INCORRECT: \"Store the files in Amazon Elastic Block Store (EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2 years\" is incorrect. You cannot identify the age of data and archive snapshots in this way with EBS. INCORRECT: \"Store the files in Amazon Elastic Block Store (EBS) volumes. Create a lifecycle policy to move files older than 2 years to Amazon S3 Glacier\" is incorrect. You cannot archive files from an EBS volume to Glacier using lifecycle policies. INCORRECT: \"Store the files on Amazon Elastic File System (EFS) with a lifecycle policy that moves objects older than 2 years to EFS Infrequent Access (EFS IA)\" is incorrect. INCORRECT: \"Store the files in Amazon Elastic Block Store (EBS) volumes. Schedule snapshots of the volumes. Use the snapshots to archive data older than 2 years\" is incorrect. INCORRECT: \"Store the files in Amazon Elastic Block Store (EBS) volumes. Create a lifecycle policy to move files older than 2 years to Amazon S3 Glacier\" is incorrect."
    },
    {
      "id": "843",
      "question": "An application generates unique files that are returned to customers after they submit requests to the application. The application uses an Amazon CloudFront distribution for sending the files to customers. The company wishes to reduce data transfer costs without modifying the application. How can this be accomplished?",
      "options": {
        "A": "Enable Amazon S3 Transfer Acceleration to reduce the transfer times.",
        "B": "Use Lambda@Edge to compress the files as they are sent to users.",
        "C": "Use AWS Global Accelerator to reduce application latency for customers.",
        "D": "Enable caching on the CloudFront distribution to store generated files at the edge."
      },
      "correct_answer": "B",
      "explanation": "Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. Lambda@Edge runs code in response to events generated by the Amazon CloudFront. You simply upload your code to AWS Lambda, and it takes care of everything required to run and scale your code with high availability at an AWS location closest to your end user. In this case Lambda@Edge can compress the files before they are sent to users which will reduce data egress costs. CORRECT: \"Use Lambda@Edge to compress the files as they are sent to users\" is the correct answer. INCORRECT: \"Enable caching on the CloudFront distribution to store generated files at the edge\" is incorrect. The files are unique to each customer request, so caching does not help. INCORRECT: \"Use AWS Global Accelerator to reduce application latency for customers\" is incorrect. The aim is to reduce cost not latency and AWS GA uses the same network as CloudFront so does not assist with latency anyway. INCORRECT: \"Enable Amazon S3 Transfer Acceleration to reduce the transfer times\" is incorrect. This does not lower costs. INCORRECT: \"Enable caching on the CloudFront distribution to store generated files at the edge\" is incorrect. INCORRECT: \"Use AWS Global Accelerator to reduce application latency for customers\" is incorrect. INCORRECT: \"Enable Amazon S3 Transfer Acceleration to reduce the transfer times\" is incorrect."
    },
    {
      "id": "844",
      "question": "A media company hosts several terabytes of multimedia content across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's marketing team needs to securely access and analyze selective data from various accounts for targeted advertisement campaigns. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Replicate the required data to a shared account. Create an IAM access role in that account. Grant access by defining a permission policy that includes users from the marketing team accounts as trusted entities.",
        "B": "Utilize Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the marketing team accounts.",
        "C": "Use AWS DataSync to synchronize the necessary data to the marketing team accounts.",
        "D": "Use the Lake Formation permissions Grant command in each account where the data is stored to permit the required marketing team users to access the data."
      },
      "correct_answer": "B",
      "explanation": "With Lake Formation tag-based access control, you can manage permissions using tags and grant cross-account permissions, which would meet the requirements with the least operational overhead. CORRECT: \"Utilize Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the marketing team accounts\" is the correct answer (as explained above.) INCORRECT: \"Replicate the required data to a shared account. Create an IAM access role in that account. Grant access by defining a permission policy that includes users from the marketing team accounts as trusted entities\" is incorrect. This solution involves the unnecessary replication of data, leading to increased storage costs and operational overhead. INCORRECT: \"Use the Lake Formation permissions Grant command in each account where the data is stored to permit the required marketing team users to access the data\" is incorrect. The Grant command would need to be manually executed in each account where data is stored, which could lead to increased operational overhead, particularly if the data is spread across many accounts. INCORRECT: \"Use AWS DataSync to synchronize the necessary data to the marketing team accounts\" is incorrect. AWS DataSync is designed for online data transfer, not for granting access permissions to data already stored in AWS, so this would not meet the requirement. INCORRECT: \"Replicate the required data to a shared account. Create an IAM access role in that account. Grant access by defining a permission policy that includes users from the marketing team accounts as trusted entities\" is incorrect. INCORRECT: \"Use the Lake Formation permissions Grant command in each account where the data is stored to permit the required marketing team users to access the data\" is incorrect. INCORRECT: \"Use AWS DataSync to synchronize the necessary data to the marketing team accounts\" is incorrect."
    },
    {
      "id": "845",
      "question": "A company is planning to use Amazon S3 to store documents uploaded by its customers. The images must be encrypted at rest in Amazon S3. The company does not want to spend time managing and rotating the keys, but it does want to control who can access those keys. What should a solutions architect use to accomplish this?",
      "options": {
        "A": "Server-Side Encryption with Customer-Provided Keys (SSE-C)",
        "B": "Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)",
        "C": "Server-Side Encryption with keys stored in an S3 bucket",
        "D": "Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)"
      },
      "correct_answer": "D",
      "explanation": "SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. Customer managed CMKs are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the CMK, and scheduling the CMKs for deletion. For this scenario, the solutions architect should use SSE-KMS with a customer managed CMK. That way KMS will manage the data key but the company can configure key policies defining who can access the keys. CORRECT: \"Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)\" is the correct answer. INCORRECT: \"Server-Side Encryption with keys stored in an S3 bucket\" is incorrect as you cannot store your keys in a bucket with server-side encryption INCORRECT: \"Server-Side Encryption with Customer-Provided Keys (SSE-C)\" is incorrect as the company does not want to manage the keys. INCORRECT: \"Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\" is incorrect as the company needs to manage access control for the keys which is not possible when they’re managed by Amazon. INCORRECT: \"Server-Side Encryption with keys stored in an S3 bucket\" is incorrect as you cannot store your keys in a bucket with server-side encryption INCORRECT: \"Server-Side Encryption with Customer-Provided Keys (SSE-C)\" is incorrect as the company does not want to manage the keys. INCORRECT: \"Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\" is incorrect as the company needs to manage access control for the keys which is not possible when they’re managed by Amazon."
    },
    {
      "id": "846",
      "question": "A company is deploying an analytics application on AWS Fargate. The application requires connected storage that offers concurrent access to files and high performance. Which storage option should the solutions architect recommend?",
      "options": {
        "A": "1. Create an Amazon EFS file share and establish an IAM role that allows Fargate to communicate with Amazon EFS.",
        "B": "1. Create an Amazon FSx for Lustre file share and establish an IAM role that allows Fargate to communicate with FSx for Lustre.",
        "C": "1. Create an Amazon EBS volume for the application and establish an IAM role that allows Fargate to communicate with Amazon EBS.",
        "D": "1. Create an Amazon S3 bucket for the application and establish an IAM role for Fargate to communicate with Amazon S3."
      },
      "correct_answer": "A",
      "explanation": "The Amazon Elastic File System offers concurrent access to a shared file system and provides high performance. You can create file system policies for controlling access and then use an IAM role that is specified in the policy for access. CORRECT: \"Create an Amazon EFS file share and establish an IAM role that allows Fargate to communicate with Amazon EFS\" is the correct answer. INCORRECT: \"Create an Amazon S3 bucket for the application and establish an IAM role for Fargate to communicate with Amazon S3\" is incorrect. S3 uses a REST API not a file system API so access can be shared but is not concurrent. INCORRECT: \"Create an Amazon EBS volume for the application and establish an IAM role that allows Fargate to communicate with Amazon EBS\" is incorrect. EBS volumes cannot be shared amongst Fargate tasks, they are used with EC2 instances. INCORRECT: \"Create an Amazon FSx for Lustre file share and establish an IAM role that allows Fargate to communicate with FSx for Lustre\" is incorrect. It is not supported to connect Fargate to FSx for Lustre. INCORRECT: \"Create an Amazon S3 bucket for the application and establish an IAM role for Fargate to communicate with Amazon S3\" is incorrect. INCORRECT: \"Create an Amazon EBS volume for the application and establish an IAM role that allows Fargate to communicate with Amazon EBS\" is incorrect. INCORRECT: \"Create an Amazon FSx for Lustre file share and establish an IAM role that allows Fargate to communicate with FSx for Lustre\" is incorrect."
    },
    {
      "id": "847",
      "question": "An application uses a MySQL database running on an Amazon EC2 instance. The application generates high I/O and constant writes to a single table on the database. Which Amazon EBS volume type will provide the MOST consistent performance and low latency?",
      "options": {
        "A": "Throughput Optimized HDD (st1)",
        "B": "Cold HDD (sc1)",
        "C": "Provisioned IOPS SSD (io1)",
        "D": "General Purpose SSD (gp2)"
      },
      "correct_answer": "C",
      "explanation": "The Provisioned IOPS SSD (io1) volume type will offer the most consistent performance and can be configured with the amount of IOPS required by the application. It will also provide the lowest latency of the options presented. CORRECT: \"Provisioned IOPS SSD (io1)\" is the correct answer. INCORRECT: \"General Purpose SSD (gp2)\" is incorrect. This is not the best solution for when you require high I/O, consistent performance and low latency. INCORRECT: \"Throughput Optimized HDD (st1)\" is incorrect. This is a HDD type of disk and not suitable for low latency workloads that require consistent performance. INCORRECT: \"Cold HDD (sc1)\" is incorrect. This is the lowest cost option and not suitable for frequently accessed workloads. INCORRECT: \"General Purpose SSD (gp2)\" is incorrect. INCORRECT: \"Throughput Optimized HDD (st1)\" is incorrect. INCORRECT: \"Cold HDD (sc1)\" is incorrect."
    },
    {
      "id": "848",
      "question": "A company needs to ensure that they can failover between AWS Regions in the event of a disaster seamlessly with minimal downtime and data loss. The applications will run in an active-active configuration. Which DR strategy should a Solutions Architect recommend?",
      "options": {
        "A": "Multi-site",
        "B": "Warm standby",
        "C": "Backup and restore",
        "D": "Pilot light"
      },
      "correct_answer": "A",
      "explanation": "A multi-site solution runs on AWS as well as on your existing on-site infrastructure in an active- active configuration. The data replication method that you employ will be determined by the recovery point that you choose. This is either Recovery Time Objective (the maximum allowable downtime before degraded operations are restored) or Recovery Point Objective (the maximum allowable time window whereby you will accept the loss of transactions during the DR process). CORRECT: \"Multi-site\" is the correct answer. INCORRECT: \"Backup and restore\" is incorrect. This is the lowest cost DR approach that simply entails creating online backups of all data and applications. INCORRECT: \"Pilot light\" is incorrect. With a pilot light strategy a core minimum of services are running and the remainder are only brought online during a disaster recovery situation. INCORRECT: \"Warm standby\" is incorrect. The term warm standby is used to describe a DR scenario in which a scaled-down version of a fully functional environment is always running in the cloud. INCORRECT: \"Backup and restore\" is incorrect. INCORRECT: \"Pilot light\" is incorrect. INCORRECT: \"Warm standby\" is incorrect."
    },
    {
      "id": "849",
      "question": "A cloud architect is assessing the resilience of a web application deployed on AWS. It was observed that the application experienced a downtime of about 3 minutes when a scheduled failover was performed on the application's Amazon RDS MySQL database as part of a scaling operation. The organization wants to mitigate such downtime in future scaling exercises while minimizing operational overhead. Which solution will be the MOST effective in achieving this?",
      "options": {
        "A": "Configure an Amazon RDS Proxy for the database and modify the application to connect to the proxy endpoint.",
        "B": "Implement more RDS MySQL read replicas in the cluster to manage the load during the failover.",
        "C": "Implement an Amazon ElastiCache for Redis cluster to manage the load during the failover.",
        "D": "Establish a secondary RDS MySQL cluster within the same AWS Region. During any future failover, modify the application to connect to the secondary cluster's writer endpoint."
      },
      "correct_answer": "A",
      "explanation": "Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon RDS that makes applications more scalable, more resilient to database failures, and more secure. During a failover, RDS Proxy automatically connects to a standby database instance while preserving connections from your application and reducing failover times for RDS and Aurora multi-AZ databases. So, there is minimal downtime for the application. CORRECT: \"Configure an Amazon RDS Proxy for the database and modify the application to connect to the proxy endpoint\" is the correct answer (as explained above.) INCORRECT: \"Implement more RDS MySQL read replicas in the cluster to manage the load during the failover\" is incorrect. Adding more read replicas to the cluster does not decrease the downtime during a failover. It only improves the database's ability to handle read-heavy workloads. Read replicas do not contribute to a faster failover process. INCORRECT: \"Establish a secondary RDS MySQL cluster within the same AWS Region. During any future failover, modify the application to connect to the secondary cluster's writer endpoint\" is incorrect. This approach is operationally heavy as it involves managing two separate RDS clusters and manually updating the application's database endpoint during a failover. Moreover, it does not necessarily reduce the downtime during a failover as there might be data inconsistency issues between the primary and secondary clusters, depending on the replication latency. INCORRECT: \"Implement an Amazon ElastiCache for Redis cluster to manage the load during the failover\" is incorrect. ElastiCache is an in-memory cache and not a relational database service. It is typically used to cache frequently accessed data to reduce latency and improve application performance, not for managing failovers. INCORRECT: \"Implement more RDS MySQL read replicas in the cluster to manage the load during the failover\" is incorrect. INCORRECT: \"Establish a secondary RDS MySQL cluster within the same AWS Region. During any future failover, modify the application to connect to the secondary cluster's writer endpoint\" is incorrect. INCORRECT: \"Implement an Amazon ElastiCache for Redis cluster to manage the load during the failover\" is incorrect."
    },
    {
      "id": "850",
      "question": "A web application in a three-tier architecture runs on a fleet of Amazon EC2 instances. Performance issues have been reported and investigations point to insufficient swap space. The operations team requires monitoring to determine if this is correct. What should a solutions architect recommend?",
      "options": {
        "A": "Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor SwapUsage metrics in CloudWatch",
        "B": "Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric. Monitor SwapUtilization metrics in CloudWatch",
        "C": "Install an Amazon CloudWatch agent on the instances. Run an appropriate script on a set schedule. Monitor SwapUtilization metrics in CloudWatch",
        "D": "Configure an Amazon CloudWatch SwapUsage metric dimension. Monitor the SwapUsage dimension in the EC2 metrics in CloudWatch"
      },
      "correct_answer": "C",
      "explanation": "You can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. There is now a unified agent and previously there were monitoring scripts. Both of these tools can capture SwapUtilization metrics and send them to CloudWatch. This is the best way to get memory utilization metrics from Amazon EC2 instnaces. CORRECT: \"Install an Amazon CloudWatch agent on the instances. Run an appropriate script on a set schedule. Monitor SwapUtilization metrics in CloudWatch\" is the correct answer. INCORRECT: \"Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric. Monitor SwapUtilization metrics in CloudWatch\" is incorrect as you do not create custom metrics in the console, you must configure the instances to send the metric information to CloudWatch. INCORRECT: \"Configure an Amazon CloudWatch SwapUsage metric dimension. Monitor the SwapUsage dimension in the EC2 metrics in CloudWatch\" is incorrect as there is no SwapUsage metric in CloudWatch. All memory metrics must be custom metrics. INCORRECT: \"Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor SwapUsage metrics in CloudWatch\" is incorrect as performance related information is not stored in metadata. INCORRECT: \"Enable detailed monitoring in the EC2 console. Create an Amazon CloudWatch SwapUtilization custom metric. Monitor SwapUtilization metrics in CloudWatch\" is incorrect as you do not create custom metrics in the console, you must configure the instances to send the metric information to CloudWatch. INCORRECT: \"Configure an Amazon CloudWatch SwapUsage metric dimension. Monitor the SwapUsage dimension in the EC2 metrics in CloudWatch\" is incorrect as there is no SwapUsage metric in CloudWatch. INCORRECT: \"Use EC2 metadata to collect information, then publish it to Amazon CloudWatch custom metrics. Monitor SwapUsage metrics in CloudWatch\" is incorrect as performance related information is not stored in metadata."
    },
    {
      "id": "851",
      "question": "An application makes calls to a REST API running on Amazon EC2 instances behind an Application Load Balancer (ALB). Most API calls complete quickly. However, a single endpoint is making API calls that require much longer to complete and this is introducing overall latency into the system. What steps can a Solutions Architect take to minimize the effects of the long-running API calls?",
      "options": {
        "A": "Increase the ALB idle timeout to allow the long-running requests to complete",
        "B": "Change the EC2 instance to one with enhanced networking to reduce latency",
        "C": "Change the ALB to a Network Load Balancer (NLB) and use SSL/TLS termination",
        "D": "Create an Amazon SQS queue and decouple the long-running API calls"
      },
      "correct_answer": "D",
      "explanation": "An Amazon Simple Queue Service (SQS) can be used to offload and decouple the long-running requests. They can then be processed asynchronously by separate EC2 instances. This is the best way to reduce the overall latency introduced by the long-running API call. CORRECT: \"Create an Amazon SQS queue and decouple the long-running API calls\" is the correct answer. INCORRECT: \"Change the EC2 instance to one with enhanced networking to reduce latency\" is incorrect. This will not reduce the latency of the API call as network latency is not the issue here, it is the latency of how long the API call takes to complete. INCORRECT: \"Increase the ALB idle timeout to allow the long-running requests to complete\" is incorrect. The issue is not the connection being interrupted, it is that the API call takes a long time to complete. INCORRECT: \"Change the ALB to a Network Load Balancer (NLB) and use SSL/TLS termination\" is incorrect. SSL/TLS termination is not of benefit here as the problem is not encryption or processing of encryption. The issue is API call latency. INCORRECT: \"Change the EC2 instance to one with enhanced networking to reduce latency\" is incorrect. INCORRECT: \"Increase the ALB idle timeout to allow the long-running requests to complete\" is incorrect. INCORRECT: \"Change the ALB to a Network Load Balancer (NLB) and use SSL/TLS termination\" is incorrect."
    },
    {
      "id": "852",
      "question": "A High Performance Computing (HPC) application needs storage that can provide 135,000 IOPS. The storage layer is replicated across all instances in a cluster. What is the optimal storage solution that provides the required performance and is cost-effective?",
      "options": {
        "A": "Use Amazon Instance Store",
        "B": "Use Amazon EC2 Enhanced Networking with an EBS HDD Throughput Optimized volume",
        "C": "Use Amazon S3 with byte-range fetch",
        "D": "Use Amazon EBS Provisioned IOPS volume with 135,000 IOPS"
      },
      "correct_answer": "A",
      "explanation": "Instance stores offer very high performance and low latency. As long as you can afford to lose an instance, i.e. you are replicating your data, these can be a good solution for high performance/low latency requirements. Also, the cost of instance stores is included in the instance charges so it can also be more cost-effective than EBS Provisioned IOPS. CORRECT: \"Use Amazon Instance Store\" is the correct answer. INCORRECT: \"Use Amazon EBS Provisioned IOPS volume with 135,000 IOPS\" is incorrect. In the case of a HPC cluster that replicates data between nodes you don’t necessarily need a shared storage solution such as Amazon EBS Provisioned IOPS – this would also be a more expensive solution as the Instance Store is included in the cost of the HPC instance. INCORRECT: \"Use Amazon S3 with byte-range fetch\" is incorrect. Amazon S3 is not a solution for this HPC application as in this case it will require block-based storage to provide the required IOPS. INCORRECT: \"Enhanced networking provides higher bandwidth and lower latency and is implemented using an Elastic Network Adapter (ENA). However, using an ENA with an HDD Throughput Optimized volume is not recommended and the volume will not provide the performance required for this use case.\" is incorrect INCORRECT: \"Use Amazon EBS Provisioned IOPS volume with 135,000 IOPS\" is incorrect. INCORRECT: \"Use Amazon S3 with byte-range fetch\" is incorrect. INCORRECT: \"Enhanced networking provides higher bandwidth and lower latency and is implemented using an Elastic Network Adapter (ENA). However, using an ENA with an HDD Throughput Optimized volume is not recommended and the volume will not provide the performance required for this use case.\" is incorrect References: https://docs."
    },
    {
      "id": "853",
      "question": "A new application will be launched on an Amazon EC2 instance with an Elastic Block Store (EBS) volume. A solutions architect needs to determine the most cost-effective storage option. The application will have infrequent usage, with peaks of traffic for a couple of hours in the morning and evening. Disk I/O is variable with peaks of up to 3,000 IOPS. Which solution should the solutions architect recommend?",
      "options": {
        "A": "Amazon EBS Provisioned IOPS SSD (io1)",
        "B": "Amazon EBS Cold HDD (sc1)",
        "C": "Amazon EBS Throughput Optimized HDD (st1)",
        "D": "Amazon EBS General Purpose SSD (gp2)"
      },
      "correct_answer": "D",
      "explanation": "General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB. In this case the volume would have a baseline performance of 3 x 200 = 600 IOPS. The volume could also burst to 3,000 IOPS for extended periods. As the I/O varies, this should be suitable. CORRECT: \"Amazon EBS General Purpose SSD (gp2)\" is the correct answer. INCORRECT: \"Amazon EBS Provisioned IOPS SSD (io1) \" is incorrect as this would be a more expensive option and is not required for the performance characteristics of this workload. INCORRECT: \"Amazon EBS Cold HDD (sc1)\" is incorrect as there is no IOPS SLA for HDD volumes and they would likely not perform well enough for this workload. INCORRECT: \"Amazon EBS Throughput Optimized HDD (st1)\" is incorrect as there is no IOPS SLA for HDD volumes and they would likely not perform well enough for this workload. INCORRECT: \"Amazon EBS Provisioned IOPS SSD (io1) \" is incorrect as this would be a more expensive option and is not required for the performance characteristics of this workload. INCORRECT: \"Amazon EBS Cold HDD (sc1)\" is incorrect as there is no IOPS SLA for HDD volumes and they would likely not perform well enough for this workload. INCORRECT: \"Amazon EBS Throughput Optimized HDD (st1)\" is incorrect as there is no IOPS SLA for HDD volumes and they would likely not perform well enough for this workload."
    },
    {
      "id": "854",
      "question": "A company has refactored a legacy application to run as two microservices using Amazon ECS. The application processes data in two parts and the second part of the process takes longer than the first. How can a solutions architect integrate the microservices and allow them to scale independently?",
      "options": {
        "A": "Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in microservice 2 to process messages from the queue",
        "B": "Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2",
        "C": "Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic",
        "D": "Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose"
      },
      "correct_answer": "A",
      "explanation": "This is a good use case for Amazon SQS. The microservices must be decoupled so they can scale independently. An Amazon SQS queue will enable microservice 1 to add messages to the queue. Microservice 2 can then pick up the messages and process them. This ensures that if there’s a spike in traffic on the frontend, messages do not get lost due to the backend process not being ready to process them. CORRECT: \"Implement code in microservice 1 to send data to an Amazon SQS queue. Implement code in microservice 2 to process messages from the queue\" is the correct answer. INCORRECT: \"Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2\" is incorrect as a message queue would be preferable to an S3 bucket. INCORRECT: \"Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic\" is incorrect as notifications to topics are pushed to subscribers. In this case we want the second microservice to pickup the messages when ready (pull them). INCORRECT: \"Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose\" is incorrect as this is not how Firehose works. Firehose sends data directly to destinations, it is not a message queue. INCORRECT: \"Implement code in microservice 1 to send data to an Amazon S3 bucket. Use S3 event notifications to invoke microservice 2\" is incorrect as a message queue would be preferable to an S3 bucket. INCORRECT: \"Implement code in microservice 1 to publish data to an Amazon SNS topic. Implement code in microservice 2 to subscribe to this topic\" is incorrect as notifications to topics are pushed to subscribers. INCORRECT: \"Implement code in microservice 1 to send data to Amazon Kinesis Data Firehose. Implement code in microservice 2 to read from Kinesis Data Firehose\" is incorrect as this is not how Firehose works."
    },
    {
      "id": "855",
      "question": "A financial institution wants to use machine learning (ML) algorithms to detect potential fraudulent transactions. They need to create ML models based on their vast financial transaction data and integrate these models into their business intelligence system for real-time decision-making. The solution should require minimal operational overhead. Which solution will best meet these requirements?",
      "options": {
        "A": "Use Amazon Comprehend for analyzing the transaction data and Amazon Elasticsearch for visualization.",
        "B": "1. Use AWS Glue to perform ETL jobs on the transaction data and use Amazon Forecast for predictive analytics.",
        "C": "Use Amazon SageMaker to build, train, and deploy ML models, and use Amazon QuickSight for data visualization.",
        "D": "Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models and use AWS Athena for data visualization."
      },
      "correct_answer": "C",
      "explanation": "Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. It can directly connect with data sources and has built-in algorithms to ease the ML process. Amazon QuickSight is a business intelligence tool that can be used to create dashboards for data visualization. This combination perfectly suits the requirement. CORRECT: \"Use Amazon SageMaker to build, train, and deploy ML models, and use Amazon QuickSight for data visualization\" is the correct answer (as explained above.) INCORRECT: \"Use AWS Glue to perform ETL jobs on the transaction data and use Amazon Forecast for predictive analytics\" is incorrect. AWS Glue is primarily used for ETL jobs - cleaning, preparing, and moving data. Amazon Forecast is a fully managed service for time-series forecasting, which might not be a complete solution for detecting fraudulent transactions. INCORRECT: \"Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models and use AWS Athena for data visualization\" is incorrect. AWS Marketplace ML AMIs can be used to create and train models, but this will require manual operational effort in terms of setting up and managing the instances. Athena is a query service and does not provide data visualization capabilities that a business intelligence tool like QuickSight provides. INCORRECT: \"Use Amazon Comprehend for analyzing the transaction data and Amazon Elasticsearch for visualization\" is incorrect. Amazon Comprehend is primarily used for natural language processing (NLP), which isn't suited for detecting fraudulent transactions. Elasticsearch is a search and analytics engine and might not be the best tool for the use case described here. INCORRECT: \"Use AWS Glue to perform ETL jobs on the transaction data and use Amazon Forecast for predictive analytics\" is incorrect. INCORRECT: \"Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models and use AWS Athena for data visualization\" is incorrect. INCORRECT: \"Use Amazon Comprehend for analyzing the transaction data and Amazon Elasticsearch for visualization\" is incorrect."
    },
    {
      "id": "856",
      "question": "A web application is running on a fleet of Amazon EC2 instances using an Auto Scaling Group. It is desired that the CPU usage in the fleet is kept at 40%. How should scaling be configured?",
      "options": {
        "A": "Use a step scaling policy that uses the PercentChangeInCapacity value to adjust the group size as required",
        "B": "Use a custom CloudWatch alarm to monitor CPU usage and notify the ASG using Amazon SNS",
        "C": "Use a target tracking policy that keeps the average aggregate CPU utilization at 40%",
        "D": "Use a simple scaling policy that launches instances when the average CPU hits 40%"
      },
      "correct_answer": "C",
      "explanation": "This is a perfect use case for a target tracking scaling policy. With target tracking scaling policies, you select a scaling metric and set a target value. In this case you can just set the target value to 40% average aggregate CPU utilization. CORRECT: \"Use a target tracking policy that keeps the average aggregate CPU utilization at 40%\" is the correct answer. INCORRECT: \"Use a simple scaling policy that launches instances when the average CPU hits 40%\" is incorrect. A simple scaling policy will add instances when 40% CPU utilization is reached, but it is not designed to maintain 40% CPU utilization across the group. INCORRECT: \"Use a step scaling policy that uses the PercentChangeInCapacity value to adjust the group size as required\" is incorrect. The step scaling policy makes scaling adjustments based on a number of factors. The PercentChangeInCapacity value increments or decrements the group size by a specified percentage. This does not relate to CPU utilization. INCORRECT: \"Use a custom CloudWatch alarm to monitor CPU usage and notify the ASG using Amazon SNS\" is incorrect. You do not need to create a custom Amazon CloudWatch alarm as the ASG can scale using a policy based on CPU utilization using standard configuration. INCORRECT: \"Use a simple scaling policy that launches instances when the average CPU hits 40%\" is incorrect. INCORRECT: \"Use a step scaling policy that uses the PercentChangeInCapacity value to adjust the group size as required\" is incorrect. INCORRECT: \"Use a custom CloudWatch alarm to monitor CPU usage and notify the ASG using Amazon SNS\" is incorrect."
    },
    {
      "id": "857",
      "question": "A financial services company is migrating its sensitive customer data and applications to AWS. They want to ensure that the data is securely stored and managed while reducing the overall maintenance and operational overhead associated with managing databases. Which solution will meet these requirements?",
      "options": {
        "A": "Migrate the data and applications to Amazon RDS instances. Enable encryption at rest using AWS Key Management Service (AWS KMS).",
        "B": "Migrate the data to Amazon RDS instances. Enable Amazon GuardDuty for data protection and threat detection.",
        "C": "Migrate the applications and data to Amazon EC2 instances. Utilize the AWS Key Management Service (AWS KMS) customer managed keys for encryption.",
        "D": "Store the data in Amazon S3. Utilize Amazon Macie for ongoing data security and threat detection."
      },
      "correct_answer": "A",
      "explanation": "Amazon RDS makes it easy to go from project conception to deployment by managing time-consuming database administration tasks including backups, software patching, monitoring, scaling, and replication. Amazon RDS supports encryption at rest, which ensures the security of sensitive data and meets regulatory compliance requirements. AWS Key Management Service (AWS KMS) is integrated with Amazon RDS to make it easier to create, control, and manage keys for encryption. CORRECT: \"Migrate the data and applications to Amazon RDS instances. Enable encryption at rest using AWS Key Management Service (AWS KMS)\" is the correct answer (as explained above.) INCORRECT: \"Migrate the applications and data to Amazon EC2 instances. Utilize the AWS Key Management Service (AWS KMS) customer managed keys for encryption\" is incorrect. While this solution offers data encryption, it does not meet the requirement to reduce operational overhead. Managing databases on EC2 instances requires additional administrative tasks, such as managing backups and applying software patches, which Amazon RDS handles automatically. INCORRECT: \"Store the data in Amazon S3. Utilize Amazon Macie for ongoing data security and threat detection\" is incorrect. Amazon S3 and Macie are suitable for data storage and security analysis, respectively. However, Amazon S3 is not designed to serve as a transactional database for applications, which is a key requirement in this scenario. INCORRECT: \"Migrate the data to Amazon RDS instances. Enable Amazon GuardDuty for data protection and threat detection\" is incorrect. While Amazon RDS is a correct choice for database management and Amazon GuardDuty offers threat detection, GuardDuty is not specifically designed for data protection within databases. It's a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. INCORRECT: \"Migrate the applications and data to Amazon EC2 instances. Utilize the AWS Key Management Service (AWS KMS) customer managed keys for encryption\" is incorrect. INCORRECT: \"Store the data in Amazon S3. Utilize Amazon Macie for ongoing data security and threat detection\" is incorrect. INCORRECT: \"Migrate the data to Amazon RDS instances. Enable Amazon GuardDuty for data protection and threat detection\" is incorrect."
    },
    {
      "id": "858",
      "question": "A company has launched a multi-tier application architecture. The web tier and database tier run on Amazon EC2 instances in private subnets within the same Availability Zone. Which combination of steps should a Solutions Architect take to add high availability to this architecture? (Select TWO.)",
      "options": {
        "A": "Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs",
        "B": "Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)",
        "C": "Create new private subnets in the same VPC but in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment",
        "D": "Create new public subnets in the same AZ for high availability and move the web tier to the public subnets"
      },
      "correct_answer": "A,C",
      "explanation": "The Solutions Architect can use Auto Scaling group across multiple AZs with an ALB in front to create an elastic and highly available architecture. Then, migrate the database to an Amazon RDS multi-AZ deployment to create HA for the database tier. This results in a fully redundant architecture that can withstand the failure of an availability zone. CORRECT: \"Create an Amazon EC2 Auto Scaling group and Application Load Balancer (ALB) spanning multiple AZs\" is a correct answer. CORRECT: \"Create new private subnets in the same VPC but in a different AZ. Migrate the database to an Amazon RDS multi-AZ deployment\" is also a correct answer. INCORRECT: \"Create new public subnets in the same AZ for high availability and move the web tier to the public subnets\" is incorrect. If subnets share the same AZ they are not suitable for splitting your tier across them for HA as the failure of a an AZ will take out both subnets. INCORRECT: \"Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)\" is incorrect. The instances are in a single AZ so the Solutions Architect should create a new auto scaling group and launch instances across multiple AZs. INCORRECT: \"Create new private subnets in the same VPC but in a different AZ. Create a database using Amazon EC2 in one AZ\" is incorrect. A database in a single AZ will not be highly available. INCORRECT: \"Create new public subnets in the same AZ for high availability and move the web tier to the public subnets\" is incorrect. INCORRECT: \"Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer (ALB)\" is incorrect. INCORRECT: \"Create new private subnets in the same VPC but in a different AZ. Create a database using Amazon EC2 in one AZ\" is incorrect."
    },
    {
      "id": "859",
      "question": "A production application runs on an Amazon RDS MySQL DB instance. A solutions architect is building a new reporting tool that will access the same data. The reporting tool must be highly available and not impact the performance of the production application. How can this be achieved?",
      "options": {
        "A": "Create a Single-AZ RDS Read Replica of the production RDS DB instance. Create a second Single-AZ RDS Read Replica from the replica",
        "B": "Use Amazon Data Lifecycle Manager to automatically create and manage snapshots",
        "C": "Create a cross-region Multi-AZ deployment and create a read replica in the second region",
        "D": "Create a Multi-AZ RDS Read Replica of the production RDS DB instance"
      },
      "correct_answer": "D",
      "explanation": "You can create a read replica as a Multi-AZ DB instance. Amazon RDS creates a standby of your replica in another Availability Zone for failover support for the replica. Creating your read replica as a Multi-AZ DB instance is independent of whether the source database is a Multi-AZ DB instance. CORRECT: \"Create a Multi-AZ RDS Read Replica of the production RDS DB instance\" is the correct answer. INCORRECT: \"Create a Single-AZ RDS Read Replica of the production RDS DB instance. Create a second Single-AZ RDS Read Replica from the replica\" is incorrect. Read replicas are primarily used for horizontal scaling. The best solution for high availability is to use a Multi-AZ read replica. INCORRECT: \"Create a cross-region Multi-AZ deployment and create a read replica in the second region\" is incorrect as you cannot create a cross-region Multi-AZ deployment with RDS. INCORRECT: \"Use Amazon Data Lifecycle Manager to automatically create and manage snapshots\" is incorrect as using snapshots is not the best solution for high availability. INCORRECT: \"Create a Single-AZ RDS Read Replica of the production RDS DB instance. Create a second Single-AZ RDS Read Replica from the replica\" is incorrect. INCORRECT: \"Create a cross-region Multi-AZ deployment and create a read replica in the second region\" is incorrect as you cannot create a cross-region Multi-AZ deployment with RDS. INCORRECT: \"Use Amazon Data Lifecycle Manager to automatically create and manage snapshots\" is incorrect as using snapshots is not the best solution for high availability."
    },
    {
      "id": "860",
      "question": "A company is in the process of improving its security posture and wants to analyze and rectify a high volume of failed login attempts and unauthorized activities being logged in AWS CloudTrail. What is the most efficient solution to help the company identify these security events with the LEAST amount of operational effort?",
      "options": {
        "A": "Utilize AWS Data Pipeline to regularly extract CloudTrail logs and use a custom script to identify the required security events.",
        "B": "Leverage AWS Lambda to trigger on CloudTrail log updates and use a custom script to scan for failed logins and unauthorized actions.",
        "C": "Use Amazon Athena to directly query CloudTrail logs for failed logins and unauthorized activities.",
        "D": "Implement Amazon Elasticsearch Service with Kibana to visualize the CloudTrail logs and manually search for these events."
      },
      "correct_answer": "C",
      "explanation": "Amazon Athena can directly query data from S3 (where CloudTrail logs are stored) using standard SQL, making it a powerful and efficient tool for analyzing these logs. You don't need to manage any infrastructure or write custom scripts, and you can quickly write and run queries to identify the required security events. CORRECT: \"Use Amazon Athena to directly query CloudTrail logs for failed logins and unauthorized activities\" is the correct answer (as explained above.) INCORRECT: \"Leverage AWS Lambda to trigger on CloudTrail log updates and use a custom script to scan for failed logins and unauthorized actions\" is incorrect. While Lambda functions can be triggered based on CloudTrail log updates and could theoretically be used to scan for security events, this would require substantial setup and ongoing maintenance of the script. It's not the most efficient choice. INCORRECT: \"Utilize AWS Data Pipeline to regularly extract CloudTrail logs and use a custom script to identify the required security events\" is incorrect. This solution could work, but the operational overhead of managing the extraction process and maintaining a custom script for analysis is not minimal. INCORRECT: \"Implement Amazon Elasticsearch Service with Kibana to visualize the CloudTrail logs and manually search for these events\" is incorrect. While Elasticsearch and Kibana provide powerful search and visualization capabilities, respectively, they require a fair amount of setup and management. This option would provide more in-depth analysis and real-time monitoring, but it wouldn't be the most efficient way to simply identify the security events mentioned. INCORRECT: \"Leverage AWS Lambda to trigger on CloudTrail log updates and use a custom script to scan for failed logins and unauthorized actions\" is incorrect. INCORRECT: \"Utilize AWS Data Pipeline to regularly extract CloudTrail logs and use a custom script to identify the required security events\" is incorrect. INCORRECT: \"Implement Amazon Elasticsearch Service with Kibana to visualize the CloudTrail logs and manually search for these events\" is incorrect."
    },
    {
      "id": "861",
      "question": "A software development company is deploying a microservices-based application on Amazon Elastic Kubernetes Service (Amazon EKS). The application's traffic fluctuates significantly throughout the day and the company wants to ensure that the EKS cluster scales up and down according to these traffic patterns. Which combination of steps would satisfy these requirements with MINIMAL operational overhead? (Select TWO.)",
      "options": {
        "A": "Employ the Kubernetes Cluster Autoscaler for dynamically managing the quantity of nodes in the EKS cluster.",
        "B": "Implement the Kubernetes Vertical Pod Autoscaler to adjust the CPU and memory allocation for the pods.",
        "C": "Utilize the Kubernetes Metrics Server to enable horizontal pod autoscaling based on resource utilization.",
        "D": "Integrate Amazon SQS and connect it to Amazon EKS for workload management."
      },
      "correct_answer": "A,C",
      "explanation": "The Metrics Server collects resource metrics like CPU and memory usage from each node and its pods and provides these metrics to the Kubernetes API server for use by the Horizontal Pod Autoscaler, which automatically scales the number of pods in a deployment, replication controller, replica set, or stateful set based on observed CPU utilization. The Kubernetes Cluster Autoscaler automatically adjusts the size of the Kubernetes cluster when there are pods that failed to run in the cluster due to insufficient resources or when there are nodes in the cluster that have been underutilized for an extended period and their pods can be placed on other existing nodes. CORRECT: \"Utilize the Kubernetes Metrics Server to enable horizontal pod autoscaling based on resource utilization\" is a correct answer (as explained above.) CORRECT: \"Employ the Kubernetes Cluster Autoscaler for dynamically managing the quantity of nodes in the EKS cluster\" is also a correct answer (as explained above.) INCORRECT: \"Implement the Kubernetes Vertical Pod Autoscaler to adjust the CPU and memory allocation for the pods\" is incorrect. The Vertical Pod Autoscaler adjusts the resources of the pods and not the number of pods or nodes, which won't directly help with scaling according to traffic patterns. INCORRECT: \"Integrate Amazon SQS and connect it to Amazon EKS for workload management\" is incorrect. Amazon SQS is a message queuing service, and while it can be used to manage workloads by decoupling microservices, it doesn't directly help with autoscaling an EKS cluster based on traffic patterns. INCORRECT: \"Leverage AWS X-Ray to track and analyze the application's network activity\" is incorrect. AWS X-Ray provides insights into the behavior of your applications, but it doesn't directly help with autoscaling an EKS cluster. INCORRECT: \"Implement the Kubernetes Vertical Pod Autoscaler to adjust the CPU and memory allocation for the pods\" is incorrect. INCORRECT: \"Integrate Amazon SQS and connect it to Amazon EKS for workload management\" is incorrect. INCORRECT: \"Leverage AWS X-Ray to track and analyze the application's network activity\" is incorrect."
    },
    {
      "id": "862",
      "question": "A company needs to store data from an application. Data in the application changes frequently. All levels of stored data must be audited under a new regulation which the company adheres to. Application storage capacity is running out on the company's on-premises infrastructure. To comply with the new regulation, a solutions architect must offload some data securely to AWS to relieve the on-premises capacity issues. Which solution will meet these requirements?",
      "options": {
        "A": "Move the existing data to Amazon S3 with AWS Snowcone. Using AWS CloudTrail, you can log management events.",
        "B": "Move existing data to Amazon S3 using AWS DataSync. Log data events using AWS CloudTrail.",
        "C": "The existing data can be transferred to Amazon S3 with the help of Amazon S3 Transfer Acceleration. Log data events using AWS CloudTrail.",
        "D": "Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events."
      },
      "correct_answer": "D",
      "explanation": "AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually unlimited cloud storage. Secondly AWS CloudTrail monitors and records account activity across your AWS infrastructure, giving you control over storage, analysis, and remediation actions. CORRECT: \"Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events\" is the correct answer (as explained above.) INCORRECT: \"Move existing data to Amazon S3 using AWS DataSync. Log data events using AWS CloudTrail” is incorrect. AWS DataSync is a secure, online service that automates and accelerates moving data between on-premises and AWS storage service and is not designed as a hybrid storage service. INCORRECT: \"The existing data can be transferred to Amazon S3 with the help of Amazon S3 Transfer Acceleration. Log data events using AWS CloudTrail” is incorrect. Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets and is not a migration service. INCORRECT: \"Move the existing data to Amazon S3 with AWS Snowcone. Using AWS CloudTrail, you can log management events\" is incorrect. AWS Snowcone is not suitable as a hybrid cloud service. INCORRECT: \"Move the existing data to Amazon S3 with AWS Snowcone. Using AWS CloudTrail, you can log management events\" is incorrect."
    },
    {
      "id": "863",
      "question": "A solutions architect is optimizing a website for real-time streaming and on-demand videos. The website’s users are located around the world and the solutions architect needs to optimize the performance for both the real-time and on-demand streaming. Which service should the solutions architect choose?",
      "options": {
        "A": "Amazon Route 53",
        "B": "Amazon S3 Transfer Acceleration",
        "C": "Amazon CloudFront",
        "D": "AWS Global Accelerator"
      },
      "correct_answer": "C",
      "explanation": "Amazon CloudFront can be used to stream video to users across the globe using a wide variety of protocols that are layered on top of HTTP. This can include both on-demand video as well as real time streaming video. CORRECT: \"Amazon CloudFront\" is the correct answer. INCORRECT: \"AWS Global Accelerator\" is incorrect as this would be an expensive way of getting the content closer to users compared to using CloudFront. As this is a use case for CloudFront and there are so many edge locations it is the better option. INCORRECT: \"Amazon Route 53\" is incorrect as you still need a solution for getting the content closer to users. INCORRECT: \"Amazon S3 Transfer Acceleration\" is incorrect as this is used to accelerate uploads of data to Amazon S3 buckets. INCORRECT: \"AWS Global Accelerator\" is incorrect as this would be an expensive way of getting the content closer to users compared to using CloudFront. INCORRECT: \"Amazon Route 53\" is incorrect as you still need a solution for getting the content closer to users. INCORRECT: \"Amazon S3 Transfer Acceleration\" is incorrect as this is used to accelerate uploads of data to Amazon S3 buckets."
    },
    {
      "id": "864",
      "question": "An application runs on Amazon EC2 instances across multiple Availability Zones. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%. What should a solutions architect do to maintain the desired performance across all instances in the group?",
      "options": {
        "A": "Use a simple scaling policy to dynamically scale the Auto Scaling group",
        "B": "Use scheduled scaling actions to scale up and scale down the Auto Scaling group",
        "C": "Use a target tracking policy to dynamically scale the Auto Scaling group",
        "D": "Use an AWS Lambda function to update the desired Auto Scaling group capacity"
      },
      "correct_answer": "C",
      "explanation": "With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to the changes in the metric due to a changing load pattern. CORRECT: \"Use a target tracking policy to dynamically scale the Auto Scaling group\" is the correct answer. INCORRECT: \"Use a simple scaling policy to dynamically scale the Auto Scaling group\" is incorrect as target tracking is a better way to keep the aggregate CPU usage at around 40% INCORRECT: \"Use an AWS Lambda function to update the desired Auto Scaling group capacity\" is incorrect as this can be done automatically. INCORRECT: \"Use scheduled scaling actions to scale up and scale down the Auto Scaling group\" is incorrect as dynamic scaling is required to respond to changes in utilization. INCORRECT: \"Use a simple scaling policy to dynamically scale the Auto Scaling group\" is incorrect as target tracking is a better way to keep the aggregate CPU usage at around 40% INCORRECT: \"Use an AWS Lambda function to update the desired Auto Scaling group capacity\" is incorrect as this can be done automatically. INCORRECT: \"Use scheduled scaling actions to scale up and scale down the Auto Scaling group\" is incorrect as dynamic scaling is required to respond to changes in utilization."
    },
    {
      "id": "865",
      "question": "A systems administrator of a company wants to detect and remediate the compromise of services such as Amazon EC2 instances and Amazon S3 buckets. Which AWS service can the administrator use to protect the company against attacks?",
      "options": {
        "A": "Amazon Inspector",
        "B": "Amazon Macie",
        "C": "Amazon Cognito",
        "D": "Amazon GuardDuty"
      },
      "correct_answer": "D",
      "explanation": "Amazon GuardDuty gives you access to built-in detection techniques that are developed and optimized for the cloud. The detection algorithms are maintained and continuously improved upon by AWS Security. The primary detection categories include reconnaissance, instance compromise, account compromise, and bucket compromise. Amazon GuardDuty offers HTTPS APIs, CLI tools, and Amazon CloudWatch Events to support automated security responses to security findings. For example, you can automate the response workflow by using CloudWatch Events as an event source to trigger an AWS Lambda function. CORRECT: \"Amazon GuardDuty\" is the correct answer. INCORRECT: \"Amazon Cognito\" is incorrect. Cognito provides sign up and sign services for mobiles apps. INCORRECT: \"Amazon Inspector\" is incorrect. Inspector is more about identifying vulnerabilities and evaluating against security best practices. It does not detect compromise. INCORRECT: \"Amazon Macie\" is incorrect. Macie is used for detecting and protecting sensitive data that is in Amazon S3. INCORRECT: \"Amazon Cognito\" is incorrect. INCORRECT: \"Amazon Inspector\" is incorrect. INCORRECT: \"Amazon Macie\" is incorrect."
    },
    {
      "id": "866",
      "question": "A company is developing a web-based application that will be used for real-time chat functionality. The application should use WebSocket APIs to maintain a persistent connection with the client. The backend services of the application, hosted in containers within private subnets of a VPC, need to be accessed securely. Which solution will meet these requirements?",
      "options": {
        "A": "Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster.",
        "B": "Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster.",
        "C": "Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster.",
        "D": "Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster."
      },
      "correct_answer": "C",
      "explanation": "The requirement is for a real-time chat application, which makes the use of WebSocket APIs more suitable. Hosting the application in Amazon EKS within a private subnet allows secure and scalable management of the application. Creating a VPC link provides secure, private connectivity between API Gateway and the Amazon EKS service hosted inside the VPC. CORRECT: \"Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster\" is the correct answer (as explained above.) INCORRECT: \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster\" is incorrect. This solution does provide the secure hosting environment and private connectivity between API Gateway and the Amazon EKS cluster, but REST APIs are not suitable for real-time applications like a chat service. This is because REST APIs use request-response model which doesn't provide the continuous connection needed for real-time communication. INCORRECT: \"Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect. This option, while correctly suggesting the use of WebSocket APIs and Amazon EKS, proposes the use of a security group for connectivity. However, security groups act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level, while access to services within VPCs is more securely managed through VPC links. INCORRECT: \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect. REST APIs are not suitable for a real-time chat application. Also, managing access via a security group is not the most secure method for accessing services hosted within private subnets in a VPC. INCORRECT: \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Establish a private VPC link for the API Gateway to securely access the Amazon EKS cluster\" is incorrect. INCORRECT: \"Develop a WebSocket API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect. INCORRECT: \"Develop a REST API using Amazon API Gateway. Host the application in Amazon Elastic Kubernetes Service (EKS) in a private subnet. Create a security group that allows API Gateway to access the Amazon EKS cluster\" is incorrect."
    },
    {
      "id": "867",
      "question": "An application runs on Amazon EC2 instances backed by Amazon EBS volumes and an Amazon RDS database. The application is highly sensitive and security compliance requirements mandate that all personally identifiable information (PII) be encrypted at rest. Which solution should a Solutions Architect choose to this requirement?",
      "options": {
        "A": "Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes.",
        "B": "Configure Amazon EBS encryption and Amazon RDS encryption with AWS KMS keys to encrypt instance and database volumes.",
        "C": "Enable encryption on Amazon RDS during creation. Use Amazon Macie to identify sensitive data.",
        "D": "Deploy AWS CloudHSM, generate encryption keys, and use the customer master key (CMK) to encrypt database volumes."
      },
      "correct_answer": "B",
      "explanation": "The data must be encrypted at rest on both the EC2 instance’s attached EBS volumes and the RDS database. Both storage locations can be encrypted using AWS KMS keys. With RDS, KMS uses a customer master key (CMK) to encrypt the DB instance, all logs, backups, and snapshots. CORRECT: \"Configure Amazon EBS encryption and Amazon RDS encryption with AWS KMS keys to encrypt instance and database volumes\" is the correct answer. INCORRECT: \"Enable encryption on Amazon RDS during creation. Use Amazon Macie to identify sensitive data\" is incorrect. This does not encrypt the EBS volumes attached to the EC2 instance and Macie cannot be used with RDS. INCORRECT: \"Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes\" is incorrect. SSL encryption encrypts data in transit but not at rest. INCORRECT: \"Deploy AWS CloudHSM, generate encryption keys, and use the customer master key (CMK) to encrypt database volumes\" is incorrect. CloudHSM is not required for this solution, and we need to encrypt the database volumes and the EBS volumes. INCORRECT: \"Enable encryption on Amazon RDS during creation. Use Amazon Macie to identify sensitive data\" is incorrect. INCORRECT: \"Configure SSL/TLS encryption using AWS KMS customer master keys (CMKs) to encrypt database volumes\" is incorrect. INCORRECT: \"Deploy AWS CloudHSM, generate encryption keys, and use the customer master key (CMK) to encrypt database volumes\" is incorrect."
    },
    {
      "id": "868",
      "question": "A legacy application is being migrated into AWS. The application has a large amount of data that is rarely accessed. When files are accessed they are retrieved sequentially. The application will be migrated onto an Amazon EC2 instance. What is the LEAST expensive EBS volume type for this use case?",
      "options": {
        "A": "Provisioned IOPS SSD (io1)",
        "B": "Cold HDD (sc1)",
        "C": "Throughput Optimized HDD (st1)",
        "D": "General Purpose SSD (gp2)"
      },
      "correct_answer": "B",
      "explanation": "The cold HDD (sc1) EBS volume type is the lowest cost option that is suitable for this use case. The sc1 volume type is suitable for infrequently accessed data and use cases that are oriented towards throughput like sequential data access. CORRECT: \"Cold HDD (sc1)\" is the correct answer. INCORRECT: \"Provisioned IOPS SSD (io1)\" is incorrect. This is the most expensive option and used for use cases that demand high IOPS. INCORRECT: \"General Purpose SSD (gp2)\" is incorrect. This is a more expensive SSD volume type that is used for general use cases. INCORRECT: \"Throughput Optimized HDD (st1)\" is incorrect. This is also used for throughput-oriented use cases however it is higher cost than sc1 and better for frequently accessed data. INCORRECT: \"Provisioned IOPS SSD (io1)\" is incorrect. INCORRECT: \"General Purpose SSD (gp2)\" is incorrect. INCORRECT: \"Throughput Optimized HDD (st1)\" is incorrect."
    },
    {
      "id": "869",
      "question": "A healthcare company is migrating its patient record system to AWS. The company receives thousands of encrypted patient data files every day through FTP. An on-premises server processes the data files twice a day. However, the processing job takes hours to finish. The company wants the AWS solution to process incoming data files as soon as they arrive with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take around 10 minutes. Which solution will meet these requirements in the MOST operationally efficient way?",
      "options": {
        "A": "Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Glacier. Configure an Amazon EC2 instance to process the files. Use Amazon EventBridge rules to invoke the EC2 instance to process the files twice a day from S3 Glacier. Delete the objects after the job has processed the objects.",
        "B": "Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Use Amazon EC2 instances managed by an Auto Scaling group to process the files. Set an S3 event notification to trigger an AWS Lambda function that launches the EC2 instances when the files arrive. Delete the files after they are processed.",
        "C": "Use an Amazon EC2 instance that runs an SFTP server to store incoming files in Amazon S3 Standard. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files twice a day. Delete the files after the job has processed the files.",
        "D": "Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive."
      },
      "correct_answer": "D",
      "explanation": "AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3 using SFTP. Storing incoming files in S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. AWS Lambda can respond immediately to S3 events, which allows processing of files as soon as they arrive. Lambda can also delete the files after processing. This meets all requirements and is operationally efficient, as it requires minimal management and has low costs. CORRECT: \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive\" is the correct answer (as explained above.) INCORRECT: \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Glacier. Configure an Amazon EC2 instance to process the files. Use Amazon EventBridge rules to invoke the EC2 instance to process the files twice a day from S3 Glacier. Delete the objects after the job has processed the objects\" is incorrect. This option involves using Amazon S3 Glacier, which is primarily used for long-term archival storage. Accessing data for processing could take longer and be more expensive than using S3 Standard. In addition, EC2 instances need to be managed and are less efficient for this scenario compared to AWS Lambda. INCORRECT: \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Use Amazon EC2 instances managed by an Auto Scaling group to process the files. Set an S3 event notification to trigger an AWS Lambda function that launches the EC2 instances when the files arrive. Delete the files after they are processed\" is incorrect. While this solution will work, it is less efficient operationally because managing EC2 instances and an Auto Scaling group is more complex and likely more expensive than simply using AWS Lambda for processing. INCORRECT: \"Use an Amazon EC2 instance that runs an SFTP server to store incoming files in Amazon S3 Standard. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files twice a day. Delete the files after the job has processed the files\" is incorrect. This option does not meet the requirement of processing incoming data files as soon as they arrive, as EventBridge rules would invoke the job only twice a day. It also involves managing an EC2 instance, which is less operationally efficient than the AWS Transfer Family and AWS Lambda option. INCORRECT: \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Glacier. Configure an Amazon EC2 instance to process the files. Use Amazon EventBridge rules to invoke the EC2 instance to process the files twice a day from S3 Glacier. Delete the objects after the job has processed the objects\" is incorrect. INCORRECT: \"Use AWS Transfer Family to create an SFTP server to store incoming files in Amazon S3 Standard. Use Amazon EC2 instances managed by an Auto Scaling group to process the files. Set an S3 event notification to trigger an AWS Lambda function that launches the EC2 instances when the files arrive. Delete the files after they are processed\" is incorrect. INCORRECT: \"Use an Amazon EC2 instance that runs an SFTP server to store incoming files in Amazon S3 Standard. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files twice a day. Delete the files after the job has processed the files\" is incorrect."
    },
    {
      "id": "870",
      "question": "A company operates multiple AWS accounts under AWS Organizations. To better manage the costs, the company wants to allocate different budgets for each of these accounts. The company also wants to prevent additional resource provisioning in an AWS account if it reaches its allocated budget before the end of the budget period. Which combination of solutions will meet these requirements? (Select THREE.)",
      "options": {
        "A": "Configure alerts in AWS Budgets to notify the company when an account is about to reach its budget threshold. Then use a budget action that links to the IAM role to prevent additional resource provisioning.",
        "B": "Create an IAM user with adequate permissions to allow AWS Budgets to enforce budget actions.",
        "C": "Set up an IAM role with the necessary permissions that allow AWS Budgets to execute budget actions.",
        "D": "Use AWS Budgets to establish different budgets for each AWS account. Configure the budgets in the Billing and Cost Management console."
      },
      "correct_answer": "A,C,D",
      "explanation": "AWS Budgets is a tool that enables you to set custom cost and usage budgets. You can set your budget amount, and AWS provides you with estimated charges and forecasted costs for your AWS usage. Configuring the budgets in the Billing and Cost Management console is a recommended step. AWS Budgets can execute budget actions (like preventing additional resource provisioning) using an IAM role with the necessary permissions. Configuring alerts in AWS Budgets and linking a budget action to an IAM role for automatic prevention of additional resource provisioning is a correct and efficient way to manage costs. CORRECT: \"Use AWS Budgets to establish different budgets for each AWS account. Configure the budgets in the Billing and Cost Management console\" is a correct answer (as explained above.) CORRECT: \"Set up an IAM role with the necessary permissions that allow AWS Budgets to execute budget actions\" is also a correct answer (as explained above.) CORRECT: \"Configure alerts in AWS Budgets to notify the company when an account is about to reach its budget threshold. Then use a budget action that links to the IAM role to prevent additional resource provisioning\" is also a correct answer (as explained above.) INCORRECT: \"Use AWS Budgets in the AWS Management Console to set up budgets and specify the cost threshold for each AWS account\" is incorrect. While AWS Budgets can indeed be set up in the AWS Management Console, the budgets aren't set in the context of cost thresholds for each AWS account. This option is not fully accurate. INCORRECT: \"Create an IAM user with adequate permissions to allow AWS Budgets to enforce budget actions\" is incorrect. Although you can create an IAM user with necessary permissions, using an IAM role is generally a better practice. An IAM user is an entity that you create in AWS to represent the person or service that uses it to interact with AWS, while an IAM role is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. A role does not have long-term credentials associated with it like an IAM user does. INCORRECT: \"Set up an alert in AWS Budgets to notify the company when a particular account meets its budget threshold. Enable real-time monitoring for immediate notification\" is incorrect. AWS Budgets doesn't allow for real-time monitoring; the data can be delayed up to 24 hours. The frequency of budget alert notifications is not customizable to the minute or hour; they are typically sent out daily, weekly, or when a certain threshold is crossed. INCORRECT: \"Use AWS Budgets in the AWS Management Console to set up budgets and specify the cost threshold for each AWS account\" is incorrect. INCORRECT: \"Create an IAM user with adequate permissions to allow AWS Budgets to enforce budget actions\" is incorrect. INCORRECT: \"Set up an alert in AWS Budgets to notify the company when a particular account meets its budget threshold. Enable real-time monitoring for immediate notification\" is incorrect."
    },
    {
      "id": "871",
      "question": "A tool needs to analyze data stored in an Amazon S3 bucket. Processing the data takes a few seconds and results are then written to another S3 bucket. Less than 256 MB of memory is needed to run the process. What would be the MOST cost-effective compute solutions for this use case?",
      "options": {
        "A": "AWS Fargate tasks",
        "B": "Amazon EC2 spot instances",
        "C": "Amazon Elastic Beanstalk",
        "D": "AWS Lambda functions"
      },
      "correct_answer": "D",
      "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda has a maximum execution time of 900 seconds and memory can be allocated up to 3008 MB. Therefore, the most cost-effective solution will be AWS Lambda. CORRECT: \"AWS Lambda functions\" is the correct answer. INCORRECT: \"AWS Fargate tasks\" is incorrect. Fargate runs Docker containers and is serverless. However, you do pay for the running time of the tasks so it will not be as cost-effective. INCORRECT: \"Amazon EC2 spot instances\" is incorrect. EC2 instances must run continually waiting for jobs to process so even with spot this would be less cost-effective (and subject to termination). INCORRECT: \"Amazon Elastic Beanstalk\" is incorrect. This services also relies on Amazon EC2 instances so would not be as cost-effective. INCORRECT: \"AWS Fargate tasks\" is incorrect. INCORRECT: \"Amazon EC2 spot instances\" is incorrect. INCORRECT: \"Amazon Elastic Beanstalk\" is incorrect."
    },
    {
      "id": "872",
      "question": "A company runs a streaming media service and the content is stored on Amazon S3. The media catalog server pulls updated content from S3 and can issue over 1 million read operations per second for short periods. Latency must be kept under 5ms for these updates. Which solution will provide the BEST performance for the media catalog updates?",
      "options": {
        "A": "Implement an Instance store volume on the media catalog server",
        "B": "Implement Amazon CloudFront and cache the content at Edge Locations",
        "C": "Update the application code to use an Amazon ElastiCache for Redis cluster",
        "D": "Update the application code to use an Amazon DynamoDB Accelerator cluster"
      },
      "correct_answer": "C",
      "explanation": "Some applications, such as media catalog updates require high frequency reads, and consistent throughput. For such applications, customers often complement S3 with an in-memory cache, such as Amazon ElastiCache for Redis, to reduce the S3 retrieval cost and to improve performance. ElastiCache for Redis is a fully managed, in-memory data store that provides sub-millisecond latency performance with high throughput. ElastiCache for Redis complements S3 in the following ways: - Redis stores data in-memory, so it provides sub-millisecond latency and supports incredibly high requests per second. - It supports key/value based operations that map well to S3 operations (for example, GET/SET => GET/PUT), making it easy to write code for both S3 and ElastiCache. - It can be implemented as an application side cache. This allows you to use S3 as your persistent store and benefit from its durability, availability, and low cost. Your applications decide what objects to cache, when to cache them, and how to cache them. In this example the media catalog is pulling updates from S3 so the performance between these components is what needs to be improved. Therefore, using ElastiCache to cache the content will dramatically increase the performance. CORRECT: \"Update the application code to use an Amazon ElastiCache for Redis cluster\" is the correct answer. INCORRECT: \"Implement Amazon CloudFront and cache the content at Edge Locations\" is incorrect. CloudFront is good for getting media closer to users but in this case we’re trying to improve performance within the data center moving data from S3 to the media catalog server. INCORRECT: \"Update the application code to use an Amazon DynamoDB Accelerator cluster\" is incorrect. DynamoDB Accelerator (DAX) is used with DynamoDB but is unsuitable for use with Amazon S3. INCORRECT: \"Implement an Instance store volume on the media catalog server\" is incorrect. This will improve local disk performance but will not improve reads from Amazon S3. INCORRECT: \"Implement Amazon CloudFront and cache the content at Edge Locations\" is incorrect. INCORRECT: \"Update the application code to use an Amazon DynamoDB Accelerator cluster\" is incorrect. INCORRECT: \"Implement an Instance store volume on the media catalog server\" is incorrect."
    },
    {
      "id": "873",
      "question": "Three AWS accounts are owned by the same company but in different regions. Account Z has two AWS Direct Connect connections to two separate company offices. Accounts A and B require the ability to route across account Z’s Direct Connect connections to each company office. A Solutions Architect has created an AWS Direct Connect gateway in account Z. How can the required connectivity be configured?",
      "options": {
        "A": "Associate the Direct Connect gateway to a virtual private gateway in account A and B",
        "B": "Associate the Direct Connect gateway to a transit gateway in each region",
        "C": "Create a VPC Endpoint to the Direct Connect gateway in account A and B",
        "D": "Create a PrivateLink connection in Account Z and ENIs in accounts A and B"
      },
      "correct_answer": "A",
      "explanation": "You can associate an AWS Direct Connect gateway with either of the following gateways: - A transit gateway when you have multiple VPCs in the same Region. - A virtual private gateway. In this case account Z owns the Direct Connect gateway so a VPG in accounts A and B must be associated with it to enable this configuration to work. After Account Z accepts the proposals, Account A and Account B can route traffic from their virtual private gateway to the Direct Connect gateway. CORRECT: \"Associate the Direct Connect gateway to a virtual private gateway in account A and B\" is the correct answer. INCORRECT: \"Associate the Direct Connect gateway to a transit gateway in each region\" is incorrect. This would be a good solution if the accounts were in VPCs within a region rather than across regions. INCORRECT: \"Create a VPC Endpoint to the Direct Connect gateway in account A and B\" is incorrect. You cannot create a VPC endpoint for Direct Connect gateways. INCORRECT: \"Create a PrivateLink connection in Account Z and ENIs in accounts A and B\" is incorrect. You cannot use PrivateLink connections to publish a Direct Connect gateway. INCORRECT: \"Associate the Direct Connect gateway to a transit gateway in each region\" is incorrect. INCORRECT: \"Create a VPC Endpoint to the Direct Connect gateway in account A and B\" is incorrect. INCORRECT: \"Create a PrivateLink connection in Account Z and ENIs in accounts A and B\" is incorrect."
    },
    {
      "id": "874",
      "question": "An e-commerce company operates a serverless web application that must interact with numerous Amazon DynamoDB tables to fulfill user requests. It is critical that the application's performance remains consistent and unaffected while interacting with these tables. Which method provides the MOST operationally efficient way to fulfill these requirements?",
      "options": {
        "A": "AWS Glue with a DynamoDB connector.",
        "B": "AWS AppSync with multiple data sources and resolvers.",
        "C": "Amazon S3 with Lambda triggers.",
        "D": "AWS Lambda with Step Functions."
      },
      "correct_answer": "B",
      "explanation": "AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need, including from multiple DynamoDB tables. AWS AppSync is designed for real-time and offline data access which makes it an ideal solution for this scenario. CORRECT: \"AWS AppSync with multiple data sources and resolvers\" is the correct answer (as explained above.) INCORRECT: \"AWS Lambda with Step Functions\" is incorrect. AWS Step Functions make it easy to coordinate the components of distributed applications and microservices using visual workflows. However, while you could theoretically build a flow to retrieve data from multiple tables, it's not the most efficient solution as it introduces additional complexity and potential latency. INCORRECT: \"Amazon S3 with Lambda triggers\" is incorrect. While you can use AWS Lambda to execute code in response to triggers like changes to data in an Amazon S3 bucket, this doesn't directly allow the application to retrieve data from multiple DynamoDB tables. This approach would also involve unnecessary data transfers and added latency. INCORRECT: \"AWS Glue with a DynamoDB connector\" is incorrect. AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load your data for analytics. However, AWS Glue isn't meant for real-time data retrieval in an application. Using it for real-time data retrieval would likely be overcomplicated and inefficient. INCORRECT: \"AWS Lambda with Step Functions\" is incorrect. INCORRECT: \"Amazon S3 with Lambda triggers\" is incorrect. INCORRECT: \"AWS Glue with a DynamoDB connector\" is incorrect."
    },
    {
      "id": "875",
      "question": "There are badge readers located at every entrance of an organization’s warehouses. A message is sent over HTTPS when badges are scanned to indicate who tried to access the entrance. A solutions architect must design a system to process these messages. A highly available solution is required. The solution must store results in a durable data store for later analysis. Which system architecture should the solutions architect recommend?",
      "options": {
        "A": "Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results.",
        "B": "Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket.",
        "C": "Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB.",
        "D": "Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function."
      },
      "correct_answer": "D",
      "explanation": "Amazon API Gateway would be ideal for providing a secure entry point for your application, and for traffic to be sent via HTTPS. AWS Lambda would integrate seamlessly with API Gateway to process the data, as an event-driven solution like this would be perfect when designing a scalable system based on sporadic use. Finally, DynamoDB is highly scalable and is a perfect repository for data to be stored for future analysis. CORRECT: \"Set up an HTTPS endpoint in Amazon API Gateway. To process the messages and save the results to Amazon DynamoDB, configure an API Gateway endpoint to invoke an AWS Lambda function\" is the correct answer (as explained above.) INCORRECT: \"Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results\" is incorrect. As the action of a badge being read to initiate access to a warehouse should only take a few seconds, spinning up an EC2 instance to serve as a HTTPS endpoint would take minutes, and is not suitable for this use case. INCORRECT: \"Direct incoming messages from the sensor to an AWS Lambda function using Amazon Route 53. Create a Lambda function that processes messages and saves results to Amazon DynamoDB” is incorrect. Amazon Route 53 is a managed DNS service, and DNS is not required in this instance as the badge reader does not have a DNS name. INCORRECT: \"Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket\" is incorrect. VPC endpoints are designed to facilitate traffic across the AWS backbone network between AWS services and are not used to create connections between external endpoints outside of the AWS network and an Amazon S3 bucket. INCORRECT: \"Create an Amazon EC2 instance to serve as the HTTPS endpoint and to process messages. An Amazon S3 bucket should be configured for the EC2 instance to save the results\" is incorrect. INCORRECT: \"Set up an Amazon S3 gateway endpoint in your VPC. Connect the facility network to the VPC via a Site-to-Site VPN connection so that sensor data can be written directly to an S3 bucket\" is incorrect."
    },
    {
      "id": "876",
      "question": "A Solutions Architect needs to capture information about the traffic that reaches an Amazon Elastic Load Balancer. The information should include the source, destination, and protocol. What is the most secure and reliable method for gathering this data?",
      "options": {
        "A": "Enable Amazon CloudTrail logging and configure packet capturing",
        "B": "Create a VPC flow log for the subnets in which the ELB is running",
        "C": "Use Amazon CloudWatch Logs to review detailed logging information",
        "D": "Create a VPC flow log for each network interface associated with the ELB"
      },
      "correct_answer": "D",
      "explanation": "You can use VPC Flow Logs to capture detailed information about the traffic going to and from your Elastic Load Balancer. Create a flow log for each network interface for your load balancer. There is one network interface per load balancer subnet. CORRECT: \"Create a VPC flow log for each network interface associated with the ELB\" is the correct answer. INCORRECT: \"Enable Amazon CloudTrail logging and configure packet capturing\" is incorrect. CloudTrail performs auditing of API actions, it does not do packet capturing. INCORRECT: \"Use Amazon CloudWatch Logs to review detailed logging information\" is incorrect as this service does not record this information in CloudWatch logs. INCORRECT: \"Create a VPC flow log for the subnets in which the ELB is running\" is incorrect as the more secure option is to use the ELB network interfaces. INCORRECT: \"Enable Amazon CloudTrail logging and configure packet capturing\" is incorrect. INCORRECT: \"Use Amazon CloudWatch Logs to review detailed logging information\" is incorrect as this service does not record this information in CloudWatch logs. INCORRECT: \"Create a VPC flow log for the subnets in which the ELB is running\" is incorrect as the more secure option is to use the ELB network interfaces."
    },
    {
      "id": "877",
      "question": "A company runs a financial application using an Amazon EC2 Auto Scaling group behind an Application Load Balancer (ALB). When running month-end reports on a specific day and time each month the application becomes unacceptably slow. Amazon CloudWatch metrics show the CPU utilization hitting 100%. What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?",
      "options": {
        "A": "Configure an Amazon CloudFront distribution in front of the ALB",
        "B": "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances",
        "C": "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization",
        "D": "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule"
      },
      "correct_answer": "D",
      "explanation": "Scheduled scaling allows you to set your own scaling schedule. In this case the scaling action can be scheduled to occur just prior to the time that the reports will be run each month. Scaling actions are performed automatically as a function of time and date. This will ensure that there are enough EC2 instances to serve the demand and prevent the application from slowing down. CORRECT: \"Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule\" is the correct answer. INCORRECT: \"Configure an Amazon CloudFront distribution in front of the ALB\" is incorrect as this would be more suitable for providing access to global users by caching content. INCORRECT: \"Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization\" is incorrect as this would not prevent the slow-down from occurring as there would be a delay between when the CPU hits 100% and the metric being reported and additional instances being launched. INCORRECT: \"Configure Amazon ElastiCache to remove some of the workload from the EC2 instances\" is incorrect as ElastiCache is a database cache, it cannot replace the compute functions of an EC2 instance. INCORRECT: \"Configure an Amazon CloudFront distribution in front of the ALB\" is incorrect as this would be more suitable for providing access to global users by caching content. INCORRECT: \"Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization\" is incorrect as this would not prevent the slow-down from occurring as there would be a delay between when the CPU hits 100% and the metric being reported and additional instances being launched. INCORRECT: \"Configure Amazon ElastiCache to remove some of the workload from the EC2 instances\" is incorrect as ElastiCache is a database cache, it cannot replace the compute functions of an EC2 instance."
    },
    {
      "id": "878",
      "question": "A company has some statistical data stored in an Amazon RDS database. The company want to allow users to access this information using an API. A solutions architect must create a solution that allows sporadic access to the data, ranging from no requests to large bursts of traffic. Which solution should the solutions architect suggest?",
      "options": {
        "A": "Set up an Amazon API Gateway and use AWS Elastic Beanstalk",
        "B": "Set up an Amazon API Gateway and use Amazon EC2 with Auto Scaling",
        "C": "Set up an Amazon API Gateway and use Amazon ECS",
        "D": "Set up an Amazon API Gateway and use AWS Lambda functions"
      },
      "correct_answer": "D",
      "explanation": "This question is simply asking you to work out the best compute service for the stated requirements. The key requirements are that the compute service should be suitable for a workload that can range quite broadly in demand from no requests to large bursts of traffic. AWS Lambda is an ideal solution as you pay only when requests are made and it can easily scale to accommodate the large bursts in traffic. Lambda works well with both API Gateway and Amazon RDS. CORRECT: \"Set up an Amazon API Gateway and use AWS Lambda functions\" is the correct answer. INCORRECT: \"Set up an Amazon API Gateway and use Amazon ECS\" is incorrect INCORRECT: \"Set up an Amazon API Gateway and use AWS Elastic Beanstalk\" is incorrect INCORRECT: \"Set up an Amazon API Gateway and use Amazon EC2 with Auto Scaling\" is incorrect INCORRECT: \"Set up an Amazon API Gateway and use Amazon ECS\" is incorrect INCORRECT: \"Set up an Amazon API Gateway and use AWS Elastic Beanstalk\" is incorrect INCORRECT: \"Set up an Amazon API Gateway and use Amazon EC2 with Auto Scaling\" is incorrect References: https://docs."
    },
    {
      "id": "879",
      "question": "A corporation has a web-based multiplayer gaming service that operates using both TCP and UDP protocols. Amazon Route 53 is currently employed to direct application traffic to a set of Network Load Balancers (NLBs) in various AWS Regions. To prepare for an increase in user activity, the company must enhance application performance and reduce latency. Which approach will best meet these requirements?",
      "options": {
        "A": "Implement AWS Global Accelerator ahead of the NLBs and align the Global Accelerator endpoint to use the appropriate listener ports.",
        "B": "Substitute the NLBs with Application Load Balancers (ALBs) and set Route 53 to utilize latency-based routing.",
        "C": "Incorporate Amazon CloudFront in front of the NLBs and extend the duration of the Cache-Control max-age directive.",
        "D": "Insert an Amazon API Gateway endpoint behind the NLBs, enable API caching, and customize method caching across different stages."
      },
      "correct_answer": "A",
      "explanation": "AWS Global Accelerator is designed to improve the availability and performance of your applications for local and global users. It directs traffic to optimal endpoints over the AWS global network, thus enhancing the performance of your TCP and UDP traffic by routing packets through the AWS global network infrastructure, reducing jitter, and improving overall game performance. CORRECT: \"Implement AWS Global Accelerator ahead of the NLBs and align the Global Accelerator endpoint to use the appropriate listener ports\" is the correct answer (as explained above.) INCORRECT: \"Incorporate Amazon CloudFront in front of the NLBs and extend the duration of the Cache-Control max-age directive\" is incorrect. Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of your static and dynamic web content. While it could potentially help with application performance, it doesn't directly improve TCP/UDP performance, which is the specific requirement in this case. INCORRECT: \"Substitute the NLBs with Application Load Balancers (ALBs) and set Route 53 to utilize latency-based routing\" is incorrect. Application Load Balancers (ALBs) are layer 7 load balancers and they do not support the handling of raw TCP and UDP traffic, which is a requirement for the gaming application in the question. NLBs, on the other hand, are suitable for extreme performance needs and for TCP/UDP traffic. INCORRECT: \"Insert an Amazon API Gateway endpoint behind the NLBs, enable API caching, and customize method caching across different stages\" is incorrect. While the API Gateway would add more control and security to the application, the caching feature is not necessarily beneficial for this real-time gaming scenario where the content is likely to change frequently and unpredictably. INCORRECT: \"Incorporate Amazon CloudFront in front of the NLBs and extend the duration of the Cache-Control max-age directive\" is incorrect. INCORRECT: \"Substitute the NLBs with Application Load Balancers (ALBs) and set Route 53 to utilize latency-based routing\" is incorrect. INCORRECT: \"Insert an Amazon API Gateway endpoint behind the NLBs, enable API caching, and customize method caching across different stages\" is incorrect."
    },
    {
      "id": "880",
      "question": "A telecommunication company has an API that allows users to manage their mobile plans and services. The API experiences significant traffic spikes during specific times such as end of the month and special offer periods. The company needs to ensure low latency response time consistently to ensure a good user experience. The solution should also minimize operational overhead. Which solution would meet these requirements MOST efficiently?",
      "options": {
        "A": "Use Amazon API Gateway with AWS Fargate tasks to handle the API requests.",
        "B": "Implement the API using AWS Elastic Beanstalk with auto-scaling groups.",
        "C": "Implement the API on an Amazon EC2 instance behind an Application Load Balancer with manual scaling.",
        "D": "Use Amazon API Gateway along with AWS Lambda functions with provisioned concurrency."
      },
      "correct_answer": "D",
      "explanation": "Amazon API Gateway and AWS Lambda together make a highly scalable solution for APIs. Provisioned concurrency in Lambda ensures that there is always a warm pool of functions ready to quickly respond to API requests, thereby guaranteeing low latency even during peak traffic times. CORRECT: \"Use Amazon API Gateway along with AWS Lambda functions with provisioned concurrency\" is the correct answer (as explained above.) INCORRECT: \"Implement the API using AWS Elastic Beanstalk with auto-scaling groups\" is incorrect. Elastic Beanstalk is a viable option for deploying applications and auto-scaling and can help handle increased traffic, but it doesn't guarantee the low latency requirement during peak traffic times. INCORRECT: \"Use Amazon API Gateway with AWS Fargate tasks to handle the API requests\" is incorrect. API Gateway with Fargate can provide scalable compute, but this approach can result in higher operational overhead because of the need to manage the container lifecycle. INCORRECT: \"Implement the API on an Amazon EC2 instance behind an Application Load Balancer with manual scaling\" is incorrect. This solution does not scale automatically and would require manual intervention to ensure optimal performance during traffic spikes. Therefore, it doesn't satisfy the requirement of minimizing operational overhead. INCORRECT: \"Implement the API using AWS Elastic Beanstalk with auto-scaling groups\" is incorrect. INCORRECT: \"Use Amazon API Gateway with AWS Fargate tasks to handle the API requests\" is incorrect. INCORRECT: \"Implement the API on an Amazon EC2 instance behind an Application Load Balancer with manual scaling\" is incorrect."
    }
  ]
}