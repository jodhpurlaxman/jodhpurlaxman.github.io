{
  "questions": [
    {
      "id": "301",
      "question": "A university research laboratory needs to migrate 30 TB of data from an on-premises Windows File server to A mazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share. The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days. Which A WS solution will meet these requirements?",
      "options": {
        "A": "A WS Snowcone",
        "B": "A mazon FSx File Gateway",
        "C": "A WS DataSync",
        "D": "A WS Transfer Family"
      },
      "correct_answer": "C",
      "explanation": "A WS DataSync is a fully managed data transfer service that can be used to simplify, automate, and accelerate copying large amounts of data between on-premises storage systems and A mazon S3, A mazon EFS, or A mazon FSx for Windows File Server.\nWith A WS DataSync, you can control the bandwidth to minimize the impact on other departments by configuring the maximum bandwidth that the data transfer service is allowed to use.\nA WS DataSync is designed for high-speed data transfers, and it can take advantage of the available network bandwidth to maximize performance.\nA WS DataSync can be quickly set up and configured for the migration, allowing the laboratory to meet the 5-day migration timeline."
    },
    {
      "id": "302",
      "question": "A company wants to create amobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an A mazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead. Which combination of solutions will meet these requirements? (Choose two.)",
      "options": {
        "A": "Deploy A mazon CloudFront for content delivery and caching.",
        "B": "Use A WS DataSync to replicate the video files across A W'S Regions in other S3 buckets.",
        "C": "Use A mazon Elastic Transcoder to convert the video files to more appropriate formats.",
        "D": "Deploy an A uto Sealing group of A mazon EC2 instances in Local Zones for content delivery and caching.",
        "E": "Deploy an A uto Scaling group of A mazon EC2 instances to convert the video files to more appropriate formats."
      },
      "correct_answer": "AC",
      "explanation": "C. Use A mazon Elastic Transcoder to convert the video files to more appropriate formats.\nA mazon CloudFront is a content delivery network (CDN) that can distribute your video content globally, reducing latency and improving the speed of delivery.\nCloudFront can cache the video content at edge locations, which helps in minimizing the load on the S3 bucket and improves playback performance for users.\nA mazon Elastic Transcoder can convert the raw video files into more appropriate formats suitable for streaming, reducing the size of the videos.\nBy using Elastic Transcoder, you can create different versions of the video files optimized for different devices, bitrates, and resolutions, which can significantly improve the playback experience on mobile devices"
    },
    {
      "id": "303",
      "question": "A company is launching a new application deployed on an A mazon Elastic Container Service (A mazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high traffic to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should A solutions architect recommend?",
      "options": {
        "A": "Use A mazon EC2 A uto Scaling to scale at certain periods based on previous traffic patterns.",
        "B": "Use an A WS Lambda function to scale A mazon ECS based on metric breaches that trigger an A mazon CloudWatch alarm.",
        "C": "Use A mazon EC2 A uto Scaling with simple scaling policies to scale when ECS metric breaches trigger an A mazon CloudWatch alarm.",
        "D": "Use A WS A pplication A uto Scaling with target tracking policies to scale when ECS metric breaches trigger an A mazon CloudWatch alarm."
      },
      "correct_answer": "D",
      "explanation": "A WS A pplication A uto Scaling is a service that can automatically adjust the number of running ECS tasks or services based on specified CloudWatch metrics.\nTarget tracking policies allow you to set a target value for a specific metric, and A WS A pplication A uto Scaling automatically adjusts the desired task count to maintain the target.\nBy using target tracking policies, you can ensure that the ECS cluster scales up or down based on the application's demand while maintaining a balance between cost efficiency and performance."
    },
    {
      "id": "304",
      "question": "A company recently created adisaster recovery site in adifferent A WS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on aperiodic basis. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Use A WS DataSync.",
        "B": "Use A WS Snowball devices.",
        "C": "Set up an SFTP server on A mazon EC2.",
        "D": "Use A WS Database Migration Service (A WS DMS)."
      },
      "correct_answer": "A",
      "explanation": "If we want to transfer large amount of data we can used A WS Datasync."
    },
    {
      "id": "305",
      "question": "A company is designing ashared storage solution for agaming application that is hosted in the A WS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which A WS solution meets these requirements?",
      "options": {
        "A": "Create an A WS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.",
        "B": "Create an A mazon EC2 Windows instance. Install and conFigure a Windows File share role on the instance. Connect the application server to the File share.",
        "C": "Create an A mazon FSx for Windows File Server file system. A ttach the file system to the origin server. Connect the application server to the file system.",
        "D": "Create an A mazon S3 bucket. A ssign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server."
      },
      "correct_answer": "C",
      "explanation": "A mazon FSx for Windows File Server is a fully managed file storage service that is compatible with the Server Message Block (SMB) protocol, making it suitable for use with SMB clients, including Windows-based systems.\nWith A mazon FSx for Windows File Server, you can create a file system that can be mounted on application servers, providing shared storage for the gaming application.\nA mazon FSx for Windows File Server handles the management aspects such as server provisioning, maintenance, and backups, making it a fully managed solution."
    },
    {
      "id": "306",
      "question": "A company wants to run an in-memory database for a latency-sensitive application that runs on A mazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs to provide acost- effective network design that minimizes data transfer charges. Which solution meets these requirements?",
      "options": {
        "A": "Launch all EC2 instances in the same A vailability Zone within the same A WS Region. Specify a placement group with cluster strategy when launching EC2 instances.",
        "B": "Launch all EC2 instances in different A vailability Zones within the same A WS Region. Specify a placement group with partition strategy when launching EC2 instances.",
        "C": "Deploy an A uto Scaling group to launch EC2 instances in different A vailability Zones based on a network utilization target.",
        "D": "Deploy an A uto Scaling group with a step scaling policy to launch EC2 instances in different A vailability Zones."
      },
      "correct_answer": "A",
      "explanation": "A placement group is a logical grouping of instances within a single A vailability Zone. The \"cluster\" strategy for placement groups places instances in close proximity to each other, providing low-latency, high-throughput communication between instances.\nBy launching all EC2 instances in the same A vailability Zone within the same A WS Region, you minimize data transfer charges because data transfer within the same A vailability Zone is not subject to additional costs."
    },
    {
      "id": "307",
      "question": "A company that primarily runs its application servers on premises has decided to migrate to A WS. The company wants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data to remain stored locally. Which A WS solution should the company use to meet these requirements?",
      "options": {
        "A": "A mazon S3 File Gateway",
        "B": "A WS Storage Gateway Tape Gateway",
        "C": "A WS Storage Gateway Volume Gateway stored volumes",
        "D": "A WS Storage Gateway Volume Gateway cached volumes"
      },
      "correct_answer": "D",
      "explanation": "A WS Storage Gateway provides a hybrid cloud storage service that enables on-premises applications to use cloud storage seamlessly.\nVolume Gateway offers two modes: cached volumes and stored volumes.\nIn the cached volumes mode, the entire dataset is stored in A mazon S3, and the most frequently accessed data is cached on-premises. This allows the company to keep recently accessed data locally, minimizing the need for on-premises scaling."
    },
    {
      "id": "308",
      "question": "A company has multiple A WS accounts that use consolidated billing. The company runs several active high performance A mazon RDS for Oracle On-Demand DB instances for 90 days. The company\u2019s Finance team has access to A WS Trusted A dvisor in the consolidated billing account and all other A WS accounts. The Finance team needs to use the appropriate A WS account to access the Trusted A dvisor check recommendations for RDS. The Finance team must review the appropriate Trusted A dvisor check to reduce RDS costs. Which combination of steps should the Finance team take to meet these requirements? (Choose two.)",
      "options": {
        "A": "Use the Trusted A dvisor recommendations from the account where the RDS instances are running.",
        "B": "Use the Trusted A dvisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.",
        "C": "Review the Trusted A dvisor check for A mazon RDS Reserved Instance Optimization.",
        "D": "Review the Trusted A dvisor check for A mazon RDS Idle DB Instances.",
        "E": "Review the Trusted A dvisor check for A mazon Redshift Reserved Node Optimization."
      },
      "correct_answer": "BD",
      "explanation": ""
    },
    {
      "id": "309",
      "question": "A solutions architect needs to optimize storage costs. The solutions architect must identify any A mazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?",
      "options": {
        "A": "A nalyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.",
        "B": "A nalyze bucket access patterns by using the S3 dashboard in the A WS Management Console.",
        "C": "Turn on the A mazon CloudWatch BucketSizeBytes metric for buckets. A nalyze bucket access patterns by using the metrics data with A mazon A thena.",
        "D": "Turn on A WS CloudTrail for S3 object monitoring. A nalyze bucket access patterns by using CloudTrail logs that are integrated with A mazon CloudWatch Logs."
      },
      "correct_answer": "A",
      "explanation": "S3 Storage Lens is a feature of A mazon S3 that provides a detailed set of reports and metrics to help you understand, analyze, and optimize your storage usage.\nThe S3 Storage Lens dashboard provides advanced activity metrics, including insights into access patterns, data transfer, and other storage-related activities.\nBy using the S3 Storage Lens dashboard, you can easily identify buckets that are no longer being accessed or are rarely accessed without the need for additional setup or operational overhead."
    },
    {
      "id": "310",
      "question": "A company sells datasets to customers who do research in artificial intelligence and machine learning (A I/ML). The datasets are large, formatted files that are stored in an A mazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to agiven dataset. The web application is deployed on multiple A mazon EC2 instances behind an A pplication Load Balancer. A fter apurchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North A merica and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. What should A solutions architect do to meet these requirements?",
      "options": {
        "A": "Configure S3 Transfer A cceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer A cceleration endpoint. Continue to use S3 signed URLs for access control.",
        "B": "Deploy an A mazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.",
        "C": "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.",
        "D": "Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application."
      },
      "correct_answer": "B",
      "explanation": "A mazon CloudFront: CloudFront is a content delivery network (CDN) service that distributes content globally with low latency and high data transfer speeds. It helps reduce data transfer costs and improves performance by caching content at edge locations.\nS3 Bucket as the Origin: By configuring the existing S3 bucket as the origin for CloudFront, you allow CloudFront to cache and serve the datasets from edge locations around the world.\nCloudFront Signed URLs: CloudFront provides the ability to generate signed URLs, allowing you to control access to your content. You can use CloudFront signed URLs for access control, providing a secure way for customers to access datasets."
    },
    {
      "id": "311",
      "question": "A company is using A WS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational eficiency and must minimize maintenance. Which solution meets these requirements?",
      "options": {
        "A": "Create multiple A mazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.",
        "B": "Create an A WS Lambda function and an A mazon Simple Notification Service (A mazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.",
        "C": "Create a single A mazon Simple Notification Service (A mazon SNS) topic. Subscribe A mazon Simple Queue Service (A mazon SQS) queues to the SNS topic. Configure SNS message Filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.",
        "D": "Create multiple A mazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an A mazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly."
      },
      "correct_answer": "C",
      "explanation": ""
    },
    {
      "id": "312",
      "question": "A company has an application that runs on several A mazon EC2 instances. Each EC2 instance has multiple A mazon Elastic Block Store (A mazon EBS) data volumes attached to it. The application\u2019s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in adifferent A WS Region. Which solution will meet these requirements in the MOST operationally eficient way?",
      "options": {
        "A": "Write an A WS Lambda function that schedules nightly snapshots of the application\u2019s EBS volumes and copies the snapshots to a different Region.",
        "B": "Create a backup plan by using A WS Backup to perform nightly backups. Copy the backups to another Region. A dd the application\u2019s EC2 instances as resources.",
        "C": "Create a backup plan by using A WS Backup to perform nightly backups. Copy the backups to another Region. A dd the application\u2019s EBS volumes as resources.",
        "D": "Write an A WS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different A vailability Zone."
      },
      "correct_answer": "C",
      "explanation": "A WS Backup: A WS Backup is a fully managed backup service that centralizes and automates the backup of data across A WS services. It provides a simple and efficient way to back up your EC2 instances and their associated EBS volumes.\nBackup Plan: With A WS Backup, you can create backup plans to define when and how your backups are performed. Backup plans allow you to schedule nightly backups and define retention policies."
    },
    {
      "id": "313",
      "question": "A company is building amobile app on A WS. The company wants to expand its reach to millions of users. The company needs to build aplatform so that authorized users can watch the company\u2019s content on their mobile devices. What should A solutions architect recommend to meet these requirements?",
      "options": {
        "A": "Publish content to a public A mazon S3 bucket. Use A WS Key Management Service (A WS KMS) keys to stream content.",
        "B": "Set up IPsec VPN between the mobile app and the A WS environment to stream content.",
        "C": "Use A mazon CloudFront. Provide signed URLs to stream content.",
        "D": "Set up A WS Client VPN between the mobile app and the A WS environment to stream content."
      },
      "correct_answer": "C",
      "explanation": "A mazon CloudFront: CloudFront is a content delivery network (CDN) service provided by A WS. It accelerates the delivery of content by caching it at edge locations globally, reducing latency for end-users.\nSigned URLs: CloudFront supports the generation of signed URLs, which can be used to control access to content. You can create time-limited URLs with specific permissions, allowing only authorized users to access the content."
    },
    {
      "id": "314",
      "question": "A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to A WS without selecting aparticular instance type in anticipation of more users in the future. Which service should A solutions architect recommend?",
      "options": {
        "A": "A mazon A urora MySQL",
        "B": "A mazon A urora Serverless for MySQL",
        "C": "A mazon Redshift Spectrum",
        "D": "A mazon RDS for MySQL"
      },
      "correct_answer": "B",
      "explanation": "A mazon A urora Serverless: A urora Serverless is a fully managed, on-demand, and auto-scaling relational database engine provided by A WS. It is suitable for infrequent access patterns and allows the database to automatically start up, shut down, and scale capacity based on actual usage."
    },
    {
      "id": "315",
      "question": "A company experienced abreach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on A mazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends areport that details the Findings. Which solution will meet these requirements?",
      "options": {
        "A": "Deploy A WS Shield to scan the EC2 instances for vulnerabilities. Create an A WS Lambda function to log any Findings to A WS CloudTrail.",
        "B": "Deploy A mazon Macie and A WS Lambda functions to scan the EC2 instances for vulnerabilities. Log any Findings to A WS CloudTrail.",
        "C": "Turn on A mazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an A WS Lambda function to automate the generation and distribution of reports that detail the Findings.",
        "D": "Turn on A mazon Inspector. Deploy the A mazon Inspector agent to the EC2 instances. Configure an A WS Lambda function to automate the generation and distribution of reports that detail the Findings."
      },
      "correct_answer": "D",
      "explanation": ""
    },
    {
      "id": "316",
      "question": "A company uses an A mazon EC2 instance to run ascript to poll for and process messages in an A mazon Simple Queue Service (A mazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process agrowing number of messages that are added to the queue. What should A solutions architect recommend to meet these requirements?",
      "options": {
        "A": "Increase the size of the EC2 instance to process messages faster.",
        "B": "Use A mazon EventBridge to turn off the EC2 instance when the instance is underutilized.",
        "C": "Migrate the script on the EC2 instance to an A WS Lambda function with the appropriate runtime.",
        "D": "Use A WS Systems Manager Run Command to run the script on demand."
      },
      "correct_answer": "C",
      "explanation": "A WS Lambda: Lambda is a serverless computing service that allows you to run code without provisioning or managing servers. It automatically scales based on the number of incoming requests.\nCost-Efficiency: With Lambda, you only pay for the compute time consumed during code execution. This can be more cost-effective than running and maintaining an EC2 instance, especially for sporadic or event-driven workloads.\nA utomatic Scaling: Lambda automatically scales based on the number of incoming events. A s the number of messages in the SQS queue grows, Lambda can scale out to handle the increased workload.\nEvent-Driven: Lambda is well-suited for event-driven architectures, making it a good fit for scenarios where messages are added to an SQS queue."
    },
    {
      "id": "317",
      "question": "A company uses alegacy application to produce data in CSV format. The legacy application stores the output data in A mazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in A mazon Redshift and A mazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Create an A WS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in A mazon Redshift.",
        "B": "Develop a Python script that runs on A mazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in A mazon S3.",
        "C": "Create an A WS Lambda function and an A mazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.",
        "D": "Use A mazon EventBridge to launch an A mazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an A mazon Redshift table."
      },
      "correct_answer": "A",
      "explanation": ""
    },
    {
      "id": "318",
      "question": "A company recently migrated its entire IT environment to the A WS Cloud. The company discovers that users are provisioning oversized A mazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise astrategy to track and audit these inventory and configuration changes. Which actions should the solutions architect take to meet these requirements? (Choose two.)",
      "options": {
        "A": "Enable A WS CloudTrail and use it for auditing.",
        "B": "Use data lifecycle policies for the A mazon EC2 instances.",
        "C": "Enable A WS Trusted A dvisor and reference the security dashboard.",
        "D": "Enable A WS ConFig and create rules for auditing and compliance purposes.",
        "E": "Restore previous resource conFigurations with an A WS CloudFormation template."
      },
      "correct_answer": "AD",
      "explanation": "D. Enable A WS Config and create rules for auditing and compliance purposes.\nA. Enable A WS CloudTrail and use it for auditing. CloudTrail provides event history of your A WS account activity, including actions taken through the A WS Management Console, A WS Command Line Interface (CLI), and A WS SDKs and A PIs. By enabling CloudTrail, the company can track user activity and changes to A WS resources, and monitor compliance with internal policies and external regulations.\nD. Enable A WS Config and create rules for auditing and compliance purposes. A WS Config provides a detailed inventory of the A WS resources in your account, and continuously records changes to the configurations of those resources. By creating rules in A WS Config, the company can automate the evaluation of resource configurations against desired state, and receive alerts when configurations drift from compliance."
    },
    {
      "id": "319",
      "question": "A company has hundreds of A mazon EC2 Linux-based instances in the A WS Cloud. Systems administrators have used shared SSH keys to manage the instances. A fter arecent audit, the company\u2019s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances. Which solution will meet this requirement with the LEAST amount of administrative overhead?",
      "options": {
        "A": "Use A WS Systems Manager Session Manager to connect to the EC2 instances.",
        "B": "Use A WS Security Token Service (A WS STS) to generate one-time SSH keys on demand.",
        "C": "A llow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.",
        "D": "Use an A mazon Cognito custom authorizer to authenticate users. Invoke an A WS Lambda function to generate a temporary SSH key."
      },
      "correct_answer": "A",
      "explanation": "A WS Systems Manager Session Manager: A WS Systems Manager provides a service called Session Manager that allows you to securely connect to your EC2 instances without the need for an external bastion host or direct access to the instances. Session Manager uses IAM roles for authentication and provides an auditable and controlled way to access instances."
    },
    {
      "id": "320",
      "question": "A company is using aFieet of A mazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-Fiight is lost. The company\u2019s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?",
      "options": {
        "A": "Publish data to A mazon Kinesis Data Streams, Use Kinesis Data A nalytics to query the data.",
        "B": "Publish data to A mazon Kinesis Data Firehose with A mazon Redshift as the destination. Use A mazon Redshift to query the data.",
        "C": "Store ingested data in an EC2 instance store. Publish data to A mazon Kinesis Data Firehose with A mazon S3 as the destination. Use A mazon A thena to query the data.",
        "D": "Store ingested data in an A mazon Elastic Block Store (A mazon EBS) volume. Publish data to A mazon ElastiCache for Redis. Subscribe to the Redis channel to query the data."
      },
      "correct_answer": "A",
      "explanation": "A mazon Kinesis Data Streams: It is a scalable and durable real-time data streaming service. It allows you to ingest, buffer, and process streaming data in real time. In this scenario, you can publish data to a Kinesis Data Stream from your EC2 instances.\nKinesis Data A nalytics: It provides an SQL-like language for querying and analyzing data in real time. You can use Kinesis Data A nalytics to process and analyze the ingested data directly within the Kinesis pipeline."
    },
    {
      "id": "321",
      "question": "What should A solutions architect do to ensure that all objects uploaded to an A mazon S3 bucket are encrypted?",
      "options": {
        "A": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.",
        "B": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.",
        "C": "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.",
        "D": "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set."
      },
      "correct_answer": "D",
      "explanation": "x-amz-server-side-encryption header: This header specifies the server-side encryption algorithm to be used for the object. If an object is uploaded without the x-amz-server-side-encryption header or with an incorrect value, it can be denied."
    },
    {
      "id": "322",
      "question": "A solutions architect is designing a multi-tier application for A company. The application's users upload images from amobile device. The application generates athumbnail of each image and returns amessage to the user to confirm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide afaster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers. What should the solutions architect do to meet these requirements?",
      "options": {
        "A": "Write a custom A WS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.",
        "B": "Create an A WS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.",
        "C": "Create an A mazon Simple Queue Service (A mazon SQS) message queue. A s images are uploaded, place a message on the SQS queue for thumbnail generation. A lert the user through an application message that the image was received.",
        "D": "Create A mazon Simple Notification Service (A mazon SNS) notiFication topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notiFication after thumbnail generation is complete."
      },
      "correct_answer": "C",
      "explanation": "A mazon SQS (Simple Queue Service): SQS is a fully managed message queuing service that enables decoupling of the components of a cloud application. By creating an SQS message queue, the image upload process can place messages in the queue for thumbnail generation."
    },
    {
      "id": "323",
      "question": "A company\u2019s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send amessage over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design asystem to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company\u2019s security team to analyze. Which system architecture should the solutions architect recommend?",
      "options": {
        "A": "Launch an A mazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an A mazon S3 bucket.",
        "B": "Create an HTTPS endpoint in A mazon A PI Gateway. Configure the A PI Gateway endpoint to invoke an A WS Lambda function to process the messages and save the results to an A mazon DynamoDB table.",
        "C": "Use A mazon Route 53 to direct incoming sensor messages to an A WS Lambda function. Configure the Lambda function to process the messages and save the results to an A mazon DynamoDB table.",
        "D": "Create a gateway VPC endpoint for A mazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint."
      },
      "correct_answer": "B",
      "explanation": ""
    },
    {
      "id": "324",
      "question": "A company wants to implement adisaster recovery plan for its primary on-premises File storage volume. The File storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on alocal storage server. The File storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all File types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?",
      "options": {
        "A": "Provision an A mazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an A mazon EC2 instance and mount the S3 bucket that contains the files.",
        "B": "Provision an A WS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an A mazon EC2 instance and restore the data to an A mazon Elastic Block Store (A mazon EBS) volume from the volumes in the virtual tape library.",
        "C": "Provision an A WS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing File server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an A mazon Elastic Block Store (A mazon EBS) volume and attach the EBS volume to an A mazon EC2 instance.",
        "D": "Provision an A WS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing File storage volume. Mount the Volume Gateway stored volume to the existing File server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an A mazon Elastic Block Store (A mazon EBS) volume and attach the EBS volume to an A mazon EC2 instance."
      },
      "correct_answer": "C",
      "explanation": ""
    },
    {
      "id": "325",
      "question": "A company is hosting a web application from an A mazon S3 bucket. The application uses A mazon Cognito as an identity provider to authenticate users and return aJSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket. Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content. Which solution meets these requirements?",
      "options": {
        "A": "Update the A mazon Cognito identity pool to assume the proper IAM role for access to the protected content.",
        "B": "Update the S3 A CL to allow the application to access the protected content.",
        "C": "Redeploy the application to A mazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.",
        "D": "Update the A mazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content."
      },
      "correct_answer": "A",
      "explanation": "A mazon Cognito Identity Pool: When users authenticate through A mazon Cognito, they assume roles that determine their access to A WS resources. By updating the Cognito identity pool, you can configure it to assume the proper IAM role that has the necessary permissions to access the protected content stored in the S3 bucket.\nIAM Role Permissions: The IAM role associated with the identity pool should have the required permissions (e.g., S3 getObject permissions) to access the protected content in the S3 bucket."
    },
    {
      "id": "326",
      "question": "A n image hosting company uploads its large assets to A mazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 A PIs and overwrites if the same object is uploaded again. For the First 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets. Which combination of actions should A solutions architect recommend to meet these requirements? (Choose two.)",
      "options": {
        "A": "Move assets to S3 Intelligent-Tiering after 30 days.",
        "B": "Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.",
        "C": "Configure an S3 Lifecycle policy to clean up expired object delete markers.",
        "D": "Move assets to S3 Standard-Infrequent A ccess (S3 Standard-IA) after 30 days.",
        "E": "Move assets to S3 One Zone-Infrequent A ccess (S3 One Zone-IA) after 30 days."
      },
      "correct_answer": "AB",
      "explanation": "B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.\nMove assets to S3 Intelligent-Tiering after 30 days: This option is suitable for objects with unknown or changing access patterns. S3 Intelligent-Tiering automatically moves objects between two access tiers (frequent and infrequent access) based on changing access patterns. It helps optimize costs by automatically selecting the most cost-effective tier for each object.\nConfigure an S3 Lifecycle policy to clean up incomplete multipart uploads: This is a good practice to clean up any incomplete multipart uploads, which can consume additional storage space without contributing to the actual objects. Cleaning up incomplete uploads helps manage storage costs efficiently."
    },
    {
      "id": "327",
      "question": "A solutions architect must secure a VPC network that hosts A mazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. A ccording to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party\u2019s URL. Other internet traffic must be blocked. Which solution meets these requirements?",
      "options": {
        "A": "Update the route table for the private subnet to route the outbound traffic to an A WS Network Firewall Firewall. Configure domain list rule groups.",
        "B": "Set up an A WS WAF web A CL. Create a custom set of rules that Filter traffic requests based on source and destination IP address range sets.",
        "C": "Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.",
        "D": "Configure an A pplication Load Balancer (A LB) in front of the EC2 instances. Direct all outbound traffic to the A LB. Use a URL-based rule listener in the A LB\u2019s target group for outbound access to the internet."
      },
      "correct_answer": "A",
      "explanation": ""
    },
    {
      "id": "328",
      "question": "A company is hosting a three-tier ecommerce application in the A WS Cloud. The company hosts the website on A mazon S3 and integrates the website with an A PI that handles sales requests. The company hosts the A PI on three A mazon EC2 instances behind an A pplication Load Balancer (A LB). The A PI consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting asignificant and sudden increase in the number of sales requests during events for the launch of new products. What should A solutions architect recommend to ensure that all the requests are processed successfully?",
      "options": {
        "A": "A dd an A mazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.",
        "B": "A dd an A mazon CloudFront distribution for the static content. Place the EC2 instances in an A uto Scaling group to launch new instances based on network traffic.",
        "C": "A dd an A mazon CloudFront distribution for the dynamic content. A dd an A mazon ElastiCache instance in front of the A LB to reduce traffic for the A PI to handle.",
        "D": "A dd an A mazon CloudFront distribution for the static content. A dd an A mazon Simple Queue Service (A mazon SQS) queue to receive requests from the website for later processing by the EC2 instances."
      },
      "correct_answer": "B",
      "explanation": "A mazon CloudFront for Static Content: By using CloudFront, you can distribute static content (like images, stylesheets) globally, reducing latency for end-users and offloading some of the traffic from your backend instances.\nA uto Scaling Group: A n A uto Scaling group allows you to automatically adjust the number of EC2 instances to handle changes in demand. By placing the EC2 instances in an A uto Scaling group, you can dynamically scale the number of instances based on network traffic, ensuring that the application can handle increased load during events."
    },
    {
      "id": "329",
      "question": "A security audit reveals that A mazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large Fieet of EC2 instances. The solution should also patch the EC2 instances on aregular schedule and provide areport of each instance\u2019s patch status. Which solution will meet these requirements?",
      "options": {
        "A": "Set up A mazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.",
        "B": "Turn on A mazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up A WS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.",
        "C": "Set up A mazon Detective to scan the EC2 instances for software vulnerabilities. Set up an A mazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.",
        "D": "Turn on A mazon Inspector in the account. Configure A mazon Inspector to scan the EC2 instances for software vulnerabilities. Set up A WS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule."
      },
      "correct_answer": "D",
      "explanation": ""
    },
    {
      "id": "330",
      "question": "A company is planning to store data on A mazon RDS DB instances. The company must encrypt the data at rest. What should A solutions architect do to meet this requirement?",
      "options": {
        "A": "Create a key in A WS Key Management Service (A WS KMS). Enable encryption for the DB instances.",
        "B": "Create an encryption key. Store the key in A WS Secrets Manager. Use the key to encrypt the DB instances.",
        "C": "Generate a certiFicate in A WS CertiFicate Manager (A CM). Enable SSL/TLS on the DB instances by using the certiFicate.",
        "D": "Generate a certiFicate in A WS Identity and A ccess Management (IAM). Enable SSL/TLS on the DB instances by using the certiFicate."
      },
      "correct_answer": "A",
      "explanation": "By creating a key in A WS KMS and enabling encryption for the RDS DB instances, you ensure that the data at rest is encrypted using the specified key.\nA WS RDS supports encryption at rest, and you can use A WS KMS to manage the encryption keys. When you enable encryption for an RDS DB instance, you can specify a KMS key to use for encryption."
    },
    {
      "id": "331",
      "question": "A company must migrate 20 TB of data from a data center to the A WS Cloud within 30 days. The company\u2019s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should A solutions architect do to meet these requirements?",
      "options": {
        "A": "Use A WS Snowball.",
        "B": "Use A WS DataSync.",
        "C": "Use a secure VPN connection.",
        "D": "Use A mazon S3 Transfer A cceleration."
      },
      "correct_answer": "A",
      "explanation": "A WS Snowball is a physical data transport solution that helps customers transfer large amounts of data into and out of A WS. It addresses challenges associated with large-scale data transfers, particularly when network constraints, transfer times, or security concerns make online data transfer less practical."
    },
    {
      "id": "332",
      "question": "A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees\u2019 devices. The files are stored in an on-premises Windows File server. However, due to an increase in remote usage, the File server is running out of capacity. . Which solution will meet these requirements?",
      "options": {
        "A": "Migrate the File server to an A mazon EC2 instance in a public subnet. Configure the security group to limit inbound traffic to the employees\u2019 IP addresses.",
        "B": "Migrate the files to an A mazon FSx for Windows File Server file system. Integrate the A mazon FSx file system with the on-premises A ctive Directory. Configure A WS Client VPN.",
        "C": "Migrate the files to A mazon S3, and create a private VPC endpoint. Create a signed URL to allow download.",
        "D": "Migrate the files to A mazon S3, and create a public VPC endpoint. A llow employees to sign on with A WS IAM Identity Center (A WS Single Sign-On)."
      },
      "correct_answer": "B",
      "explanation": "A mazon FSx for Windows File Server: It is a fully managed file storage service built on Windows Server. It is designed to be integrated with on-premises A ctive Directory, allowing for a seamless extension of your existing directory and authentication infrastructure to the A WS Cloud.\nIntegrate with On-Premises A ctive Directory: With A mazon FSx, you can integrate the file system with your on-premises A ctive Directory, ensuring that the same user accounts and permissions are used both on-premises and in the cloud."
    },
    {
      "id": "333",
      "question": "A company\u2019s application runs on A mazon EC2 instances behind an A pplication Load Balancer (A LB). The instances run in an A mazon EC2 A uto Scaling group across multiple A vailability Zones. On the First day of every month at midnight, the application becomes much slower when the month-end Financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application. What should A solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?",
      "options": {
        "A": "Configure an A mazon CloudFront distribution in front of the A LB.",
        "B": "Configure an EC2 A uto Scaling simple scaling policy based on CPU utilization.",
        "C": "Configure an EC2 A uto Scaling scheduled scaling policy based on the monthly schedule.",
        "D": "Configure A mazon ElastiCache to remove some of the workload from the EC2 instances."
      },
      "correct_answer": "C",
      "explanation": "By configuring a scheduled scaling policy, the EC2 A uto Scaling group can proactively launch additional EC2 instances before the CPU utilization peaks to 100%. This will ensure that the application can handle the workload during the month-end financial calculation batch, and avoid any disruption or downtime.\nConfiguring a simple scaling policy based on CPU utilization or adding A mazon CloudFront distribution or A mazon ElastiCache will not directly address the issue of handling the monthly peak workload."
    },
    {
      "id": "334",
      "question": "A company wants to give acustomer the ability to use on-premises Microsoft A ctive Directory to download files that are stored in A mazon S3. The customer\u2019s application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer\u2019s application?",
      "options": {
        "A": "Set up A WS Transfer Family with SFTP for A mazon S3. Configure integrated A ctive Directory authentication.",
        "B": "Set up A WS Database Migration Service (A WS DMS) to synchronize the on-premises client with A mazon S3. Configure integrated A ctive Directory authentication.",
        "C": "Set up A WS DataSync to synchronize between the on-premises location and the S3 location by using A WS IAM Identity Center (A WS Single Sign-On).",
        "D": "Set up a Windows A mazon EC2 instance with SFTP to connect the on-premises client with A mazon S3. Integrate A WS Identity and A ccess Management (IAM)."
      },
      "correct_answer": "A",
      "explanation": "A WS Transfer Family with SFTP for A mazon S3: A WS Transfer Family is a fully managed service that allows you to set up an SFTP (Secure File Transfer Protocol) service for A mazon S3. It enables you to transfer files directly to and from A mazon S3 using the SFTP protocol.\nIntegrated A ctive Directory A uthentication: A WS Transfer Family allows you to configure authentication with Microsoft A ctive Directory. By integrating with A ctive Directory, you can provide users with seamless access to S3 resources using their existing credentials without modifying their applications."
    },
    {
      "id": "335",
      "question": "A company is experiencing sudden increases in demand. The company needs to provision large A mazon EC2 instances from an A mazon Machine Image (A MI). The instances will run in an A uto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?",
      "options": {
        "A": "Use the aws ec2 register-image command to create an A MI from a snapshot. Use A WS Step Functions to replace the A MI in the A uto Scaling group.",
        "B": "Enable A mazon Elastic Block Store (A mazon EBS) fast snapshot restore on a snapshot. Provision an A MI by using the snapshot. Replace the A MI in the A uto Scaling group with the new A MI.",
        "C": "Enable A MI creation and deFine lifecycle rules in A mazon Data Lifecycle Manager (A mazon DLM). Create an A WS Lambda function that modiFies the A MI in the A uto Scaling group.",
        "D": "Use A mazon EventBridge to invoke A WS Backup lifecycle policies that provision A MIs. Configure A uto Scaling group capacity limits as an event source in EventBridge."
      },
      "correct_answer": "B",
      "explanation": "A mazon EBS Fast Snapshot Restore: Enabling fast snapshot restore allows you to provision A mazon EBS volumes based on snapshots with faster performance. This is particularly useful when creating A MIs from snapshots, as it reduces the time it takes to create EBS volumes from those snapshots.\nMinimum Initialization Latency: Fast snapshot restore helps in minimizing initialization latency as it provides a way to quickly create EBS volumes from snapshots.\nProvisioning A MI from Snapshot: You can create an A mazon Machine Image (A MI) from an A mazon EBS snapshot. This allows you to capture a point-in-time snapshot of the file system, and then use that snapshot to create new instances."
    },
    {
      "id": "336",
      "question": "A company hosts a multi-tier web application that uses an A mazon A urora MySQL DB cluster for storage. The application tier is hosted on A mazon EC2 instances. The company\u2019s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should A solutions architect do to meet this requirement with the LEAST operational effort?",
      "options": {
        "A": "Create a new A WS Key Management Service (A WS KMS) encryption key. Use A WS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. A ssociate the secret with the A urora DB cluster. Configure a custom rotation period of 14 days.",
        "B": "Create two parameters in A WS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select A WS Key Management Service (A WS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an A WS Lambda function that rotates the password every 14 days.",
        "C": "Store a File that contains the credentials in an A WS Key Management Service (A WS KMS) encrypted A mazon Elastic File System (A mazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the File on the file system so that the application can read the File and that only super users can modify the File. Implement an A WS Lambda function that rotates the key in A urora every 14 days and writes new credentials into the File.",
        "D": "Store a File that contains the credentials in an A WS Key Management Service (A WS KMS) encrypted A mazon S3 bucket that the application uses to load the credentials. Download the File to the application regularly to ensure that the correct credentials are used. Implement an A WS Lambda function that rotates the A urora credentials every 14 days and uploads these credentials to the File in the S3 bucket."
      },
      "correct_answer": "A",
      "explanation": "A proposes to create a new A WS KMS encryption key and use A WS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Then, the secret will be associated with the A urora DB cluster, and a custom rotation period of 14 days will be configured. A WS Secrets Manager will automate the process of rotating the database credentials, which will reduce the operational effort required to meet the IT security guidelines."
    },
    {
      "id": "337",
      "question": "A company has deployed a web application on A WS. The company hosts the backend database on A mazon RDS for MySQL with a primary DB instance and Five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. A s traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?",
      "options": {
        "A": "Migrate the database to A mazon A urora MySQL. Replace the read replicas with A urora Replicas, and conFigure A urora A uto Scaling. Replace the stored procedures with A urora MySQL native functions.",
        "B": "Deploy an A mazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with A WS Lambda functions.",
        "C": "Migrate the database to a MySQL database that runs on A mazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.",
        "D": "Migrate the database to A mazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and conFigure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams."
      },
      "correct_answer": "A",
      "explanation": "A mazon A urora MySQL: A urora Replicas in A mazon A urora MySQL are designed to have minimal replication lag compared to traditional MySQL read replicas. A urora is built for high performance and low replication lag, making it a suitable choice for reducing lag in read replicas.\nA urora A uto Scaling: A urora A uto Scaling allows you to automatically adjust the number of A urora Replicas based on actual application usage. This ensures that you have the right amount of read capacity during periods of peak load, minimizing replication lag."
    },
    {
      "id": "338",
      "question": "A solutions architect must create adisaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. A ll data for the platform is stored in an A mazon A urora MySQL DB cluster. The DR plan must replicate data to a secondary A WS Region. Which solution will meet these requirements MOST cost-effectively?",
      "options": {
        "A": "Use MySQL binary log replication to an A urora cluster in the secondary Region. Provision one DB instance for the A urora cluster in the secondary Region.",
        "B": "Set up an A urora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.",
        "C": "Use A WS Database Migration Service (A WS DMS) to continuously replicate data to an A urora cluster in the secondary Region. Remove the DB instance from the secondary Region.",
        "D": "Set up an A urora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region."
      },
      "correct_answer": "D",
      "explanation": ""
    },
    {
      "id": "339",
      "question": "A company has acustom application with embedded credentials that retrieves information from an A mazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should A solutions architect do to meet these requirements?",
      "options": {
        "A": "Use A WS Key Management Service (A WS KMS) to create keys. Configure the application to load the database credentials from A WS KMS. Enable automatic key rotation.",
        "B": "Create credentials on the RDS for MySQL database for the application user and store the credentials in A WS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an A WS Lambda function that rotates the credentials in Secret Manager.",
        "C": "Create credentials on the RDS for MySQL database for the application user and store the credentials in A WS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.",
        "D": "Create credentials on the RDS for MySQL database for the application user and store the credentials in A WS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store."
      },
      "correct_answer": "C",
      "explanation": "C is a valid solution for securing the custom application with the least amount of programming effort. It involves creating credentials on the RDS for MySQL database for the application user and storing them in A WS Secrets Manager. The application can then be configured to load the database credentials from Secrets Manager. A dditionally, the solution includes setting up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager, which will automatically rotate the credentials at a specified interval without requiring any programming effort."
    },
    {
      "id": "340",
      "question": "A media company hosts its website on A WS. The website application\u2019s architecture includes aFieet of A mazon EC2 instances behind an A pplication Load Balancer (A LB) and a database that is hosted on A mazon A urora. The company\u2019s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?",
      "options": {
        "A": "Use A WS WAF in front of the A LB. A ssociate the appropriate web A CLs with A WS WAF.",
        "B": "Create an A LB listener rule to reply to SQL injections with a Fixed response.",
        "C": "Subscribe to A WS Shield A dvanced to block all SQL injection attempts automatically.",
        "D": "Set up A mazon Inspector to block all SQL injection attempts automatically."
      },
      "correct_answer": "A",
      "explanation": "A WS WAF (Web A pplication Firewall): A WS WAF is designed to protect web applications from common web exploits, including SQL injection. It allows you to create web access control lists (web A CLs) to define rules that filter and monitor HTTP traffic to your application.\nA ssociating Web A CLs with A WS WAF: By using A WS WAF in front of the A LB, you can define rules to block or allow web requests based on conditions that you specify. This includes protection against SQL injection attempts. A WS WAF provides a range of conditions and rulesets that you can use to mitigate common security threats."
    },
    {
      "id": "341",
      "question": "A company has an A mazon S3 data lake that is governed by A WS Lake Formation. The company wants to create avisualization in A mazon Quicksight by joining the data in the data lake with operational data that is stored in an A mazon A urora MySQL database. The company wants to enforce column-level authorization so that the company\u2019s marketing team can access only asubset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Use A mazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.",
        "B": "Use A WS Glue Studio to ingest the data from the database to the S3 data lake. A ttach an IAM policy to the QuickSight users to enforce column-level access control. Use A mazon S3 as the data source in QuickSight.",
        "C": "Use A WS Glue Elastic Views to create a materialized view for the database in A mazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use A mazon S3 as the data source in QuickSight.",
        "D": "Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use A mazon A thena as the data source in QuickSight."
      },
      "correct_answer": "D",
      "explanation": ""
    },
    {
      "id": "342",
      "question": "A transaction processing company has weekly scripted batch jobs that run on A mazon EC2 instances. The EC2 instances are in an A uto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run. Currently, engineers complete this task by manually modifying the A uto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the A uto Scaling group counts. The company needs an automated way to modify the A uto Scaling group\u2019s desired capacity. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Create a dynamic scaling policy for the A uto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.",
        "B": "Create a scheduled scaling policy for the A uto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.",
        "C": "Create a predictive scaling policy for the A uto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.",
        "D": "Create an A mazon EventBridge event to invoke an A WS Lambda function when the CPU utilization metric value for the A uto Scaling group reaches 60%. Configure the Lambda function to increase the A uto Scaling group\u2019s desired capacity and maximum capacity by 20%."
      },
      "correct_answer": "C",
      "explanation": "In general, if you have regular patterns of traffic increases and applications that take a long time to initialize, you should consider using predictive scaling. Predictive scaling can help you scale faster by launching capacity in advance of forecasted load, compared to using only dynamic scaling, which is reactive in nature."
    },
    {
      "id": "343",
      "question": "A solutions architect is designing A company\u2019s disaster recovery (DR) architecture. The company has aMySQL database that runs on an A mazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple A WS Regions. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.",
        "B": "Migrate the MySQL database to A mazon RDS. Use a Multi-A Z deployment. Turn on read replication for the primary DB instance in the different A vailability Zones.",
        "C": "Migrate the MySQL database to an A mazon A urora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.",
        "D": "Store the scheduled backup of the MySQL database in an A mazon S3 bucket that is conFigured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region."
      },
      "correct_answer": "C",
      "explanation": ""
    },
    {
      "id": "344",
      "question": "A company has aJava application that uses A mazon Simple Queue Service (A mazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?",
      "options": {
        "A": "Use the A mazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in A mazon S3.",
        "B": "Use A mazon EventBridge to post large messages from the application instead of A mazon SQS.",
        "C": "Change the limit in A mazon SQS to handle messages that are larger than 256 KB.",
        "D": "Store messages that are larger than 256 KB in A mazon Elastic File System (A mazon EFS). Configure A mazon SQS to reference this location in the messages."
      },
      "correct_answer": "A",
      "explanation": "A mazon SQS Extended Client Library for Java: This library is specifically designed to handle larger messages in A mazon SQS by transparently offloading them to A mazon S3. It allows you to send a reference to the S3 object in the SQS message while keeping the actual payload in S3.\nMinimal Code Changes: Using the A mazon SQS Extended Client Library for Java requires minimal changes to the existing code. Developers need to integrate the library, and the library itself handles the details of storing and retrieving large messages from A mazon S3."
    },
    {
      "id": "345",
      "question": "A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on A WS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?",
      "options": {
        "A": "Use A mazon Cognito for authentication. Use Lambda@Edge for authorization. Use A mazon CloudFront to serve the web application globally.",
        "B": "Use A WS Directory Service for Microsoft A ctive Directory for authentication. Use A WS Lambda for authorization. Use an A pplication Load Balancer to serve the web application globally.",
        "C": "Use A mazon Cognito for authentication. Use A WS Lambda for authorization. Use A mazon S3 Transfer A cceleration to serve the web application globally.",
        "D": "Use A WS Directory Service for Microsoft A ctive Directory for authentication. Use Lambda@Edge for authorization. Use A WS Elastic Beanstalk to serve the web application globally."
      },
      "correct_answer": "A",
      "explanation": "A mazon Cognito for A uthentication: A mazon Cognito is a fully managed service for user identity and access control. It provides easy integration for authentication with a serverless architecture and supports a user pool for fewer than 100 users.\nLambda@Edge for A uthorization: Lambda@Edge allows you to run custom code in response to CloudFront events, including authorization. You can implement authorization logic at the edge locations closest to the end-users, providing low-latency access.\nA mazon CloudFront for Content Delivery: A mazon CloudFront is a global content delivery network (CDN) that integrates seamlessly with Lambda@Edge. CloudFront can serve the web application globally, distributing content from edge locations for low-latency access."
    },
    {
      "id": "346",
      "question": "A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array\u2019s support contract. Some of the data is accessed frequently, but much of the data is inactive. A solutions architect needs to implement a solution that migrates the data to A mazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified A WS Storage Gateway as part of the solution. Which type of storage gateway should the solutions architect provision to meet these requirements?",
      "options": {
        "A": "Volume Gateway",
        "B": "Tape Gateway",
        "C": "A mazon FSx File Gateway",
        "D": "A mazon S3 File Gateway"
      },
      "correct_answer": "D",
      "explanation": "A mazon S3 File Gateway provides on-premises applications with access to virtually unlimited cloud storage using NFS and SMB file interfaces. It seamlessly moves frequently accessed data to a low-latency cache while storing colder data in A mazon S3, using S3 Lifecycle policies to transition data between storage classes over tim"
    },
    {
      "id": "347",
      "question": "A company has an application that is running on A mazon EC2 instances. A solutions architect has standardized the company on aparticular instance family and various instance sizes based on the current needs of the company. The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage. Which solution will meet these requirements MOST cost-effectively?",
      "options": {
        "A": "Compute Savings Plan",
        "B": "EC2 Instance Savings Plan",
        "C": "Zonal Reserved Instances",
        "D": "Standard Reserved Instances"
      },
      "correct_answer": "A",
      "explanation": "Compute Savings Plans provide significant cost savings over On-Demand pricing in exchange for a commitment to a consistent amount of compute usage (measured in $/hr) for a 1 or 3 year period. They offer flexibility by allowing you to switch between instance families, sizes, and A Zs (A vailability Zones) while still benefiting from the savings plan pricing. This aligns well with the company's requirement to change instance family and sizes based on application needs."
    },
    {
      "id": "348",
      "question": "A company collects data from a large number of participants who use wearable devices. The company stores the data in an A mazon DynamoDB table and uses applications to analyze the data. The data workload is constant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB. Which solution will meet these requirements MOST cost-effectively?",
      "options": {
        "A": "Use provisioned mode and DynamoDB Standard-Infrequent A ccess (DynamoDB Standard-IA). Reserve capacity for the forecasted workload.",
        "B": "Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).",
        "C": "Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to accommodate changes in the workload.",
        "D": "Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with reserved capacity."
      },
      "correct_answer": "B",
      "explanation": "In provisioned mode, you provision a specific amount of read and write capacity, which allows you to manage costs more effectively based on your expected workload. This approach is suitable when your workload is predictable, as you can provision the capacity to meet your known requirements. DynamoDB Standard-Infrequent A ccess (Option A) is designed for cost savings on long-term storage and retrieval of infrequently accessed data, and it might not be the best fit for a constant and predictable workload."
    },
    {
      "id": "349",
      "question": "A company stores confidential data in an A mazon A urora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an A WS Key Management Service (A WS KMS) customer managed key. The company was recently acquired and must securely share abackup of the database with the acquiring company\u2019s A WS account in ap-southeast-3. What should A solutions architect do to meet these requirements?",
      "options": {
        "A": "Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company\u2019s A WS account.",
        "B": "Create a database snapshot. A dd the acquiring company\u2019s A WS account to the KMS key policy. Share the snapshot with the acquiring company\u2019s A WS account.",
        "C": "Create a database snapshot that uses a different A WS managed KMS key. A dd the acquiring company\u2019s A WS account to the KMS key alias. Share the snapshot with the acquiring company's A WS account.",
        "D": "Create a database snapshot. Download the database snapshot. Upload the database snapshot to an A mazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company\u2019s A WS account."
      },
      "correct_answer": "B",
      "explanation": "sharing encrypted snapshots involves granting permission not only on the snapshot itself but also on the underlying A WS Key Management Service (KMS) key used for encryption.\nBy adding the acquiring company's A WS account to the KMS key policy, you ensure that they have the necessary permissions to decrypt and access the snapshot.\nSharing the snapshot with the acquiring company's A WS account completes the process, allowing them to restore the database from the shared snapshot."
    },
    {
      "id": "350",
      "question": "A company uses a 100 GB A mazon RDS for Microsoft SQL Server Single-A Z DB instance in the us-east-1 Region to store customer transactions. The company needs high availability and automatic recovery for the DB instance. The company must also run reports on the RDS database several times ayear. The report process causes transactions to take longer than usual to post to the customers\u2019 accounts. The company needs a solution that will improve the performance of the report process. Which combination of steps will meet these requirements? (Choose two.)",
      "options": {
        "A": "Modify the DB instance from a Single-A Z DB instance to a Multi-A Z deployment.",
        "B": "Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another A vailability Zone.",
        "C": "Create a read replica of the DB instance in a different A vailability Zone. Point all requests for reports to the read replica.",
        "D": "Migrate the database to RDS Custom.",
        "E": "Use RDS Proxy to limit reporting requests to the maintenance window."
      },
      "correct_answer": "AC",
      "explanation": "Enabling Multi-A Z deployment provides high availability by replicating the database to a standby instance in another A vailability Zone. This helps in automatic failover and recovery in case of a primary instance failure.\nC. Create a read replica of the DB instance in a different A vailability Zone. Point all requests for reports to the read replica:\nBy creating a read replica in a different A vailability Zone, you offload the reporting workload from the primary instance, reducing the impact on transaction processing. Read replicas can be used to scale read-heavy workloads and improve overall performance."
    }
  ]
}