{
  "questions": [
    {
      "id": "351",
      "question": "A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workfiow. The company also wants to minimize operational overhead. Which solution will meet these requirements?",
      "options": {
        "A": "Build out the workFiow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workFiow steps.",
        "B": "Build out the workFiow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workFiow steps on the EC2 instances.",
        "C": "Build out the workFiow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workFiow steps.",
        "D": "Build out the workFiow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workFiow steps."
      },
      "correct_answer": "D",
      "explanation": "AWS Step Functions allows you to coordinate the components of distributed applications using visual workflows. It is a fully managed service, which means you don't need to worry about operational overhead.\nState machines in AWS Step Functions enable you to define the workflow of your application by specifying a series of steps. Each step can invoke an AWS Lambda function, among other things.\nAWS Lambda is a serverless compute service, and it automatically scales with the workload. This aligns with the goal of using serverless concepts and minimizing operational overhead."
    },
    {
      "id": "352",
      "question": "A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users ahigh-quality gaming experience. Which solution will meet these requirements?",
      "options": {
        "A": "Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.",
        "B": "Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.",
        "C": "Set up Amazon CloudFront with UDP turned on. ConFigure an origin in each Region.",
        "D": "Set up a VPC peering mesh between each Region. Turn on UDP for each VPC."
      },
      "correct_answer": "B",
      "explanation": "AWS Global Accelerator is designed to improve the availability and performance of applications by using static IP addresses (Anycast) and directing traffic over the AWS global network. It provides low-latency and high-performance routing, making it well-suited for applications with a global user base, such as multi-player games.\nBy setting up UDP listeners and endpoint groups in each Region with AWS Global Accelerator, you can efficiently route traffic to the nearest game servers, reducing latency and improving the overall gaming experience."
    },
    {
      "id": "353",
      "question": "A company hosts athree-tier web application on Amazon EC2 instances in asingle Availability Zone. The web application uses aself-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects trafic of 1,000 IOPS for both reads and writes at peak trafic. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to afully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?",
      "options": {
        "A": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.",
        "B": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.",
        "C": "Use Amazon S3 Intelligent-Tiering access tiers.",
        "D": "Use two large EC2 instances to host the database in active-passive mode."
      },
      "correct_answer": "B",
      "explanation": ""
    },
    {
      "id": "354",
      "question": "A company hosts aserverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak trafic or unpredictable trafic. The company needs asolution that reduces the application failures with the least amount of change to the code. What should A solutions architect do to meet these requirements?",
      "options": {
        "A": "Reduce the Lambda concurrency rate.",
        "B": "Enable RDS Proxy on the RDS DB instance.",
        "C": "Resize the RDS DB instance class to accept more connections.",
        "D": "Migrate the database to Amazon DynamoDB with on-demand scaling."
      },
      "correct_answer": "B",
      "explanation": "RDS Proxy is a fully managed, highly available database proxy that can handle database connections for serverless and highly scalable applications. It helps manage database connections efficiently, reducing issues related to connection timeouts and errors."
    },
    {
      "id": "355",
      "question": "A company is migrating an old application to AWS. The application runs abatch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory. Which solution will run the batch job within 15 minutes with the LEAST operational overhead?",
      "options": {
        "A": "Use AWS Lambda with functional scaling.",
        "B": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
        "C": "Use Amazon Lightsail with AWS Auto Scaling.",
        "D": "Use AWS Batch on Amazon EC2."
      },
      "correct_answer": "D",
      "explanation": "AWS Batch on Amazon EC2: AWS Batch is a fully managed service for batch computing that dynamically provisions the optimal quantity and type of compute resources (Amazon EC2 instances) based on the volume and specific resource requirements of the batch jobs. If the batch job is CPU-intensive and can be parallelized, AWS Batch can efficiently manage the compute resources needed for the job, and it provides a higher level of control over the environment compared to serverless options like AWS Lambda."
    },
    {
      "id": "356",
      "question": "A company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the same high availability and resiliency, but the company wants to minimize storage costs. Which storage solution will meet these requirements?",
      "options": {
        "A": "Move the data objects to S3 Glacier Deep Archive after 30 days.",
        "B": "Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
        "C": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
        "D": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately."
      },
      "correct_answer": "B",
      "explanation": "S3 Standard-Infrequent Access (S3 Standard-IA): This storage class is designed for infrequently accessed data but still provides low-latency and high-throughput performance. It maintains the same high availability and durability as S3 Standard, making it suitable for data that is accessed less frequently."
    },
    {
      "id": "357",
      "question": "A gaming company is moving its public scoreboard from adata center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs ahighly available storage solution for the application. The application consists of static Files and dynamic server-side code. Which combination of steps should A solutions architect take to meet these requirements? (Choose two.)",
      "options": {
        "A": "Store the static Files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.",
        "B": "Store the static Files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.",
        "C": "Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the Files.",
        "D": "Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the Files.",
        "E": "Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the Files."
      },
      "correct_answer": "AD",
      "explanation": "D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.\nAmazon S3 is a highly scalable and durable object storage service, and it is well-suited for storing static files. Using CloudFront as a content delivery network (CDN) improves the delivery of static content by caching objects at edge locations, reducing latency for end users.\nAmazon FSx for Windows File Server provides a fully managed Windows file system that is accessible from Windows-based EC2 instances. This is suitable for storing dynamic server-side code that requires file sharing across multiple instances. It offers high availability and supports Windows-native features."
    },
    {
      "id": "358",
      "question": "Asocial media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon Cloudfront distribution. The application has more than abillion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Install an external image management library on an EC2 instance. Use the image management library to process the images.",
        "B": "Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.",
        "C": "Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.",
        "D": "Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request."
      },
      "correct_answer": "C",
      "explanation": "Lambda@Edge: Allows you to run code in response to CloudFront events without provisioning or managing servers. In this case, a Lambda@Edge function can be used to dynamically resize images based on the request.\nExternal image management library: Since the company wants to minimize operational overhead, using an external image management library within a Lambda@Edge function is a good choice. This eliminates the need to manage EC2 instances or other infrastructure."
    },
    {
      "id": "359",
      "question": "Ahospital needs to store patient records in an Amazon S3 bucket. The hospital’scompliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?",
      "options": {
        "A": "Create a public SSL/TLS certiFicate in AWS CertiFicate Manager (ACM). Associate the certiFicate with Amazon S3. ConFigure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.",
        "B": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). ConFigure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.",
        "C": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). ConFigure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.",
        "D": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie."
      },
      "correct_answer": "C",
      "explanation": "it allows the compliance team to manage the KMS keys used for server-side encryption, thereby providing the necessary control over the encryption keys. Additionally, the use of the \"aws:SecureTransport\" condition on the bucket policy ensures that all connections to the S3 bucket are encrypted in transit."
    },
    {
      "id": "360",
      "question": "A company uses Amazon API Gateway to run aprivate gateway with two REST APIs in the same VPC. The Buystock RESTful web service calls the Checkfunds RESTful web service to ensure that enough funds are available before astock can be purchased. The company has noticed in the VPC Fiow logs that the Buystock RESTful web service calls the Checkfunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement asolution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?",
      "options": {
        "A": "Add an X-API-Key header in the HTTP header for authorization.",
        "B": "Use an interface endpoint.",
        "C": "Use a gateway endpoint.",
        "D": "Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs."
      },
      "correct_answer": "B",
      "explanation": "Interface Endpoint (VPC Endpoint for API Gateway): An interface endpoint allows private connectivity to API Gateway within your VPC. By creating a VPC endpoint for API Gateway, you can ensure that the communication between the BuyStock and CheckFunds RESTful web services stays within the VPC, eliminating the need for traffic to go over the internet."
    },
    {
      "id": "361",
      "question": "A company hosts amultiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.",
        "B": "Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long- term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.",
        "C": "Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.",
        "D": "Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket."
      },
      "correct_answer": "C",
      "explanation": "Amazon DynamoDB with DynamoDB Accelerator (DAX):\nDynamoDB is a highly scalable and low-latency NoSQL database, suitable for frequently accessed data.\nDynamoDB Accelerator (DAX) is a caching layer that provides sub-millisecond read latencies for DynamoDB.\nExport Data to Amazon S3:\nUse DynamoDB table export to periodically export historical data to an Amazon S3 bucket.\nThis allows you to store historical data in a cost-effective manner while still benefiting from DynamoDB for frequently accessed data.\nAmazon Athena for One-time Queries:\nAmazon Athena allows you to run SQL queries directly on data stored in Amazon S3.\nBy using Athena, you can perform one-time queries on the historical data without the need to manage a separate database."
    },
    {
      "id": "362",
      "question": "A company uses apayment processing system that requires messages for aparticular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly. Which actions should A solutions architect take to meet this requirement? (Choose two.)",
      "options": {
        "A": "Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.",
        "B": "Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.",
        "C": "Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.",
        "D": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.",
        "E": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID."
      },
      "correct_answer": "BE",
      "explanation": "E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.\nAmazon Kinesis data streams can be used with partition keys to ensure that messages with the same partition key are processed in order. In this case, using the payment ID as the partition key will help maintain the order of messages.\nSQS FIFO queues ensure that messages are processed in the order they are received. By using message groups and setting the payment ID as the message group, you can guarantee that messages for the same payment ID will be processed sequentially."
    },
    {
      "id": "363",
      "question": "A company is building agame system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events. Which solution will meet these requirements?",
      "options": {
        "A": "Amazon EventBridge event bus",
        "B": "Amazon Simple NotiFication Service (Amazon SNS) FIFO topics",
        "C": "Amazon Simple NotiFication Service (Amazon SNS) standard topics",
        "D": "Amazon Simple Queue Service (Amazon SQS) FIFO queues"
      },
      "correct_answer": "B",
      "explanation": "SNS FIFO also can send events or messages cocurrently to many subscribers while maintaining the order it receives. SNS fanout pattern is set in standard SNS which is commonly used to fan out events to large number of subscribers and usually for duplicated messages."
    },
    {
      "id": "364",
      "question": "A hospital is designing anew application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
      "options": {
        "A": "Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.",
        "B": "Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.",
        "C": "Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.",
        "D": "Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.",
        "E": "Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS."
      },
      "correct_answer": "BD",
      "explanation": "D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.\nThis option ensures that data at rest in the SNS components is encrypted using an AWS KMS customer managed key. The key policy restricts key usage to authorized personnel.\nThis option ensures that data at rest in the SQS components is encrypted using an AWS KMS customer managed key. The key policy restricts key usage to authorized personnel, and the queue policy ensures that only encrypted connections over TLS are allowed."
    },
    {
      "id": "365",
      "question": "A company runs aweb application that is backed by Amazon RDS. Anew database administrator caused data loss by accidentally editing information in adatabase table. To help recover from this type of incident, the company wants the ability to restore the database to its state from 5 minutes before any change within the last 30 days. Which feature should the solutions architect include in the design to meet this requirement?",
      "options": {
        "A": "Read replicas",
        "B": "Manual snapshots",
        "C": "Automated backups",
        "D": "Multi-AZ deployments"
      },
      "correct_answer": "C",
      "explanation": "Amazon RDS (Relational Database Service) can automatically create backups of your database every day.\nThese backups are like snapshots of your entire database, capturing all the data.\nThey happen automatically, so you don't have to remember to do it.\nYou can decide how long you want to keep these backup snapshots. For example, you might choose to keep them for up to 35 days.\nThis is like saying, \"I want to keep the pictures of my database for the last 35 days.\""
    },
    {
      "id": "366",
      "question": "A company’sweb application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have asubscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?",
      "options": {
        "A": "Enable API caching and throttling on the API Gateway API.",
        "B": "Set up AWS WAF on the API Gateway API. Create a rule to Filter users who have a subscription.",
        "C": "Apply Fine-grained IAM permissions to the premium content in the DynamoDB table.",
        "D": "Implement API usage plans and API keys to limit the access of users who do not have a subscription."
      },
      "correct_answer": "D",
      "explanation": ""
    },
    {
      "id": "367",
      "question": "A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company'son-premises data centers in the United States, Asia, and Europe. The company’scompliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application. What should A solutions architect do to meet these requirements?",
      "options": {
        "A": "ConFigure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.",
        "B": "ConFigure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.",
        "C": "ConFigure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.",
        "D": "ConFigure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS."
      },
      "correct_answer": "A",
      "explanation": "This option suggests configuring three Network Load Balancers (NLBs) in the three AWS Regions to address on-premises endpoints. While AWS Global Accelerator is used, the NLBs are registered as its endpoints. This does not meet the requirement of hosting the application on premises."
    },
    {
      "id": "368",
      "question": "A solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods for IAM user passwords. What should the solutions architect do to accomplish this?",
      "options": {
        "A": "Set an overall password policy for the entire AWS account.",
        "B": "Set a password policy for each IAM user in the AWS account.",
        "C": "Use third-party vendor software to set password requirements.",
        "D": "Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the appropriate requirements."
      },
      "correct_answer": "A",
      "explanation": "Amazon Web Services (AWS) allows you to set an account-wide password policy using AWS Identity and Access Management (IAM). This policy defines the rules and requirements for all IAM users in the AWS account. It's a centralized approach to enforce security measures consistently across all users. In this case, the solutions architect can set the specific complexity requirements and mandatory rotation periods by configuring the password policy at the AWS account level."
    },
    {
      "id": "369",
      "question": "A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on aschedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on asingle instance. A solutions architect needs to implement asolution to resolve these concerns. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).",
        "B": "Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.",
        "C": "Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).",
        "D": "Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance."
      },
      "correct_answer": "A",
      "explanation": "AWS Batch: AWS Batch is a fully managed service for running batch computing workloads. It dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of the batch jobs. It allows you to run tasks written in different programming languages with minimal operational overhead."
    },
    {
      "id": "370",
      "question": "A company runs apublic three-tier web application in aVPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with alicense server over the internet. The company needs amanaged solution that minimizes operational maintenance. Which solution meets these requirements?",
      "options": {
        "A": "Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.",
        "B": "Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.",
        "C": "Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.",
        "D": "Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway."
      },
      "correct_answer": "C",
      "explanation": "NAT Gateway: A NAT gateway is a managed service provided by AWS that allows EC2 instances in private subnets to initiate outbound traffic to the internet while preventing unsolicited inbound traffic from reaching those instances. NAT gateways are fully managed, highly available, and require minimal maintenance.\nPublic Subnet: Placing the NAT gateway in a public subnet allows it to have access to the internet, fulfilling the requirement for private instances to communicate with a license server over the internet.\nDefault Route: Modifying each private subnet's route table with a default route that points to the NAT gateway ensures that traffic from private instances is directed through the NAT gateway for outbound communication."
    },
    {
      "id": "371",
      "question": "A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host adigital media streaming application. The EKS cluster will use amanaged node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using acustomer managed key that is stored in AWS Key Management Service (AWS KMS). Which combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)",
      "options": {
        "A": "Use a Kubernetes plugin that uses the customer managed key to perform data encryption.",
        "B": "After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.",
        "C": "Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.",
        "D": "Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.",
        "E": "Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes."
      },
      "correct_answer": "CD",
      "explanation": "D. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.\nEBS encryption is set regionally. AWS account is global but it does not mean EBS encryption is enable by default at account level. default EBS encryption is a regional setting within your AWS account. Enabling it in a specific region ensures that all new EBS volumes created in that region are encrypted by default, using either the default AWS managed key or a customer managed key that you specify."
    },
    {
      "id": "372",
      "question": "A company wants to migrate an Oracle database to AWS. The database consists of asingle table that contains millions of geographic information systems (GIS) images that are high resolution and are identified by ageographic code. When anatural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has asingle image or row that is associated with it. The company wants asolution that is highly available and scalable during such events. Which solution meets these requirements MOST cost-effectively?",
      "options": {
        "A": "Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.",
        "B": "Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.",
        "C": "Store the images and geographic codes in an Amazon DynamoDB table. ConFigure DynamoDB Accelerator (DAX) during times of high load.",
        "D": "Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance."
      },
      "correct_answer": "D",
      "explanation": "In this we cannot used DynamoDB database bcz it is a NOSQL database and we want a SQL  database bcz oracle database is SQL DATABASE thats why the correct ans is D. bcz used S3 bucket for storing data and used Oracle database for SQL we used Amazon RDS."
    },
    {
      "id": "373",
      "question": "A company has an application that collects data from IoT sensors on automobiles. The data is streamed and stored in Amazon S3 through Amazon Kinesis Data Firehose. The data produces trillions of S3 objects each year. Each morning, the company uses the data from the previous 30 days to retrain asuite of machine learning (ML) models. Four times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes. Which storage solution meets these requirements MOST cost-effectively?",
      "options": {
        "A": "Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.",
        "B": "Use the S3 Intelligent-Tiering storage class. ConFigure S3 Intelligent-Tiering to automatically move objects to S3 Glacier Deep Archive after 1 year.",
        "C": "Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.",
        "D": "Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year."
      },
      "correct_answer": "D",
      "explanation": "S3 Standard Storage Class:\nUse S3 Standard for the first 30 days because it's the default storage class for frequently accessed data.\nThis is suitable for the initial period when you need quick and frequent access to your data.\nS3 Standard-Infrequent Access (S3 Standard-IA) Storage Class:\nAfter the initial 30 days, transition the data to S3 Standard-IA.\nS3 Standard-IA is designed for data that is accessed less frequently but still requires quick retrieval when needed.\nIt's more cost-effective for data that is accessed less often compared to S3 Standard.\nS3 Glacier Deep Archive:\nAfter 1 year, transition the data from S3 Standard-IA to S3 Glacier Deep Archive using an S3 Lifecycle policy.\nS3 Glacier Deep Archive is the most cost-effective option for long-term archival storage.\nThis is suitable for storing data that you need to retain for compliance or archival purposes but don't need to access frequently."
    },
    {
      "id": "374",
      "question": "A company is running several business applications in three separate VPCs within the us-east-1 Region. The applications must be able to communicate between VPCs. The applications also must be able to consistently send hundreds of gigabytes of data each day to alatency- sensitive application that runs in asingle on-premises data center. A solutions architect needs to design anetwork connectivity solution that maximizes cost-effectiveness. Which solution meets these requirements?",
      "options": {
        "A": "ConFigure three AWS Site-to-Site VPN connections from the data center to AWS. Establish connectivity by conFiguring one VPN connection for each VPC.",
        "B": "Launch a third-party virtual network appliance in each VPC. Establish an IPsec VPN tunnel between the data center and each virtual appliance.",
        "C": "Set up three AWS Direct Connect connections from the data center to a Direct Connect gateway in us-east-1. Establish connectivity by conFiguring each VPC to use one of the Direct Connect connections.",
        "D": "Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway."
      },
      "correct_answer": "D",
      "explanation": "AWS Direct Connect: Using a single AWS Direct Connect connection from the data center to AWS is more cost-effective than setting up multiple connections. It provides a dedicated and consistent network connection between the on-premises data center and AWS.\nTransit Gateway: The use of a transit gateway simplifies network connectivity. It acts as a hub, allowing communication between the VPCs and the on-premises data center without requiring separate connections for each VPC. This reduces complexity and costs associated with managing multiple connections."
    },
    {
      "id": "375",
      "question": "An ecommerce company is building adistributed application that involves several serverless functions and AWS services to complete order- processing tasks. These tasks require manual approvals as part of the workfiow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Use AWS Step Functions to build the application.",
        "B": "Integrate all the application components in an AWS Glue job.",
        "C": "Use Amazon Simple Queue Service (Amazon SQS) to build the application.",
        "D": "Use AWS Lambda functions and Amazon EventBridge events to build the application."
      },
      "correct_answer": "A",
      "explanation": "Step Functions provide a way to coordinate and orchestrate multiple AWS services, including AWS Lambda functions, in a serverless workflow. They allow you to build applications by connecting various serverless functions and services without managing the underlying infrastructure."
    },
    {
      "id": "376",
      "question": "A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to the database come from serverless applications. Application trafic to the database changes significantly at random intervals. At times of high demand, users report that their applications experience database connection rejection errors. Which solution will resolve this issue with the LEAST operational overhead?",
      "options": {
        "A": "Create a proxy in RDS Proxy. ConFigure the users’ applications to use the DB instance through RDS Proxy.",
        "B": "Deploy Amazon ElastiCache for Memcached between the users’ applications and the DB instance.",
        "C": "Migrate the DB instance to a different instance class that has higher I/O capacity. ConFigure the users’ applications to use the new DB instance.",
        "D": "ConFigure Multi-AZ for the DB instance. ConFigure the users’ applications to switch between the DB instances."
      },
      "correct_answer": "A",
      "explanation": "RDS Proxy is a fully managed, highly available database proxy for Amazon RDS that makes applications more scalable, more resilient to database failures, and more secure. It automatically routes database traffic to the appropriate DB instance, handling connection pooling and failover."
    },
    {
      "id": "377",
      "question": "A company recently deployed anew auditing system to centralize information about operating system versions, patching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated. Which solution achieves these goals MOST eficiently?",
      "options": {
        "A": "Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the audit system.",
        "B": "Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.",
        "C": "Use an EC2 Auto Scaling launch conFiguration to run a custom script through user data to send data to the audit system when instances are launched and terminated.",
        "D": "Run a custom script on the instance operating system to send data to the audit system. ConFigure the script to be invoked by the EC2 Auto Scaling group when the instance starts and is terminated."
      },
      "correct_answer": "B",
      "explanation": ""
    },
    {
      "id": "378",
      "question": "A company is developing areal-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in adatabase solution that will scale without intervention. Which solution should A solutions architect recommend?",
      "options": {
        "A": "Use Amazon Route 53 for traFic distribution and Amazon Aurora Serverless for data storage.",
        "B": "Use a Network Load Balancer for traFic distribution and Amazon DynamoDB on-demand for data storage.",
        "C": "Use a Network Load Balancer for traFic distribution and Amazon Aurora Global Database for data storage.",
        "D": "Use an Application Load Balancer for traFic distribution and Amazon DynamoDB global tables for data storage."
      },
      "correct_answer": "B",
      "explanation": "Think of an NLB like a traffic cop for your game. It helps distribute and manage the incoming traffic from players to your game servers. It ensures that the load is balanced across your servers, which is crucial for handling the expected spikes in demand.\nDynamoDB is a type of database that can store data for your game, such as gamer scores. \"On-demand\" means that DynamoDB automatically scales to handle the amount of data and traffic your game is experiencing."
    },
    {
      "id": "379",
      "question": "A company hosts afrontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company'soperations. Which solution will meet these requirements?",
      "options": {
        "A": "Establish a connection between the frontend application and the database to make queries faster by bypassing the API.",
        "B": "ConFigure provisioned concurrency for the Lambda function that handles the requests.",
        "C": "Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.",
        "D": "Increase the size of the database to increase the number of connections Lambda can establish at one time."
      },
      "correct_answer": "B",
      "explanation": "Provisioned Concurrency: Provisioned concurrency allows you to pre-warm a specific number of instances of your Lambda function. This ensures that there are already instances available to handle incoming requests, reducing the cold start latency. Since the Lambda function loads many libraries, reducing cold start latency is crucial for optimizing response time."
    },
    {
      "id": "380",
      "question": "A company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon EC2 instances and Amazon RDS DB instances. The company wants asolution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance. Which solution will meet these requirements?",
      "options": {
        "A": "Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.",
        "B": "Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.",
        "C": "Launch another EC2 instance. ConFigure a crontab schedule to run shell scripts that will start and stop the existing EC2 instances and DB instances on a schedule.",
        "D": "Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. ConFigure Amazon EventBridge to invoke the Lambda function on a schedule."
      },
      "correct_answer": "D",
      "explanation": "AWS Lambda Function: Create a Lambda function that contains the logic to start and stop the EC2 instances and DB instances. Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. It is a cost-effective and maintenance-free solution.\nAmazon EventBridge: Configure EventBridge (formerly CloudWatch Events) to invoke the Lambda function on a schedule. EventBridge provides a reliable and scalable way to schedule the execution of Lambda functions at specified intervals, such as starting and stopping instances during business hours."
    },
    {
      "id": "381",
      "question": "A company hosts athree-tier web application that includes aPostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in areport each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes afew hours with the use of relational queries. The reporting process must not prevent any document modifications or the addition of new documents. A solutions architect needs to implement asolution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?",
      "options": {
        "A": "Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.",
        "B": "Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.",
        "C": "Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. ConFigure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.",
        "D": "Set up a new Amazon DynamoDB table to store the documents. Use a Fixed write capacity to support new document entries. Automatically scale the read capacity to support the reports."
      },
      "correct_answer": "B",
      "explanation": "Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports) is the best option for speeding up the reporting process for a three-tier web application that includes a PostgreSQL database storing metadata from documents, while not impacting document modifications or additions, with the least amount of change to the application code."
    },
    {
      "id": "382",
      "question": "A company has athree-tier application on AWS that ingests sensor data from its users’ devices. The trafic Fiows through aNetwork Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and Finally to EC2 instances for the application tier. The application tier makes calls to adatabase. What should A solutions architect do to improve the security of the data in transit?",
      "options": {
        "A": "ConFigure a TLS listener. Deploy the server certiFicate on the NLB.",
        "B": "ConFigure AWS Shield Advanced. Enable AWS WAF on the NLB.",
        "C": "Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.",
        "D": "Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS)."
      },
      "correct_answer": "A",
      "explanation": "TLS Listener on NLB: By configuring a TLS (Transport Layer Security) listener on the NLB, you can encrypt the traffic between the users' devices and the web tier EC2 instances. This helps protect the data in transit from eavesdropping and other potential security threats."
    },
    {
      "id": "383",
      "question": "A company is planning to migrate acommercial off-the-shelf application from its on-premises data center to AWS. The software has asoftware licensing model using sockets and cores with predictable capacity and uptime requirements. The company wants to use its existing licenses, which were purchased earlier this year. Which Amazon EC2 pricing option is the MOST cost-effective?",
      "options": {
        "A": "Dedicated Reserved Hosts",
        "B": "Dedicated On-Demand Hosts",
        "C": "Dedicated Reserved Instances",
        "D": "Dedicated On-Demand Instances"
      },
      "correct_answer": "A",
      "explanation": "A Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. When you launch instances on a Dedicated Host, those instances run on the dedicated hardware of that host.\nDedicated Hosts provide control over the placement of instances for compliance, licensing, or regulatory requirements.\nYou can purchase Dedicated Hosts on a reservation model (Reserved Hosts) or pay for them on-demand. The host remains dedicated to you for the specified term in the case of Reserved Hosts.\nDedicated Hosts can be useful for workloads with specific licensing models tied to physical sockets or cores."
    },
    {
      "id": "384",
      "question": "A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application needs astorage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The storage layer must provide maximum data durability and must be shareable across the EC2 instances. The data in the storage layer will be accessed frequently for the First 30 days and will be accessed infrequently after that time. Which solution will meet these requirements MOST cost-effectively?",
      "options": {
        "A": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Glacier.",
        "B": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Standard-Infrequent Access (S3 Standard-IA).",
        "C": "Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).",
        "D": "Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a lifecycle management policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One Zone-IA)."
      },
      "correct_answer": "C",
      "explanation": "Amazon EFS provides scalable and highly available file storage in the cloud. The Standard storage class is designed for frequently accessed data, making it suitable for the initial 30 days of frequent access.\nYou can create a lifecycle management policy for EFS that automatically transitions infrequently accessed files to the EFS Standard-Infrequent Access (EFS Standard-IA) storage class. This helps optimize costs by moving less frequently accessed data to a lower-cost storage tier."
    },
    {
      "id": "385",
      "question": "A solutions architect is creating anew VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created asecurity group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks. Which additional configuration strategy should the solutions architect use to meet these requirements?",
      "options": {
        "A": "Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.",
        "B": "Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.",
        "C": "Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.",
        "D": "Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group."
      },
      "correct_answer": "C",
      "explanation": ""
    },
    {
      "id": "386",
      "question": "An ecommerce company is running amulti-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?",
      "options": {
        "A": "Implement Amazon SNS to store the database calls.",
        "B": "Implement Amazon ElastiCache to cache the large datasets.",
        "C": "Implement an RDS for MySQL read replica to cache database calls.",
        "D": "Implement Amazon Kinesis Data Firehose to stream the calls to the database."
      },
      "correct_answer": "B",
      "explanation": "Amazon ElastiCache: Amazon ElastiCache is a fully managed in-memory caching service. By implementing ElastiCache, you can cache frequently accessed data in-memory, reducing the need to make repeated calls to the database. This helps improve the performance of your application by serving data directly from the cache instead of querying the database every time.\nCaching Large Datasets: In scenarios where identical datasets are frequently requested, caching the results in ElastiCache can significantly reduce the load on the database and improve response times for subsequent requests. It is particularly effective for read-heavy workloads where the data does not change frequently."
    },
    {
      "id": "387",
      "question": "A new employee has joined A company as adeployment engineer. The deployment engineer will be using AWS Cloudformation templates to create multiple AWS resources. A solutions architect wants the deployment engineer to perform job activities while following the principle of least privilege. Which combination of actions should the solutions architect take to accomplish this goal? (Choose two.)",
      "options": {
        "A": "Have the deployment engineer use AWS account root user credentials for performing AWS CloudFormation stack operations.",
        "B": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the PowerUsers IAM policy attached.",
        "C": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the AdministratorAccess IAM policy attached.",
        "D": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only.",
        "E": "Create an IAM role for the deployment engineer to explicitly deFine the permissions speciFic to the AWS CloudFormation stack and launch stacks using that IAM role."
      },
      "correct_answer": "DE",
      "explanation": "E. Create an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launch stacks using that IAM role.\nThis ensures that the IAM user has the necessary permissions for AWS CloudFormation but not unnecessary permissions for other AWS services.\nIAM roles are more suitable for temporary elevated permissions needed during AWS CloudFormation stack operations. The deployment engineer can assume the role when required, limiting their permissions to only what is needed for those specific actions."
    },
    {
      "id": "388",
      "question": "A company is deploying atwo-tier web application in aVPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information. The web application is not working as intended. The web application reports that it cannot connect to the database. The database is confirmed to be up and running. All configurations for the network ACLs, security groups, and route tables are still in their default states. What should A solutions architect recommend to Fix the application?",
      "options": {
        "A": "Add an explicit rule to the private subnet’s network ACL to allow traFic from the web tier’s EC2 instances.",
        "B": "Add a route in the VPC route table to allow traFic between the web tier’s EC2 instances and the database tier.",
        "C": "Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and conFigure VPC peering.",
        "D": "Add an inbound rule to the security group of the database tier’s RDS instance to allow traFic from the web tiers security group."
      },
      "correct_answer": "D",
      "explanation": "Security Groups: Security groups act as virtual firewalls for your instances to control inbound and outbound traffic. By default, they deny all inbound traffic. In this scenario, the default security group associated with the RDS instance is likely denying incoming traffic from the web tier.\nInbound Rule: To allow traffic from the web tier's EC2 instances to the database tier's RDS instance, you need to add an inbound rule to the security group associated with the RDS instance. This rule should permit traffic from the security group associated with the web tier's EC2 instances."
    },
    {
      "id": "389",
      "question": "A company has alarge dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance in asingle Availability Zone. The company wants business reporting queries to run without impacting the write operations to the production DB instance. Which solution meets these requirements?",
      "options": {
        "A": "Deploy RDS read replicas to process the business reporting queries.",
        "B": "Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.",
        "C": "Scale up the DB instance to a larger instance type to handle write operations and queries.",
        "D": "Deploy the DB instance in multiple Availability Zones to process the business reporting queries."
      },
      "correct_answer": "A",
      "explanation": "Amazon RDS provides the ability to create read replicas of a source DB instance. Read replicas can be used to offload read traffic from the primary (write) DB instance, allowing you to scale read operations horizontally. This is particularly useful for scenarios where you want to run reporting queries without affecting the write performance of the production DB instance."
    },
    {
      "id": "390",
      "question": "A company hosts athree-tier ecommerce application on aFieet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)",
      "options": {
        "A": "Turn on the sticky sessions feature (session aFinity) on the ALB.",
        "B": "Use an Amazon DynamoDB table to store customer session information.",
        "C": "Deploy an Amazon Cognito user pool to manage user session information.",
        "D": "Deploy an Amazon ElastiCache for Redis cluster to store customer session information.",
        "E": "Use AWS Systems Manager Application Manager in the application to manage user session information."
      },
      "correct_answer": "BD",
      "explanation": ""
    },
    {
      "id": "391",
      "question": "A company needs abackup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with adynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’srecovery point objective (RPO) is 2 hours. The backup strategy must maximize scalability and optimize resource utilization for this environment. Which solution will meet these requirements?",
      "options": {
        "A": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.",
        "B": "ConFigure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.",
        "C": "Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.",
        "D": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO."
      },
      "correct_answer": "C",
      "explanation": "Snapshots of EBS volumes would be necessary if you want to back up the entire EC2 instance, including any applications and temporary data stored on the EBS volumes attached to the instances. When you take a snapshot of an EBS volume, it backs up the entire contents of that volume. This ensures that you can restore the entire EC2 instance to a specific point in time more quickly. However, if there is no temporary data stored on the EBS volumes, then snapshots of EBS volumes are not necessary."
    },
    {
      "id": "392",
      "question": "A company wants to deploy anew public web application on AWS. The application includes aweb server tier that uses Amazon EC2 instances. The application also includes adatabase tier that uses an Amazon RDS for MySQL DB instance. The application must be secure and accessible for global customers that have dynamic IP addresses. How should A solutions architect configure the security groups to meet these requirements?",
      "options": {
        "A": "ConFigure the security group for the web servers to allow inbound traFic on port 443 from 0.0.0.0/0. ConFigure the security group for the DB instance to allow inbound traFic on port 3306 from the security group of the web servers.",
        "B": "ConFigure the security group for the web servers to allow inbound traFic on port 443 from the IP addresses of the customers. ConFigure the security group for the DB instance to allow inbound traFic on port 3306 from the security group of the web servers.",
        "C": "ConFigure the security group for the web servers to allow inbound traFic on port 443 from the IP addresses of the customers. ConFigure the security group for the DB instance to allow inbound traFic on port 3306 from the IP addresses of the customers.",
        "D": "ConFigure the security group for the web servers to allow inbound traFic on port 443 from 0.0.0.0/0. ConFigure the security group for the DB instance to allow inbound traFic on port 3306 from 0.0.0.0/0."
      },
      "correct_answer": "A",
      "explanation": ""
    },
    {
      "id": "393",
      "question": "Apayment processing company records all voice communication with its customers and stores the audio Files in an Amazon S3 bucket. The company needs to capture the text from the audio Files. The company must remove from the text any personally identifiable information (PII) that belongs to customers. What should A solutions architect do to meet these requirements?",
      "options": {
        "A": "Process the audio Files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.",
        "B": "When an audio File is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.",
        "C": "ConFigure an Amazon Transcribe transcription job with PII redaction turned on. When an audio File is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.",
        "D": "Create an Amazon Connect contact Fiow that ingests the audio Files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact Fiow when an audio File is uploaded to the S3 bucket."
      },
      "correct_answer": "C",
      "explanation": "Amazon Transcribe is a fully managed service provided by Amazon Web Services (AWS) that enables automatic speech recognition (ASR). It allows developers to convert spoken language into written text, making it useful for various applications such as transcription services, voice analytics, and content indexing."
    },
    {
      "id": "394",
      "question": "A company is running amulti-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is configured with the latest generation DB instance with 2,000 GB of storage in aGeneral Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand. Adatabase administrator analyzes the logs in Amazon Cloudwatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should A solutions architect do to improve the application performance?",
      "options": {
        "A": "Replace the volume with a magnetic volume.",
        "B": "Increase the number of IOPS on the gp3 volume.",
        "C": "Replace the volume with a Provisioned IOPS SSD (io2) volume.",
        "D": "Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes."
      },
      "correct_answer": "C",
      "explanation": "io2 volumes are designed for high-performance, low-latency applications such as databases.\nProvisioned IOPS allows you to specify the amount of IOPS the volume needs, ensuring consistent performance.\nFor applications with high demand and where consistent performance is crucial, io2 volumes provide better control over IOPS compared to gp3 volumes."
    },
    {
      "id": "395",
      "question": "An IAM user made several configuration changes to AWS resources in their company'saccount during aproduction deployment last week. A solutions architect learned that acouple of security group rules are not configured as desired. The solutions architect wants to confirm which IAM user was responsible for making changes. Which service should the solutions architect use to Find the desired information?",
      "options": {
        "A": "Amazon GuardDuty",
        "B": "Amazon Inspector",
        "C": "AWS CloudTrail",
        "D": "AWS ConFig"
      },
      "correct_answer": "C",
      "explanation": "AWS CloudTrail is a service provided by Amazon Web Services (AWS) that allows you to monitor and log AWS account activity. It records API calls made on your AWS account, capturing information such as the identity of the caller, the time of the API call, the source IP address, the request parameters, and the response elements returned by the AWS service."
    },
    {
      "id": "396",
      "question": "A company has implemented aself-managed DNS service on AWS. The solution consists of the following: • Amazon EC2 instances in different AWS Regions • Endpoints of astandard accelerator in AWS Global Accelerator The company wants to protect the solution against DDoS attacks. What should A solutions architect do to meet this requirement?",
      "options": {
        "A": "Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.",
        "B": "Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.",
        "C": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.",
        "D": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2 instances."
      },
      "correct_answer": "A",
      "explanation": "AWS Shield Advanced is a managed Distributed Denial of Service (DDoS) protection service provided by AWS.\nBy subscribing to AWS Shield Advanced, you gain access to enhanced DDoS protection capabilities, including automatic detection and mitigation of DDoS attacks."
    },
    {
      "id": "397",
      "question": "An ecommerce company needs to run ascheduled daily job to aggregate and Filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?",
      "options": {
        "A": "Create an AWS Lambda function that has an Amazon EventBridge notiFication. Schedule the EventBridge event to run once a day.",
        "B": "Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.",
        "C": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.",
        "D": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job."
      },
      "correct_answer": "C",
      "explanation": "C. Amazon ECS with Fargate: Fargate allows you to run containers without managing the underlying infrastructure. You can schedule the ECS task with EventBridge, and since Fargate manages the resources, you don't need to worry about scaling or infrastructure maintenance. This is a good fit for long-running jobs."
    },
    {
      "id": "398",
      "question": "A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company’sinternet connection can support an upload speed of 100 Mbps. Which solution meets these requirements MOST cost-effectively?",
      "options": {
        "A": "Use Amazon S3 multi-part upload functionality to transfer the Files over HTTPS.",
        "B": "Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.",
        "C": "Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.",
        "D": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3."
      },
      "correct_answer": "C",
      "explanation": "Transferring 600 TB of data over a 100 Mbps connection would take a very long time. AWS Snowball Edge devices allow for offline data transfer, and you can transfer the data to the devices at your location before shipping them to AWS. This way, you are not constrained by the upload speed during the 2-week period."
    },
    {
      "id": "399",
      "question": "AFinancial company hosts aweb application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’ssecurity team has noticed an increase in the number of API requests. The security team is concerned that HTTP Fiood attacks might take the application ofiine. A solutions architect must design asolution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?",
      "options": {
        "A": "Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.",
        "B": "Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.",
        "C": "Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predeFined rate is reached.",
        "D": "Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predeFined rate."
      },
      "correct_answer": "B",
      "explanation": "Rate-based Rule with AWS WAF: AWS WAF provides protection against various web application attacks, including HTTP flood attacks. By using a rate-based rule, you can set thresholds for the number of requests from a client IP within a specified time period. This helps in detecting and mitigating HTTP flood attacks effectively."
    },
    {
      "id": "400",
      "question": "Ameteorological startup company has acustom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build anew service that sends an alert to the managers of four internal teams every time anew weather event is recorded. The company does not want this new service to affect the performance of the current application. What should A solutions architect do to meet these requirements with the LEAST amount of operational overhead?",
      "options": {
        "A": "Use DynamoDB transactions to write new event data to the table. ConFigure the transactions to notify internal teams.",
        "B": "Have the current application publish a message to four Amazon Simple NotiFication Service (Amazon SNS) topics. Have each team subscribe to one topic.",
        "C": "Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple NotiFication Service (Amazon SNS) topic to which the teams can subscribe.",
        "D": "Add a custom attribute to each record to Fiag new items. Write a cron job that scans the table every minute for items that are new and notiFies an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe."
      },
      "correct_answer": "C",
      "explanation": "Using a single SNS topic simplifies the notification process. The trigger can publish a message to this topic, and each internal team can subscribe to this topic. This reduces the operational overhead compared to managing multiple SNS topics (Option B)."
    }
  ]
}